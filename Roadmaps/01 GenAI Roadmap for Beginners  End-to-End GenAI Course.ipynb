{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8a1c97-ded7-4c76-95c9-5710c3b6a4f0",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's start this video, guys.\n",
    "\n",
    "Before we start our discussion, I want to convince you a little bit about why you should study(read) Generative AI today.\n",
    "Let's start this discussion from the very beginning. First, let me tell you what generative AI is. \n",
    "I have a definition for you. So, I'll read the definition once, and then we'll discuss it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e27ff4-e9c8-45c4-9726-388c77142b5d",
   "metadata": {},
   "source": [
    "<img src=\"Images-of-Genai-Roadmap/Image01.png\" alt=\"What is GenAI\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ac275-4d61-48aa-917c-c126ae54c836",
   "metadata": {},
   "source": [
    "# **What is GenAI?**\n",
    "```\n",
    "So, generative AI is a type of AI that creates new content such as text, images, music, or even code by learning patterns from existing data, mimicking human creativity. \n",
    "\n",
    "This is a very good definition, and I'll give you a little more background so that you can understand this definition a little better. So, if you look at the history of AI, work on AI has been going on for the last 60-70 years, which means that generations of brilliant individuals have come and given their all to this field, and the result has been that many technologies and techniques have emerged to build AI systems. For example, in the 80s, there was a very popular technique called Symbolic AI, with the help of which expert systems were created, which is a type of AI system. After that, a lot of work was done on fuzzy logic, a lot of work was done on evolutionary algorithms, a lot of work was done separately on the field of NLP, and a lot of work was done separately on the field of computer vision.\n",
    "\n",
    "\n",
    "But one technique that was able to create the most impact in the domain of AI was machine learning. In machine learning, what you do is you provide a mathematical or statistical model with a lot of data, and here our goal is that the statistical model should be able to extract patterns so that when you give it new data, it can give you predictions. Now, this simple idea was so powerful that it is still relevant today. We still use machine learning. We use it, and even today, machine learning-related jobs are available in the market. \n",
    "\n",
    "Okay, so what happens in machine learning is that you can create three or four types of models. \n",
    "First, you can create models that can predict numbers, like, for example, you can create a model that can predict what the stock price of a particular company will be tomorrow. Basically, a regression problem. \n",
    "Or you can create an ML model where you can classify one category from a given set of categories. For example, if we show our model a photo, it can predict whether it contains a cat or a dog. A classification problem. \n",
    "Or you can use ML for ranking problems, such as given a product, you want to rank and show the most similar products. Recommendation systems. \n",
    "\n",
    "So, you used ML in these types of problems, but machine learning was never used in problem statements where human creativity was required. But after the arrival of generative AI, this has changed. What is the biggest feature of generative AI? The biggest differentiating factor is that it can create new text, create new images and videos, and even create code. And this is the biggest power of generative AI. \n",
    "\n",
    "Until two or three years ago, people used to say about AI that no matter how strong the systems you build, AI will not replace human creativity, at least not yet. But guess what? Within two years, after the arrival of generative AI, this statement has become completely false. Now, today, human creativity can be very easily mimicked with the help of generative AI, and this is the biggest superpower. Power That generative AI has, Now you understand what generative AI is. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869c272-5fc9-4f4e-ac49-3418d2560639",
   "metadata": {},
   "source": [
    "<img src=\"Images-of-Genai-Roadmap/Image01.png\" alt=\"What is GenAI\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa862fac-cc75-45d3-9115-d5852a9c25d4",
   "metadata": {},
   "source": [
    "```\n",
    "One more thing, let's quickly look at where generative AI lies in this entire AI landscape. \n",
    "So, you must have seen this famous diagram somewhere, I'm pretty sure. So, in the outermost circle is AI. Everything comes under this particular umbrella. \n",
    "\n",
    "And within AI, there are many techniques, as I mentioned, there is symbolic AI, expert systems, fuzzy logic, but among them, the most powerful technique is machine learning. \n",
    "\n",
    "Within machine learning, there are also many techniques, statistics, your different model-based algorithms, but another technique that emerged prominently is deep learning, where you use neural network-based models. And finally, when there was a lot of advancement in deep learning, the transformer architecture came, and after that, generative AI finally originated from there. \n",
    "\n",
    "So this diagram is a very good mental model to track this entire history. \n",
    "On the outside is AI, inside that is machine learning, inside that is deep learning, and finally, inside that is generative AI. \n",
    "\n",
    "Okay? So you should kind of know this, if you want to study generative AI. So now you understand this. Now let's have an important\n",
    "discussion about how in the last two years, like within two years, generative AI has become the most powerful technology in our current world. I would like to show you the impact areas where generative AI has created the most impact. So, there are many impact areas where generative AI has done something or the other.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f6ce0-d97d-4e92-b972-52c4dc72896d",
   "metadata": {},
   "source": [
    "# **GenAI Impact Areas**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image02.png\" alt=\"GenAI Impact Areas\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16c91f-4a9c-45b1-ac58-7da3fe2b16be",
   "metadata": {},
   "source": [
    "```\n",
    "Generative AI has become the most powerful technology in our current world. I would like to show you the impact areas where Generative AI has created the most impact. So, there are many impact areas where Generative AI has made some significant contribution, but I will talk about four areas where I think Generative AI's contribution is very significant in further enhancing that field.  \n",
    "\n",
    "GENAI Impact Areas:\n",
    "\n",
    "\n",
    "So, the first area is customer support. So, this is a field where Generative AI has created a significant impact. So, until now, for all the big companies, customer handling was a very big challenge. The more customers, the more queries, the more complaints.\n",
    "\n",
    "To handle all this, companies used to set up their call centers and have their employees handle the queries through phone or chat messages. But as you can see, this is a very costly model, and businesses had to invest a lot of money unnecessarily. After the arrival of Generative AI, this problem has been solved to a great extent.\n",
    "\n",
    "Now, what happens is, take any big company, let's say Zomato, where a lot of customers come and place orders on a daily basis. So now, what happens is that at the first level, if you go to customer support, you are not directly communicate with an human executive. first of all, you communicate with a chatbot. It solves the problem based on its level. If the problem was solved means very good, otherwise if the problem was deeper and it unsolved then what happens is, it send forward to human executives, and then they solve the problem. But by implementing this simple model, what has happened is that where you needed 10 people for customer handling, now perhaps two to three people can handle the same workload. You can solve the problem and this model is now being used by the entire industry. Gradually, everyone is having a first layer of chatbot, and after that, they have their customer support team. Okay, so this is one area where a lot of progress and innovation is already visible with the help of Gen AI.\n",
    "\n",
    "\n",
    "Second is content creation. So, content creation was a field where it was said that AI wouldn't be coming anytime soon, but since generative AI can help in creating content, it was obvious that generative AI would penetrate the content creation industry significantly. So now, if you take any form of content, text-based content like blogs and websites, or video-based content, like Instagram and YouTube I don't know. Take any kind of content, you would notice that generative AI tools have now arrived there and are helping a lot in content creation. In fact, today, if I read an article on Medium, I can't be 100% sure whether it was written by a human or by AI. The output of generative AI models is that good. So, content creation is one such domain where there has already been a lot of penetration of generative AI, and it will increase even more in the future. \n",
    "\n",
    "\n",
    "Okay. Third is education. You yourself will agree that there has been a change in your own way of studying in the last two years since tools like ChatGPT have come into the market. Now, it has become quite easy for us to study and explore any new topic. If I need to plan a curriculum, I can go to ChatGPT. If I'm facing a problem... I'm stuck, I can go to ChatGPT. I need practice questions on a particular problem. I can go to ChatGPT, right? It's like having a personal tutor with you all the time, and that's why education is also evolving in a different way. Even schools have to think about how they should implement educational methods going forward. So education is the third frontier where the significant impact of generative AI is visible. \n",
    "\n",
    "\n",
    "And the fourth is software development. Until now, it was considered that software development is a job that requires very intelligent people because writing code is a very creative and intellectual task. But apparently, generative models are really, really good at writing code, and not only can they write code, but they can write very good production-ready code. So here too, it seems that a lot of changes are going to happen. Already, many tools have come out that have made programming kind of easy. You can simply make changes in a prompt. So where you needed five programmers, there's a good chance that two or three programmers can easily handle that task. So these are some impact areas that I think have been most impacted after the arrival of generative AI.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc921a69-888a-4ab3-83bc-32ed31abcfcd",
   "metadata": {},
   "source": [
    "# **Is GenAI Successful technology**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image03.png\" alt=\"Is GenAI Successful technology\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad4d74-2f05-4559-bdf0-5c4d8c3eb241",
   "metadata": {},
   "source": [
    "```\n",
    "Now, an important question I should ask you here is whether generative AI is successful?\n",
    "\n",
    "\n",
    "Is GENAI successful? as A technology will be considered successful, because if the technology is not successful, then there's no point in studying it. \n",
    "\n",
    "So I personally feel whenever you are going to spend some time on a technology, you have to be sure that this technology is worth studying, that it is successful. So next, we will discuss whether generative AI should be considered a successful technology or not. And for reference, I will talk about two other technologies and compare generative AI with them. \n",
    "\n",
    "One is the internet. In my lifetime, I think the most successful technology has been the internet, right? And I will take another example, blockchain and crypto, because this is another technology that has come into existence in my lifetime and maybe hasn't reached its full potential yet. So I just want to see where generative AI lies in this entire spectrum between the internet and crypto. If it lies more on the internet side, then it means it's on the path to being successful. If it lies on the crypto side, then maybe it's not that successful. So if someone asks me whether a technology is successful or not, I ask a series of questions. If the answer to those questions is yes, then I consider that technology successful. So I'll show you that series of questions. \n",
    "\n",
    "So the first question I ask is whether that technology is solving real-world problems or not. \n",
    "\n",
    "\n",
    "So if you talk about the internet, the internet is truly solving real-world problems. Earlier, we used to write letters to our relatives, which had a turnaround time of 15 days, right? Earlier, We used to go to the bank physically to do transactions, we used to go to the railway station to book tickets, we used to go to shops to buy things. After the arrival of the internet, all of this can be done from your phone while sitting at home. You are actually solving a problem, but at the same time, if you talk about crypto, I don't have any very concrete example in my mind right now where a real-world problem is being solved. Again, it might be, but I don't see any very popular example. \n",
    "\n",
    "Okay, now let's talk about generative AI. Generative AI is most likely solving many real-world problems right now.\n",
    "For example, a little while ago I told you about the customer support problem. Companies all over the world are struggling with this problem of how to handle customers at scale. After the arrival of generative AI, this problem seems to be getting solved. Similarly, I gave you the example of education, how easily everyone has got their personal tutor. This is a real-world problem that generative AI is solving. So for me, the answer to this question is yes.\n",
    "\n",
    "\n",
    "The next question is, is it useful on a daily basis? \n",
    "\n",
    "Again, the answer is yes. \n",
    "The tools you have in generative AI can definitely be used on a daily basis, and you will benefit from it.\n",
    "\n",
    "\n",
    "The third question is, is it impacting the world economy? \n",
    "\n",
    "Now what can I say? Let's take today's news. So, there is news today that a new AI model has come from China, DeepSeek R1, and just its arrival in the market has wiped out one trillion dollars from the stock shares of US-based tech companies.\n",
    "Can you imagine? Imagine a trillion dollars, that's roughly equal to 80 lakh crore rupees. If the arrival of just one AI model can have such an impact on another country's stock market, then I don't need to answer this question. The answer is absolutely yes. \n",
    "\n",
    "\n",
    "Question number four is: Is it creating new jobs? \n",
    "\n",
    "Now this is a bit controversial, because you might think that with the arrival of generative AI, many jobs will be lost. But we are not discussing that point. We are discussing it strictly from a technical point of view.\n",
    "\n",
    "In my opinion, after the arrival of generative AI, a completely new job has emerged in the technical field, which we call an AI engineer, and the number of these jobs is increasing every single day. I constantly go to websites like naukari.com and indeed.com and check. I have seen that the demand for this particular job role is increasing over time, and there is a good chance that in the next 5 years, this particular job role will become as popular as a software developer or a web developer job. So, again, the answer is yes. \n",
    "\n",
    "\n",
    "And the last question is, is it accessible? Again, the answer is yes. \n",
    "\n",
    "You can use generative AI tools very easily. Not only you, but even your parents, my parents can easily use these tools because you don't need to write code to use these tools. You can simply generate responses from these tools by talking in English or Hindi. So this technology is accessible. \n",
    "\n",
    "So since all of these questions have a \"yes\" as the answer, that's why in my mind, generative AI is on its way to becoming a successful technology. it's on the internet route, So that is why I feel that if you are working in AI or related fields or want to work in them, then generative AI is a must.\n",
    "\n",
    "\n",
    "You should definitely invest your time and effort in this because this technology has just arrived and it hasn't yet reached its full potential. So when it reaches its full potential, those who are working with this technology will definitely benefit a lot. So I really hope I was able to convince you a little bit. What do you think?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d7d87-9186-4db3-aa11-383cbef208a8",
   "metadata": {},
   "source": [
    "# **Why not?**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image04.png\" alt=\"Why not?\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4294f-f211-4b81-a328-d5ffbe2c7a0f",
   "metadata": {},
   "source": [
    "```\n",
    "You're saying that, \"Sir, we already know that this technology is important and will be in high demand in the future. \n",
    "That's why we kept messaging you to please upload videos on Generative AI.  \n",
    "Tell us why you weren't uploading videos?\" So, let me tell you why I was so late\n",
    "in uploading videos on Generative AI. There are three reasons. \n",
    "\n",
    "\n",
    "The first reason is that I had some doubts about this technology because it's new and only came out two years ago, and in these two years it has become very popular. So I felt a little unsure whether this is truly a powerful technology or just a bubble that everyone is creating hype about. \n",
    "\n",
    "\n",
    "The second problem was that there was a bit of a time commitment issue.\n",
    "In my life, Other things were going on due to which I wasn't able to allocate time to teach Generative AI.\n",
    "\n",
    "\n",
    "And third, even if I could find the time, I was a little intimidated by Generative AI, and the reason is that something new would come out every day. A new model would come out, a new tool would come out, a new research paper would come out, a new terminology would come out. So basically, the development was happening at such a fast pace that I felt like, \"What should I study? Where should I study from?\" And I'm pretty sure, you can relate to this, that if you try to study Generative AI, the biggest problem is that so much information is coming out on a daily basis that it's very difficult to extract meaningful things from it and learn the technology in the right way. \n",
    "\n",
    "So now two of these problems have been solved for me. The first problem was that I had The doubts I had are now gone because I've read a lot and understood many things, and now I feel that this technology has potential. The second problem, the time commitment issue, has also been solved. I have a lot of time now, so I can create content and make videos on generative AI. \n",
    "\n",
    "\n",
    "But the third problem still remained: getting involved in this fast-paced development that's happening on a daily basis, understanding things, and then making videos for you – that was still a challenging task for me. So, let's do one thing...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda4944-06b3-48c2-aee6-535b1d2d4ae5",
   "metadata": {},
   "source": [
    "# **The Problem**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image05.png\" alt=\"The Problem\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d8582-036f-48e0-9ce5-1a910acb7f36",
   "metadata": {},
   "source": [
    "```\n",
    "Let's discuss Problem Number Three in a little more detail. \n",
    "\n",
    "\n",
    "(1) So the problem is that this technology is growing very rapidly, which means that a lot of advancements are happening on a daily basis, and tracking them and then extracting some meaning from them has become a very challenging task. \n",
    "\n",
    "(2) Second, there's a lot of noise. You'll find people who are constantly creating a FOMO (Fear Of Missing Out) environment, saying things like, \"If    you haven't studied Generative AI, then what are you even doing?\" \"This is happening in Generative AI, that is happening,\" because of which you feel very demotivated.\n",
    "\n",
    "(3) There is no single source or established curriculum for this yet because it's a new technology. \n",
    "\n",
    "So my problem was that I wanted to teach you Generative AI properly, and the first step is designing a good curriculum. \n",
    "But because of these reasons, I wasn't even able to create a very good curriculum. So I felt that first, I needed to sit down and solve this problem and create a mental model that would help me design the curriculum first, and then create videos for you.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b51321-79ce-49b0-bc55-87e78aed7de4",
   "metadata": {},
   "source": [
    "# **Mental Model**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image06.png\" alt=\"Mental Model\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73256cd6-10da-4279-8c2d-50faacfbe847",
   "metadata": {},
   "source": [
    "```\n",
    "So, all the experience I had in AI, machine learning, and deep learning, I used all of that, I used my knowledge to create a mental model, a framework, with which you can ease out this entire information overload. And that's the mental model I'm going to share with you now. \n",
    "\n",
    "So, in the process of creating this mental model, I felt that first I should find a term that becomes the center of our entire mental model.  I mean, there are so many terms and so many tools in generative AI. Is there any one term or tool among them that we can put at the center to simplify things? The answer is yes, there is one such term that comes at the center of generative AI, and that term is Foundation Models. \n",
    "\n",
    "Let me tell you. \n",
    "\n",
    "What are Foundation Models? \n",
    "\n",
    "So, Foundation Models are actually a type of AI models that are very large-scale AI models. To train them, you need a huge amount of data, literally like perhaps as much data as there is on the internet, and to train them, you need a lot of hardware, like a lot of GPUs, and it costs millions of rupees to train them. But their biggest feature is that, unlike other machine learning models, these models are generalized models; they are not specialized models. \n",
    "\n",
    "\n",
    "So what does this mean?\n",
    "\n",
    "If you have created models in machine learning so far, you would know that the models you create are task-specific models, right? If you want to predict what the stock price of a company will be on a future date, then if you are creating a model for that, that model, It's not like it can only do one thing; it's not like it will go and predict cricket scores. So these are like specific models, but foundation models are generalized models, meaning they can do more than one thing, they can solve more than one task, and this happens because, first of all, the architecture of the models you're building is very large, they have a lot of parameters, and secondly, you're giving them a lot of data. So basically, you're giving a very intelligent person a lot of books to read, so obviously, they can solve problems in more than one domain. \n",
    "\n",
    "So that's what foundation models are, and a very good example of a foundation model is LLMs, Large Language Models, which are basically the backbone of generative AI today. You can take any LLM and have it solve many kinds of problems. \n",
    "\n",
    "You can have it do text generation, you can have it do sentiment analysis, summarization, question answering, all because in the process of training the LLM, you gave a very large AI model with many parameters the data of the entire internet. And then in this whole process, what happened was that not only did it learn to predict the next word, but at the same time, it learned sentiment analysis, question answering, summarization, and all these things. \n",
    "\n",
    "So I think if you have to put something at the center of this entire generative AI landscape, it's the foundation model. By the way, foundation models don't just include LLMs; there are also LMMs, Large Multimodal Models, models that don't just work with text. They can also work with images, videos, and sound. That is why I haven't written LLMs here, but rather Foundation Models. But if you want to simplify things for now, you can write LLMs instead of Foundation Models. So now a very good thing has happened: a term has come right at the center of the mental model I created, and that is Foundation Model. Now see how I simplified things. \n",
    "\n",
    "\n",
    "\n",
    "Based on my knowledge, I felt that the entire Generative AI can be divided into two parts, okay? \n",
    "\n",
    "So at its center is the Foundation Model. Now, in Generative AI, you can do two tasks: either you can use Foundation Models or you can build Foundation Models. These are the only two things that happen. The entire Generative AI is divided into these two parts. \n",
    "\n",
    "Any term you hear, any tool you hear, any terminology you hear, it will either be from the perspective of building or from the perspective of using. So, in short, my mental model says that in the entire Generative AI, the most core component is the Foundation Model, and in the entire Generative AI, you are doing one of two things: either you have a user perspective where you are using a ready-made Foundation Model for your work, or the second perspective is the builder's perspective where, as a big company, you are building these Foundation Models and deploying them somewhere. \n",
    "\n",
    "So this is my mental model, with the help of which I simplified the entire thing. So now something very interesting started here. As soon as I created this mental model, after that, I kind of created a game for myself. After that, whenever I came across any new term or heard about something new, I would first try to think, whether it would go to the user side, meaning the application building side, or the build side.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2b318-3ef7-482f-9a3b-937d8932ec50",
   "metadata": {},
   "source": [
    "<img src=\"Images-of-Genai-Roadmap/Image13.png\" alt=\"Matching Terms\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16d801-d36d-4567-aa79-ffb410188af1",
   "metadata": {},
   "source": [
    "```\n",
    "It will go to the side where we build foundation models. So let's do one thing, let's check this a little bit, let's check this entire mental model a little bit. Let's play a match the following kind of game. \n",
    "\n",
    "\n",
    "So let's say you heard a new term \"prompt engineering.\" What will you do? \n",
    "\n",
    "You will immediately go and ask ChatGPT what prompt engineering is, and then based on that description, you will decide whether it goes to the user side or the builder side. So, prompt engineering is when you have an LLM, you can talk to it directly in English, you can ask it questions, and you can refine your questions in such a way that you get better answers. \n",
    "\n",
    "That is what prompt engineering is. So, since you are using a ready-made LLM here, this will go to the user side. \n",
    "\n",
    "\n",
    "Second, RLHF, Reinforcement Learning with the help of Human Feedback. Now, this is a technology that helps you modify the behavior of an LLM a little bit, safeguard it a little so that it doesn't say anything inappropriate. So, this is used in the process of building LLMs. That is why it will go to the builders' perspective. \n",
    "\n",
    "\n",
    "Similarly, RAG is a very famous term. In RAG, what you do is you can perform question answering on your documents using a given LLM. For example, I want to perform question answering on my private data, so I can merge it with ChatGPT, right? So again, since we are using a ready-made LLM, this will also go to the user. \n",
    "\n",
    "\n",
    "What is pre-training? \n",
    "\n",
    "Pre-training is when you take a foundational model and train it with data from all over the world on powerful hardware. Since this is the process of creating LLMs, it falls under the builders' perspective. \n",
    "\n",
    "\n",
    "What is quantization? \n",
    "\n",
    "Quantization is the process where you can optimize a model, which then allows you to run it in different environments. \n",
    "So this also comes under the process of creating LLMs, hence the builders' perspective. \n",
    "\n",
    "\n",
    "AI agents are a term that is very commonly heard these days, where you create software with the help of LLMs that not only can answer questions but can also perform tasks for you, such as booking tickets. So again, something you create with the help of LLMs, so this will also go under the user perspective. \n",
    "\n",
    "\n",
    "Vector databases are used when implementing RAGs, so this will also go under the user perspective. \n",
    "\n",
    "And then there are some terms that fall on both sides, such as fine-tuning. \n",
    "You do fine-tuning in the process of creating LLMs and also in the process of using LLMs, so it will go on both sides. \n",
    "\n",
    "\n",
    "So this is what I've been doing for the past six to eight months. Whenever I heard a new term, the first thing I used to ask was whether it would go on the user side or the builders' side. And by doing this for the past six to eight months, I have finally prepared a curriculum where there are two aspects: one is the builders' perspective and the other is the users' perspective. Both have their own curriculum and I will now share those two with you.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9509a24-2c49-423c-a854-94c4fb37da70",
   "metadata": {},
   "source": [
    "# **Builder Perspective**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image07.png\" alt=\"Builder Perspective\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdadcc-ed92-44ea-a8b5-63303523d59d",
   "metadata": {},
   "source": [
    "```\n",
    "So guys, now Builder's Perspective \n",
    "\n",
    "I'm going to share with you the builder's side of the curriculum, the side where you build foundation models in generative AI and deploy them so that the whole world can use them. \n",
    "\n",
    "Okay? So this side is a bit more technical, and honestly, it will seem more relevant to us because we've been doing this work in machine learning and deep learning until now. So the entire curriculum for the builder's side starts. In fact, before it starts, let me tell you the prerequisites for covering this curriculum. First, you should know the fundamentals of machine learning. Second, you should know the fundamentals of deep learning, and you should have an idea of a deep learning framework, either TensorFlow or PyTorch, both will work, although PyTorch is more preferred. \n",
    "\n",
    "\n",
    "Okay, now let's talk about this curriculum. This curriculum starts with the Transformer architecture. Obviously, because any foundation model you pick up today is based on the Transformer model, based on the Transformer architecture. So if you want to understand this perspective of how foundation models are built, it starts with the Transformer architecture. You should know the entire Transformer architecture very well, what's happening on the encoder side, what's happening on the decoder side, what embeddings are, how self-attention works, how layer normalization works, the concept of language modeling – you should know all these things. This is step number one. \n",
    "\n",
    "In this curriculum, once you've studied the Transformer architecture, the second thing you need to study is the types of Transformers. So you know that multiple types of Transformers exist today. So you should have an idea of what encoder-only Transformers are, what decoder-only Transformers are, and what encoder and decoder-based Transformers are. You should know the architecture of BERT, you should know the architecture of GPT, and you should know the architecture of some other famous Transformers. This is step number two. \n",
    "\n",
    "Once you learn all this, then the training part of the foundation model begins. So here you will first study pre-training. Pre-training is the part where you train foundation models. So here you have to study several things. First, you have to study about training objectives. Different foundation models are trained with different training objectives. You have to study about tokenization strategies, especially if you are working with LLMs. Different training strategies need to be understood. Are you training on your machine, training on the cloud, training on one machine, or doing distributed training on multiple machines? You should know all these concepts. Then, what challenges can arise? For example, training such a large-scale model can present a thousand kinds of challenges. So you should be aware of those challenges and their solutions. So this comes under pre-training. Once you study pre-training, evaluation also comes here. Once you've completed the pre-training, to check how well it went, you also evaluate your models. \n",
    "\n",
    "\n",
    "So, once the pre-training is done, then comes optimization. What happens is that foundation models are very large in size, and mostly you can't run them on normal hardware. So, you have to optimize them in different ways. You can perform optimizations during the training process. You can compress the final model that has been created. Here, you might have heard terms like quantization, which we heard a little while ago, and knowledge distillation. All those things come here. Finally, how you can reduce the time for inference, the predictions you're getting from your LLM, that optimization also comes here. So, this is the third module, and the fourth module of this entire curriculum. \n",
    "\n",
    "Once you've done the optimization, then finally you learn about fine-tuning. What happens is that a foundation model is trained on a large dataset, and it's a generalized model. That means it can perform many types of tasks. But then you fine-tune that model for a particular task or a type of task according to your needs, so that its performance is further enhanced in that particular task. So, here there are many types of things: task-specific tuning, which I just told you, instruction tuning, and continual pre-training, where you pre-train it again a little more in a particular domain. RLHF comes here. There's a very famous technique called PEFT, that also comes here. So fine-tuning is one thing that is very important, and you perform it right after optimization.\n",
    "\n",
    "\n",
    "Evaluation: Once you have fine-tuned it, you can implement it in a particular domain, but before implementing it, you apply very thorough evaluation techniques to find out the performance. Okay?  Today, you constantly hear about LLM leaderboards where you hear that this LLM beat another LLM, DeepSeek r1 beat ChatGPT. So how is this decision-making happening? You try to understand this in this particular module, where you perform evaluation on different metrics for your LLM. \n",
    "\n",
    "And lastly, when you have done all this, the final task is to deploy them. If you can't deploy them, then it's all useless. So the last but a very important step is deployment. \n",
    "\n",
    "\n",
    "Okay? So what you see on the screen is from the curriculum builders' side. Any data scientist who works at Google or OpenAI takes the transformer architecture and build models then do deployment\n",
    "\n",
    "All the steps are listed here, so if you conquer this, then you will understand the builder's perspective well, okay? So now that we know what things are done in the process of building a foundation model, let's discuss what things need to be done in the process of using a ready-made foundation model. So now let's talk about the user side.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a65fb-00f2-4d6a-8d44-bad9ac48c186",
   "metadata": {},
   "source": [
    "# **User Perspective**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image08.png\" alt=\"User Perspective\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45a95c-9f61-4520-8922-953902958964",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's discuss what's involved in the process of using a pre-built foundation model. \n",
    "So now let's talk about the user side curriculum, where you learn to use a pre-built foundation model. \n",
    "So, first, a disclaimer: this side, personally, I find a little less technical and a little easier compared to the builder side.\n",
    "\n",
    "Obviously, building foundation models is more difficult than using them, so this side will seem a bit easier and perhaps more fun because you get to build quite interesting things here. So, what do you need to learn first? You first need to learn to build basic apps using LLMs. \n",
    "\n",
    "Here, you first learn about the different types of LLMs or foundation models available, whether they are closed-source or open-source. If they are closed-source, you learn to use them through APIs. If they are open-source, you learn to use them through different tools. Hugging Face is a library; with its help, you learn to run LLMs on your machine, or you use a tool like Ollama to run LLMs on your machine or server. After that, you learn a framework like LangChain, with the help of which you learn to build LLM-based applications. And I've already told you about Hugging Face, ollama, etc. So this is the first part where you know that an LLM exists, and you are learning different ways to use it, whether it's an API-based method or running it locally on your machine. \n",
    "The way to run it is fine. \n",
    "\n",
    "Once you've done all that, now comes a very important stage where you learn to improve the response you're getting from the LLM.\n",
    "So there are three techniques involved. \n",
    "\n",
    "The first is prompt engineering. We discussed prompt engineering a little while ago. Prompt engineering is the art and science of writing prompts. What is a prompt? It's the input you send to a foundation model.\n",
    "And if you can handle your prompts correctly, you can also improve the response you're getting from the LLM. So a lot of work has been done in prompt engineering, and there's a lot to read about here. So this is the first way you can improve the output of an LLM. \n",
    "\n",
    "The second is RAG. LLMs are trained on their own data; they have no idea about your personal data. But with the help of RAGs, you can show an LLM your private data and do question answering with it. So RAGs are becoming very famous. So RAGs themselves are a very large field of study. \n",
    "\n",
    "And third, fine-tuning. Fine-tuning also happens here, on the user side as well. Okay? So you'll find fine-tuning in both places. The fine-tuning you're studying on the builder side is a bit more technically advanced fine-tuning. There, you go to deeper layers and kind of fine-tune the model. Here, you do fine-tuning at a shallower level, but again, you can do it at the user level too. So you'll see many techniques here on the user side for fine-tuning. \n",
    "\n",
    "So, these are the three techniques, with the help of which you can improve the output of an LLM. \n",
    "\n",
    "\n",
    "After that, another very big field of study that is emerging is that of agents, creating AI agents. So, LLM applications are mostly used to build chatbots, etc. But with the help of agents, what you can do is you can create chatbots that not only interact with you but can also perform tasks for you. \n",
    "\n",
    "For example, you're having a conversation where you're trying to find out what the best tourist destinations in India are. The chatbot gives you an answer, let's say it says Goa. You can immediately tell it, \"Guess what, book a hotel for me there,\" and it will book it. So, that's the difference between a chatbot and an AI agent. So, what you're doing here is basically providing an LLM with some tools so that it can access these tools and perform various tasks. This entire concept where you're implementing chatbots plus tools to create an AI agent is currently very popular, and this entire field of study falls under the user side. \n",
    "\n",
    "Okay? Then comes LLMOps. I'm pretty sure you've heard this term a lot as well. So, the applications you're building using LLMs, obviously you're building them for your customers and clients, so you need to deploy them somewhere so that your customers can use them. So, in the process of building these LLM-based applications, and Whatever things you do in deployment, you perform evaluations, you perform improvements, you handle all the technical aspects in deployment—all of that combined, the umbrella term for all of that is called LLMOps. Now, many libraries have emerged to help you with this, so this is also emerging as a very important field of study.\n",
    "\n",
    "\n",
    "And lastly, there are some miscellaneous things.Such as using multimodal foundation models.  \n",
    "Until now, most of our discussion has been around LLMs, but what if you're working with a model that works on audio input or gives you video input/output? You can put these kinds of things under miscellaneous. You must have heard a lot about Stable Diffusion or diffusion-based models these days, so I'm putting all the learning related to those under miscellaneous. \n",
    "\n",
    "But yeah, mostly this is what we have on the user side. This is the curriculum that you have to follow in order to learn how to build applications using large language models. So yeah guys, we did it! The entire field of generative AI, which is growing so rapidly, and all the terms, tools, and technologies that are coming out, we have organized them into two parts: one is the builder side and the other is the user side. And now I have shown you the curriculum for both sides, and this is what we are going to cover.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624b3c8-db44-46dd-9008-aa058e2bfd89",
   "metadata": {},
   "source": [
    "# **Do we need to learn both**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image09.png\" alt=\"Do we need to learn both\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e2f2f-8c16-4e63-beee-daf448bcfb50",
   "metadata": {},
   "source": [
    "```\n",
    "For this, you should look at the diagram. \n",
    "So the scenario is that the builder side, where you create a foundation model and deploy it, is the job of a research scientist or a data scientist, right? \n",
    "\n",
    "Where you need some machine learning engineering skills and MLOps skills as well. Whereas this user side, where you use a ready-made LLM, I personally think that anyone can do it if they have even a little bit of software development knowledge. They can do 80-85\n",
    "percent of the work on the user side. \n",
    "\n",
    "That's why you'll see that building LLM-based applications means that they have been given a foundation model, an LLM, and they are building an application on top of that LLM. But what is special about this person is that they have a lot of knowledge of the builder side as well. That is, they know how foundation models are built. And having this knowledge is important because if you have this knowledge, you will be able to operate more effectively on the user side.\n",
    "\n",
    "So, undoubtedly, a software developer can easily build AI applications today, but a person who has knowledge of both fields will always be able to demand a better salary. And that is why you should learn both sides. But again, it depends. You can focus more on the user side if you want. If you want to become a research scientist, you can focus more on the builder side. But if you want to become an AI engineer, then you will have to work on both these fields simultaneously and in parallel. Okay? So the answer is yes.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e04b78-01ae-47b4-a037-c49e20c9ac78",
   "metadata": {},
   "source": [
    "# **How to cover**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image10.png\" alt=\"How to cover\" width=\"500px\">\n",
    "<img src=\"Images-of-Genai-Roadmap/Image11.png\" alt=\"How to cover\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670d45b-1c4b-46c8-9278-bb7b39758b7a",
   "metadata": {},
   "source": [
    "```\n",
    "The next question that might be coming to your mind is:\n",
    "\n",
    "Okay sir, that sounds good. We've divided the entire curriculum into two parts, we've organized everything. Now,\n",
    "\n",
    "\n",
    "How to cover GENAI?\n",
    "\n",
    "\n",
    "Tell me how you're going to cover this whole thing. So my answer is that I will cover both of these sides\n",
    "in parallel, okay? Because there isn't much overlap, so we can cover both things together. In fact, let me tell you my strategy. So if we focus on the builder side,\n",
    "\n",
    "on the builder side, I have planned that we have already studied the Transformer architecture, okay? I have covered this in quite\n",
    "detail in the Deep Learning playlist. I will teach about pre-requisites, encoder-only transformers, decoder-only transformers, and encoder-decoder based transformers, but I will discuss GPT and all of that there. Once this is done, then we will create another playlist on the pre-training part, then another playlist on fine-tuning, and another playlist on deployment. So basically, the idea is not to have one big playlist, but to have small playlists where we cover these small modules one by one. Calling them small is wrong, there's quite a lot in them, but yes, my plan is to create dedicated playlists on the builder side. \n",
    "\n",
    "\n",
    "Now let's talk about the user side. So on the user side, my plan is similar. Right now, I am planning to release a playlist on the \"Building Basic LLM Applications\" part by the end of this month, and then gradually I will cover prompt engineering, RAG, fine-tuning, etc. \n",
    "\n",
    "Okay? So again, the plan is to cover both sides in parallel and instead of creating one big playlist, to cover each topic in small playlists, because in a big playlist. The problem is a lack of commitment. But at least we'll cover things in small chunks.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c5c58-de86-4d88-a8dc-c0d9f9d2456b",
   "metadata": {},
   "source": [
    "# **FAQs and a Promise**\n",
    "<img src=\"Images-of-Genai-Roadmap/Image12.png\" alt=\"FAQs and a Promise\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c965a2-4bba-42b1-afaa-454029fb0411",
   "metadata": {},
   "source": [
    "```\n",
    "And some more questions might come to your mind.\n",
    "\n",
    "FAQs and a Promise:\n",
    "\n",
    "In fact, I get asked this question a lot, which is why I've included it here: Why am I not running a paid course?\n",
    "In a paid course, there are more focused classes, you can dedicate more time, and doubt clearing is possible.\n",
    "\n",
    "Why aren't you running a paid course? \n",
    "\n",
    "So I'll talk to you a little honestly. In the last year, I can't even count how many messages, emails, and comments I've received from people saying, \"Sir, please launch a course on Generative AI.\" And honestly, this was a huge earning opportunity for me. I mean, I could have earned a lot of money. In fact, not only this, many big EdTech companies approached me in the last six months to teach their Generative AI course for them.\n",
    "\n",
    "I couldn't do it, and the biggest reason behind that was that I feel that I haven't yet 100% mastered this technology. Okay? And since\n",
    "I haven't mastered it 100%, I feel that if I release a paid course, I won't be able to fully justify its price, and I don't want to give you a subpar experience. That's why I felt that YouTube is better because the content reaches a very wide audience, so you instantly start receiving feedback, and you can improve based on that. So my goal right now is to understand and learn this technology inside and out and teach you. So maybe in the future, I can have a paid course, but right now, my goal is to teach you. On I've already made four YouTube videos. You guys wait for my next video. In the next video, there will be another big announcement, and we will work accordingly. \n",
    "\n",
    "And the last question is what is the timeline? \n",
    "\n",
    "This entire curriculum that I showed you, the builder side, the user side, how much time do I think it should take to cover all of this? So again, giving you an estimate at this point is not possible for me because I haven't already created all the content. So it's a bit flexible. My goal will be to make two to three videos a week, and one video will be a long video, meaning it will cover a lot. So if I can give you a rough timeline, I think the entire year will be spent on this, which is not too much time, by the way. To master this entire thing, it will take at least a year for anyone. But what I can do is I can promise you that whatever content I release here And the rest, let's see how this whole thing turns out. \n",
    "\n",
    "I'm super excited because I have an inherent curiosity to learn this whole thing and become an expert in it. I also want to see how a technology can wipe out one trillion dollars in a single day from a stock market. Learning such a technology will truly be a very powerful experience, and I'm pretty sure you are also very excited.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d14114-39d8-409f-8335-94ede1a7e598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
