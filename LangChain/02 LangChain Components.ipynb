{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931ca16e-b6e7-44ee-9c55-6f22c2336aee",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "```\n",
    "Now, if you watch today's video completely, then going forward, you will get two benefits. \n",
    "\n",
    "First, you will get a very deep insight into how LangChain is organized as a framework. You will understand the thought process of the people who built this library.\n",
    "\n",
    "And the second benefit will be that going forward, all the videos I will be uploading in this playlist will be based on today's video.  That means, the components I have discussed in this video will be the basis for the detailed videos you will see in the future. So, by watching this video, you will get a kind of roadmap of what we will be covering in this playlist. Okay, now before starting the video, I would like to give a disclaimer that in today's video, we will not be doing any coding or building any projects. And the reason behind that is very simple. \n",
    "\n",
    "I told you before starting this playlist that generally, I have observed that in all the resources available online about LangChain, people start building projects directly without giving a proper foundation of LangChain. And I think that's not the right way to learn this library. I think the right way is to first get a conceptual overview of the library and then start coding. So, in this playlist too, I have planned the same. \n",
    "\n",
    "The first two videos, the previous one and today's video, will provide you with a theoretical foundation about the library. Once this foundation is set, we will do coding and build projects in all the subsequent lectures. That is why, in this video too, I have planned not to include any kind of code. But this doesn't mean that this video will be boring. I have made sure that while explaining each component, I give you very interesting and relevant industrial examples, and most likely, you won't find this video boring.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef946708-6548-4cdd-8324-172cc9c04cdf",
   "metadata": {},
   "source": [
    "# **Recap**\n",
    "```\n",
    "Now I think I've given you enough introduction about this video, let's start.\n",
    "\n",
    "Recap:\n",
    "\n",
    "Now guys, before starting the video, I would like to give you a quick recap of the previous video. The previous video was the first video in this playlist, and there I tried to explain to you in quite detail what LangChain is and why it is needed. Okay, so I told you there that LangChain is an open-source framework with the help of which you can build LLM-powered applications. Then we discussed why LangChain was needed. There I gave you an example that we want to build an application where anyone can interact with a PDF, and then I tried to explain to you through a system design how many components would be involved in building such a system, and there would be many interactions between those components. \n",
    "\n",
    "And if someone wants to code such a system from scratch, they will have to put in a lot of effort. And then I explained to you how LangChain comes into the picture, and the biggest advantage of this entire complexity is that LangChain can efficiently orchestrate all these components, build pipelines, and you get maximum output with very minimal code. I also told you that LangChain has the concept of chains, where you can string together different components in a chain, and the biggest beauty of chains is that the output of one component automatically\n",
    "serves as the input for the next component, and we don't need to code this manually.\n",
    "\n",
    "I also told you about another feature: how LangChain is a model-agnostic framework, which means that if tomorrow you want to use Google's Gemini models instead of OpenAI's GPT models, you don't need to change much code. Literally, it will be done in one or two lines of code. So I told you about all these benefits in the last video. I also explained what people are using LangChain for these days. \n",
    "\n",
    "I gave you examples of how many people are creating conversational chatbots for their companies, many are building AI knowledge assistants, and many companies have now jumped into creating AI agents. We covered all of this discussion in the last video, and I really hope you watched that video. If you haven't watched that video, I would recommend that you please go back and watch it.\n",
    "\n",
    "After watching that video, you'll be able to understand today's video a little better.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6e95a-4774-4fdc-9adb-60e8cae99e30",
   "metadata": {},
   "source": [
    "# **LangChain Components**\n",
    "<img src=\"Images/Images-of-Components/Image01.png\" alt=\"Components\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158a4e7-f8af-4a1c-9175-4fd94f8a6096",
   "metadata": {},
   "source": [
    "```\n",
    "So that was the recap. Now, let's move on to today's topic. Our topic today is that we want to learn about the LangChain Components.\n",
    "\n",
    "What are the different components in LangChain? Okay, so first of all, I would like to show you this diagram because in this diagram I have listed down all the components. So in total, LangChain has six different components, and if you understand these six components, trust me, you will understand the majority of the concepts of LangChain. Okay? \n",
    "\n",
    "So the first component is Models, the second is Prompts, the third is Chains, the fourth is Memory, the fifth is Indexes, and the sixth is Agents. \n",
    "\n",
    "Our goal in this video is to discuss each of these components in detail, one by one, and you will notice later in this playlist that we will be talking about these six components throughout this entire playlist. Okay? So now let's start with our first component, which is Models.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0f2af-382a-423a-8413-9b83a35f0b6a",
   "metadata": {},
   "source": [
    "# **Models**\n",
    "<img src=\"Images/Images-of-Components/Image02.png\" alt=\"Models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32c2b0-3b75-4000-b4ce-4a089147f75c",
   "metadata": {},
   "source": [
    "```\n",
    "So guys, Models\n",
    "\n",
    "We'll study the Models component in detail because it's the most important component of LangChain. Look here, it says, \"In LangChain, models are the core interface through which you interact with AI.\" Models. Now, I know you won't understand much from this definition, so to explain things better, let me give you a little backstory. \n",
    "\n",
    "So, in the entire history of NLP, there was one particular application that everyone wanted to build, and that was a chatbot. Chatbots are probably the most popular application in the world of NLP. Everyone wants to build their own chatbot in NLP. But there were two major problems in building chatbots. Not anymore.\n",
    "\n",
    "So, the first problem was making the chatbot understand the user's query. Right? I typed, \"Hi, can you check my email now?\"  Making the chatbot understand what I meant was the first big challenge in building a chatbot, and we call this NLU, Natural Language Understanding. This is the challenge. The second challenge is that even if the chatbot understands what I'm asking it, making it reply is also a big challenge. So, basically, context-aware text generation is also a big challenge. \n",
    "\n",
    "And a lot of effort was put into both of these challenges, a lot of attempts were made to solve them. Eventually, LLMs came along, and LLMs solved both of these problems together. So, to train LLMs, we used almost the entire internet's data, which resulted in not only the development of an understanding of natural language within the LLMs, but at the same time, they also gained the capability of context-aware text generation. \n",
    "\n",
    "So, what I mean is that after the arrival of LLMs, these two major problems of NLP were solved simultaneously. Okay? But a new problem came into the picture. The new problem was that since LLMs are trained on the entire internet's data, they have a lot of parameters, like billions Of the parameters that make the size of LLMs huge, like really huge, I mean, many of the good LLMs in the market have a size greater than 100GB. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06196e68-e1aa-4350-99ae-00d973ca9529",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image03.png\" alt=\"Models-server-API-Implementation\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb54ad-069d-4f6f-b9b6-60e33ecdac5b",
   "metadata": {},
   "source": [
    "```\n",
    "Now, the problem is that a normal person cannot run such large files on their computer. For that matter, even small companies cannot run them on their servers because the cloud bill would be so high that you wouldn't be able to pay it. So, this was the second major challenge that arose after the arrival of LLMs. This challenge was also solved with the help of APIs. So, what big companies like OpenAI and Google did was they kept the LLM files on their servers and created an API. Now, anyone in the world can hit these APIs, send their queries, and these APIs will communicate with the LLM.\n",
    "\n",
    "The LLM will give its response back, and that response will be received by the user. Now, what is the advantage of this model? I don't have to run the LLM on my computer or on my cloud services. I will send as many questions to the API as I need, and I will only have to pay for that usage. So, this problem was also solved. \n",
    "\n",
    "But now a third problem came into the picture, and that problem was implementation.  Different LLM providers wrote their APIs in different ways, which means if I am an application developer and I need to communicate with two different LLM APIs in my application, I will have to write different types of code. Let me show you something here. Pay attention here; two pieces of code are written here. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb7591-0f7e-4fd6-822a-b46245ba5192",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image04.png\" alt=\"Models-API-code-comparision\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3fb2c-1e78-48a2-9c25-91b0e14ff614",
   "metadata": {},
   "source": [
    "```\n",
    "This first code is for OpenAI. If I want to communicate with OpenAI's GPT model, with their API, this is the code for that. And this code is for the Claude API. If I want to talk to Anthropic's Claude Sonnet, this is the code to communicate with its API. And you can already see that there are slight differences, so if I need to use both LLMs in the same application for different purposes, I have to write two different types of code. \n",
    "\n",
    "Or, let's say I built an application, and initially, I built it using OpenAI, and I wrote this code. Now, suddenly, I don't want to use the OpenAI API anymore because it's expensive, and I want to use the Claude API. Then I'll have to change my code base and convert it, which is a lot of work. So, basically, standardization becomes a challenge. Gemini's APIs behave differently, OpenAI's APIs behave differently, and Claude's APIs behave differently. Even the response I'm getting back is of a different type. \n",
    "\n",
    "So, LangChain identified this problem and created a model component, which is basically an interface that allows anyone to interact with any company's API in a standardized fashion. This means you don't have to make many changes to the code, and you can immediately communicate with OpenAI, and with a few changes to the code, you can also communicate with Gemini. So, that's the model component. \n",
    "\n",
    "LangChain's model component is basically an interface that allows you to communicate with any company's AI model. In fact, let me show you the code. Look down here. These two pieces of code are from LangChain.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5eb6c-95cc-4af7-9b45-fbbcf91b4a65",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image05.png\" alt=\"Models\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image06.png\" alt=\"Models-API-code-Comparision\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8a753-e1fd-4479-8989-7dabeac7d8ce",
   "metadata": {},
   "source": [
    "```\n",
    "In this code, we're talking to OpenAI's API, and in the second one, we're talking to Claude. Now, go and see for yourself how much difference there is in the code. Here, you're just calling a different package.\n",
    "\n",
    "After that, the way you make the call is exactly the same.\n",
    "\n",
    "The way you print the result that comes back is exactly the same. You literally just change one line, in fact, just two lines, this line and this line, and your work is done. You can instantly switch from OpenAI to Claude.\n",
    "\n",
    "In fact, the answers you get back will also be very similar, so parsing them becomes very easy. So, in short, the Models component standardizes the entire interface for interacting with AI models. Okay? I hope you understand what the Models component is. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3cb567-f378-4fe0-ba54-ea2ab463c100",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image07.png\" alt=\"Models-llms-embedding-models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af263fa3-c9f5-4122-8e13-881856fcf9d5",
   "metadata": {},
   "source": [
    "```\n",
    "Now, one more thing I'll discuss here: in LangChain, you can communicate with two types of AI models. The first one is a Language Model, and the second one is an Embedding Model.\n",
    "\n",
    "Okay, let me quickly tell you about both of them. So, Language Models are basically LLMs, where you give these models a text input, such as \"How are you today?\", and they give you a text output in return, like \"I'm good, how about you?\".\n",
    "\n",
    "So, LLMs are basically Language Models that work on the text-in, text-out philosophy. Okay? With their help, you can build all sorts of applications. You can create chatbots, AI agents, you can do all of this with the help of Language Models. Embedding Models are slightly different. These are models that you give text as input, but they give you a vector as output. We discussed this in the last video. And their main use case is semantic search. I also told you about this in the last video. Okay? So, LangChain can communicate with both of these types of models. It can be done with language models and also with embedding models. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763a3e4-6e02-4077-9ee9-0f77cafbcd1f",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image08.png\" alt=\"Models-Chat-models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b75e8-a7c5-4c4a-9444-6a14f9e12ee5",
   "metadata": {},
   "source": [
    "```\n",
    "In fact, let me show you which language models are available in LangChain and which embedding models are available. So if you go to the documentation, this is the LangChain documentation. Here, you come to the chat models section, and you will see below all the providers you can communicate with using LangChain.\n",
    "\n",
    "There's Anthropic, Chat Mistral AI, Chat Azure, Chat OpenAI, Chat Vertex AI, Chat Bedrock (this is from AWS), Chat Hugging Face, and many more names you'll recognize. Not only that, it also tells you what features are available in those language models. For example, whether the tools calling feature is available or not (tools calling is useful when you create an agent), whether you get structured output from the model, whether you get output in JSON mode, whether you can run that model locally on your machine, whether you can provide multimodal input â€“ all of this is given here. So you should definitely go and study this list to see which language models are available in LangChain. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef18e7a-9acb-4d65-ae93-1ba25969bf57",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image09.png\" alt=\"Models-embedding-models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3494be1-98ba-4d16-b68a-55b495573304",
   "metadata": {},
   "source": [
    "```\n",
    "Secondly, this page tells you which embedding models are available in LangChain. And again, you will find a lot of famous names like OpenAI, Mistral AI, and IBM also has embedding models, Llama, etc. So I would recommend that you go and explore this page and read it a bit; it will help build your perspective. In a nutshell, if I can summarize, the model component is basically an interface that allows you to communicate with AI models, and the biggest problem this interface or component solves is that it addresses the previous limitations. Different APIs and LLMs were all doing their own thing. This particular component standardizes the whole process so that you can communicate with any LLM provider with just a few code changes. That's the whole idea behind this particular component. I really hope you understood this.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff858e5-8560-4341-87d0-b6a0294ee49a",
   "metadata": {},
   "source": [
    "# **Prompts**\n",
    "<img src=\"Images/Images-of-Components/Image10.png\" alt=\"Prompts\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6e122-93d4-44c6-9a16-807c1b0753c0",
   "metadata": {},
   "source": [
    "```\n",
    "LangChain's second component is Prompts\n",
    "\n",
    "Now, let's understand what prompts are. So, if you are working with an LLM, the input you send to that LLM is called a prompt. So, prompts are basically inputs provided to an LLM. For example, if you are working on ChatGPT and you ask ChatGPT the question, \"What is CampusX?\", this string, \"What is CampusX?\", can be called a prompt.\n",
    "\n",
    "Now, prompts are very, very important in the world of LLMs. The output of an LLM depends heavily on the prompts. In fact, it's very sensitive. This means that if you change the prompts even slightly, the LLM's output changes significantly.\n",
    "\n",
    "Let's take an example. Suppose you asked an LLM, \"Explain linear regression in an academic tone,\" and then you asked the same LLM another question, \"Explain linear regression in a fun tone.\" You will notice that you only changed one word, but the output of your LLM will be very different. So, prompts are super important.\n",
    "\n",
    "In fact, in the last two years, a field of study has emerged around prompts, and you will find many jobs related to it. This field is called prompt engineering, and the job profile is called prompt engineers. Although it has a somewhat negative reputation on social media right now, trust me, it's an important field of study around LLMs. So, LangChain also identified this.\n",
    "\n",
    "Since LLMs help in building LLM-powered applications, and LLMs depend heavily on prompts, we need a very good component to handle prompts. And then they developed the prompts component. LangChain gives you a lot of flexibility in creating prompts. In fact, you can create many different and powerful types of prompts. You can do this in LangChain. Let me give you some examples.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad41d6-1d51-4ed4-87ec-7a7ea3990fe4",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image11.png\" alt=\"Dynamic-Reusable-prompts\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d8771-c57d-4ff1-a39f-7cc36e11f5f7",
   "metadata": {},
   "source": [
    "```\n",
    "Let's look at the first example here. What you can do is, with the help of LangChain, you can create very powerful, dynamic, and reusable prompts. For example, you don't know beforehand which topic the user will ask you to summarize and in what tone. So, what I can do is, I can create a dynamic prompt where my prompt is \"Summarize this topic in this tone.\"\n",
    "\n",
    "I've simply put some placeholders here. I haven't specified the topic yet, nor the emotion. Now, what I'll do is, I'll ask the user. The user will tell me, \"Tell me about cricket in a fun tone.\" So now, what I can do is, I can quickly replace this placeholder with \"cricket\" and this placeholder with \"fun,\" and now I'll send this prompt to my LLM. Similarly, tomorrow another user might come and use the same prompt for, let's say, biology in a serious tone, right? So this is how you can create dynamic and reusable prompts in LangChain. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f7b1b-f7ad-4f84-a907-731a5431c305",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image12.png\" alt=\"Role-based-prompts\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6e5a4-74fa-4586-b495-8b92845945ac",
   "metadata": {},
   "source": [
    "```\n",
    "Besides this, if you want, you can also create role-based prompts. So what you can do is, you first create a system-level prompt where you write a message like, \"Hi, you are an experienced...\" and you put a placeholder for the profession. The user will tell you the profession later. And then you create a user-level prompt where the user says, \"Tell me about this topic.\" Now, in the future, a user will come and say, \"You are an experienced doctor,\" so this will be filled in here, and the topic will be \"viral fever,\" so that topic will come here. \n",
    "\n",
    "So, in this way, you are guiding your LLM, telling it, \"You are an experienced doctor, and now, as an experienced doctor, tell me about viral fever.\" Similarly, tomorrow another user will come and say, \"Hi, you are an...\" Experienced engineer, tell me about developing bridges. I don't know anything like that. So you can also do role-based prompting using LangChain. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad033a-c715-4ed8-9673-92e61fb2e00a",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image13.png\" alt=\"Few-shot-prompting\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image14.png\" alt=\"Few-shot-prompting\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image15.png\" alt=\"Few-shot-prompting\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f35bf-af47-493d-9607-9d32ae11cb3f",
   "metadata": {},
   "source": [
    "```\n",
    "The next example is also very interesting. You can do few-shot prompting using LangChain. So what is few-shot prompting? You show your LLM some examples first and then ask it a question. For example, let's say you are building a chatbot for customer support. So what will you do first? You will show your LLM some examples of previous messages and tell it what type of message it is. \n",
    "\n",
    "For example, here it says, \"I was charged twice for my subscription this month.\" This is a billing issue. I have already told the LLM this. I gave another example: \"The app crashes every time I try to log in.\" This is a technical issue. \"Can you explain how to upgrade my plan?\" This is a general inquiry. So, in this way, I gave examples and told it what the output should be. \n",
    "\n",
    "Now what I'm doing is creating a template where I will ask the LLM questions in this format. This will be my ticket, as in, my query, and the output should be the category. Okay? And now I will create a few-shot prompt template where I will give all my examples. \n",
    "\n",
    "I will give my example prompt where, I'll provide my example template, and now what I'll do is send a new query and ask, \"Based on the previous examples you've seen, tell me which category this new example falls into.\" So now, based on this, \"Classify the following customer support tickets into one of the following categories: Billing Issue, Technical Problem, or General Inquiry.\" This is my Example One, this is my Example Two, this is my Example Three, this is my Example Four, and this is my final query that my user asked me at this point. And now the category will be printed here by my LLM. Okay? \n",
    "\n",
    "So this is a very excellent few-shot prompt template. Okay? So you can easily create this using LangChain. Now, at this point, if this whole code seems a little difficult to you, because obviously you haven't studied all these things, then don't worry, I just wanted to show you the concept of how many different types of prompting techniques you can implement in LangChain with the help of the prompts component. So again, a very important component, and in the future, we will do one or two dedicated videos on this.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60767917-20e2-49c7-b8ea-5d596d34132b",
   "metadata": {},
   "source": [
    "# **Chains**\n",
    "<img src=\"Images/Images-of-Components/Image16.png\" alt=\"Chains\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231e56-b702-42d2-832d-19990b2b8132",
   "metadata": {},
   "source": [
    "```\n",
    "The next component of LangChain is called Chains.\n",
    "\n",
    "Chains: \n",
    "\n",
    "And this is such an important component that LangChain is named after it. Okay, so let me explain it to you. What are chains and what can they do? So, chains are basically a component that allows us to build pipelines in LangChain.\n",
    "So basically, the idea is that you can take any LLM application you build and give it the shape of a pipeline, right? \n",
    "\n",
    "And that pipeline can be built using chains. Okay, let me explain it with an example. Let's say you want to build an LLM application where the user gives you a large English text as input, around 1000 words, and in the output, you need to provide a Hindi summary of it in less than 100 words. This is the LLM application you need to build. \n",
    "\n",
    "Okay, so you decided that the flow of this entire LLM application will be that you will first receive the input. You will first send this input to an LLM, and the job of this LLM will be to translate this input into Hindi. So, translation is happening here. After that, we will send this translated text to a second LLM, and this LLM will generate a Hindi summary of it in less than 100 words.\n",
    "\n",
    "So, you have decided this flow for your LLM application. Now you can easily represent this flow in the form of a pipeline.\n",
    "\n",
    "Okay? Now, if you don't use chains, what you will have to do is design this entire pipeline manually. Which means you will take input from the user, then you will call this LLM, provide the input to it, and tell it to translate it into Hindi. You will get the Hindi translation, and then you will take that, go to another LLM, and tell it to generate a summary of this Hindi translation in Hindi, and Then you will get your final output.\n",
    "\n",
    "So, if you design this entire pipeline manually, you have to manually extract the output from each stage and feed it into the input of the next stage. Chains solve this problem. What do chains do? The biggest advantage of chains is that they automatically make the output of one stage the input of the next stage, and you don't need to write any manual code for this. Which means, if you build this entire task using chains, you simply need to provide an English text here and call the chain, and behind the scenes, all these tasks will be executed automatically, and you will directly get your result here. You don't need to worry about manually feeding the output of LLM 1 into LLM 2. Chains are doing all this heavy lifting behind the scenes. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf67c78-e072-4846-bd3e-a21a81b01da9",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image17.png\" alt=\"Chains-sequential-chain\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image18.png\" alt=\"Chains-sequential-chain\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image19.png\" alt=\"Chains-sequential-chain\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d7982-81cc-4865-8216-2094e3f5358d",
   "metadata": {},
   "source": [
    "```\n",
    "So, in simple terms, a chain is this concept with the help of which you can create a pipeline in LangChain, and the biggest beauty of this pipeline is that the output of the previous stage automatically becomes the input of the next stage. You don't have to write manual code for it. Okay? Now, what's even better is that the chain I showed you is a simple sequential chain where one stage follows another.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e13fff-1723-4eaf-93aa-ee9a620cca98",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image20.png\" alt=\"Chains-Parallel-chain\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8a336-62c6-481c-b729-58a5ddbaecc2",
   "metadata": {},
   "source": [
    "```\n",
    "But you can create many more complex chains with the help of chains. Okay? I'll actually give you some examples. For example, one pipeline you can create is a parallel chain. I will write it: Parallel Chain. So, in this, let's say you want to create an application where the user gives you an input, and you have to print a detailed report based on that input and give it to the user. For example, if the user writes \"9/11 incident,\" a detailed report will come in your output. Now, what you want to implement is that you want to combine the output of multiple LLMs. You want to demonstrate it, right? Okay.\n",
    "\n",
    "This means your flow will be something like this: you'll receive input, and you'll send that input to LLM One.\n",
    "LLM One will then generate a report on that topic.  Then you're simultaneously sending the same input to LLM Two, and LLM Two is also generating a report on it. Then you're sending it to a third LLM, and you're asking it to combine those two reports. And then you're showing the combined result to the user as output. Now, this is an example of a parallel chain, where you're executing things in parallel. So you can easily execute this entire flow with the help of chains. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb5e05-6968-410c-934a-a6d832997d83",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image21.png\" alt=\"Chains-conditional-chain\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image22.png\" alt=\"Chains-conditional-chain\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429488d-3346-4c4e-b867-970bd16bcf11",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, let me give you another example. Another example is conditional chains, where you can perform different types of processing based on a condition. For example, let's say you're building an AI agent that receives feedback from the user. So you're asking the user how they liked your service. The user tells you what they thought of the service.\n",
    "\n",
    "Now you're processing this feedback with the help of an LLM. If the feedback is good, you thank them, and the task is done. But if the feedback is bad, you immediately send an email to your customer support team.\n",
    "\n",
    "You're sending it now. This is also an application that you can easily build, and the processing here is happening based on a condition, and this can also be very easily implemented with the help of chains. Okay? \n",
    "\n",
    "And there are countless other complex applications that can be built using LangChain. It's very beautifully designed and minimizes a lot of your hard work. Later on, we will study chains in a little more detail. Okay?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55679d64-e2a5-48da-bb39-e1178e381d21",
   "metadata": {},
   "source": [
    "# **Indexes**\n",
    "<img src=\"Images/Images-of-Components/Image24.png\" alt=\"Indexes\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-Components/Image28.png\" alt=\"Indexes\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ccf3a9-18c0-4440-8409-75efc6f04e5b",
   "metadata": {},
   "source": [
    "```\n",
    "The next LangChain component is Indexes, and honestly, reading about indexes will seem a bit simple because I explained this topic in great detail in the previous video. But nevertheless, let's read its definition. \n",
    "\n",
    "Indexes:\n",
    "\n",
    "So, it says here that Indexes connect your application to external knowledge such as PDFs, websites, or databases. Okay, now before discussing what indexes are, I want to tell you what all comes under indexes. So, there are four things that come under indexes. The first is the Document Loader, the second is the Text Splitter, the third is the Vector Store, and the fourth is the Retrievers.\n",
    "\n",
    "These four things together make up indexes. Now let's have a detailed discussion about what indexes are and why they are needed. So, you use ChatGPT for all your queries, right? And ChatGPT mostly gives you answers to all your queries because it has been trained on data from the entire internet, so it knows most things. But there are certain scenarios where ChatGPT will not be able to answer you. \n",
    "\n",
    "For example, I work at a company called XYZ. Now, if I go to ChatGPT and ask ChatGPT, \"Can you tell me what is the leave policy of my company XYZ?\" or if I go and ask, \"What is the notice period policy of my company XYZ?\" Do you think ChatGPT will be able to answer this question? The answer is no. Since these questions are asked about my company's private data, ChatGPT will not be able to answer it because ChatGPT did not see this data during its training. \n",
    "\n",
    "So, this is a problem we all face: we cannot ask ChatGPT questions related to the things we are doing in our private and professional lives because obviously, ChatGPT doesn't know about them. \n",
    "\n",
    "So, is there a solution? The answer is yes. What you can do is connect your LLM to an external knowledge source. For example, I can take an LLM and provide it with the entire rulebook of my XYZ company. Okay? Now, if I ask a normal question like, \"Who is the Prime Minister of India?\", the LLM will answer it because it has seen all this during its training. But if I ask, let's say, \"What is the leave policy of XYZ company?\", it will also answer that because it now has this external knowledge source. Okay? \n",
    "\n",
    "So, indexes are used to build these kinds of applications, and its four main components are listed here. Okay? Now, the good thing is that I showed you a system like this in the last video. I'll show you the same example again. So, generally, how is this type of system implemented? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04999b-5ef1-4df8-877f-a0c8e64bd938",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image25.png\" alt=\"Indexes-document-loader-text-splitter-embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41baaee1-f60d-47b2-b99e-d5c11f06ed4e",
   "metadata": {},
   "source": [
    "```\n",
    "First, you take your PDF, which in our case is our company's rulebook. Since it's very large, let's say it's 1000 pages long, and it's a very large company, so there are many rules. So, first, you have to load that rulebook from wherever it's stored, let's say it's stored on Google Drive. You load it from there, and the job of loading is done by the document loader. Okay? \n",
    "\n",
    "So, wherever you need to get the data from, the document loader will help you. Now, once you have the data, once you have this PDF, what you do is, so that semantic search can be performed on this document, you break this entire document into small chunks, based on pages, paragraphs, or chapters. Like we're doing it based on pages, so we've created 1000 chunks from the 1000 pages. This chunking operation is being done by our second component. The component is a text splitter. \n",
    "\n",
    "After that, we convert our document into vectors so that semantic search can be performed. So, for each of our pages, we will create embeddings using some embedding model that we discussed a little while ago. So now I have 1000 embedding vectors corresponding to 1000 pages. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b78d1-0af6-41e8-8cc2-542b4f839c76",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image26.png\" alt=\"Indexes-Database\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7986ef9-a951-4c57-806a-0afc6e7456c2",
   "metadata": {},
   "source": [
    "```\n",
    "Now, obviously, since I can search anytime, I can do it today, I can do it tomorrow, I can do it 10 days later, so I will have to store these vectors somewhere. So I store these vectors in a database, and since we are storing vectors in this database, this special database is called a vector database or a vector store, which is our third component. Okay? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a08cd-70a8-49f9-89e5-eef69250d352",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image27.png\" alt=\"Indexes-Retriever\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea1746-742f-46e7-aaa4-ee648d99d265",
   "metadata": {},
   "source": [
    "```\n",
    "So now I have my entire embedding vector stored in a vector store. Now what happens is, as soon as a user's query comes in, like \"Tell me what is the leave policy of XYZ company?\", a fourth component comes into the picture, which is called the retriever.\n",
    "\n",
    "The retriever quickly takes this query and generates its embedding with the help of an embedding model, and then, using the vector we get, it performs a semantic search in this database, and from the semantic search, it gets relevant results, and those relevant results\n",
    "\n",
    "It takes the text and the user's query, gives it to the LLM, and the LLM then replies back to you. This entire process is executed through indexes, and these four components in the indexes do all the heavy lifting. So, in simple terms, indexes are the way using which you can build LLM applications that have access to external knowledge sources. Okay?\n",
    "\n",
    "Now, here, the external knowledge source can be anything. It can be PDFs, it can be a website, or it can be a company's database. It's completely flexible here. Okay? \n",
    "\n",
    "So, later in this playlist, we will show you all these things. Okay? I will practically show you by building proper projects how all this works. And trust me, because of LangChain, this entire process becomes very easy. So, you don't need to write a lot of code. So, I really hope you got a little idea about indexes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76b28b-ed61-4a79-9b8f-ba78989459fc",
   "metadata": {},
   "source": [
    "# **Memory**\n",
    "<img src=\"Images/Images-of-Components/Image29.png\" alt=\"Memory\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6615d5-e364-4196-abcb-0c1eadda9039",
   "metadata": {},
   "source": [
    "```\n",
    "The next component of LangChain is Memory.\n",
    "\n",
    "Memory:\n",
    "\n",
    "You might already have some understanding from the name, but let's discuss it in a little more detail. Here I've written a statement; let's read it and try to understand it. It says, \"LLM API calls are stateless.\" And this is a big problem. When you build LLM-based applications, you'll understand that when you make API calls to an LLM, all those calls are stateless. What does this mean? \n",
    "\n",
    "Let me explain with an example. Let's say I'm using an LLM API, for example, I'm using the OpenAI API and interacting with the GPT model. I sent this question to the model: \"Who is Narendra Modi?\" The model quickly replied, \"Narendra Modi is an Indian politician who is the current Prime Minister of India.\" \n",
    "\n",
    "Okay, now what I did was, I went into my code and changed my query, and there I wrote, \"How old is he?\" Which means I'm asking about his age. Now I hit the same thing on the API, and guess what? This is the result I'm getting. It says, \"As an AI, I don't have access to personal data about individuals unless it has been shared with me.\" Which means it doesn't remember the previous question about Narendra Modi.\n",
    "\n",
    "So here, it's not able to decode \"he.\" Now I guess you understand the meaning of this sentence and the meaning of this word, \"stateless.\" Stateless means that whenever you make calls or send requests to the LLM API, each request is independent.  It has no memory of the previous requests. And this is a big problem. \n",
    "\n",
    "Now think about it, if you build a chatbot with this kind of system, how frustrating it would be to talk to that chatbot, because it... The conversation won't have any memory; you'll have to remind it every time what you were talking about, and this is a huge problem when you're building LLM applications. This is the problem that LangChain's component called Memory solves.\n",
    "\n",
    "So, with the help of Memory, you can add the memory feature to your entire conversation. Okay, now this is a slightly advanced topic, but I'll still tell you a little bit about the different types of memories available in LangChain. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae142d0-44c1-4feb-9711-e6f460c08572",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image30.png\" alt=\"Memory-types\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422b532-ca99-413f-90a6-a49a28376cb1",
   "metadata": {},
   "source": [
    "```\n",
    "There are many different types of memories, but the most frequently used memory is Conversation Buffer Memory. Here, what you do is, as you're talking to your chatbot, you store all the conversation that has happened so far, and when you make the next API call, you send this entire chat history to the model. Then it understands what the conversation is about. Okay, the only problem with this is that if your chat becomes very long, then this chat history will also become very long, and processing so much text will cost you a lot of money. So, this is one type of memory. \n",
    "\n",
    "The second is Conversation Buffer Window Memory. Here, you store the last N interactions. For example, you might say to store the last 100 messages. This constantly updates, and at any point, you have the last 100 interactions with you and send them in the next API call. \n",
    "\n",
    "Okay, then the third memory is Summarizer-based Memory. Here, what you do is you generate a summary of your entire chat history so far, and you send that summary in your API call. This allows us to save some text and pay a little less money.  \n",
    "\n",
    "Besides this, there is another memory called Custom Memory, where for more advanced use cases, you store very specialized pieces of information in memory, such as the user's... There are certain preferences, and there are some facts and figures about them, which you always keep in mind. This makes it easier for you to have conversations later on. Okay, so this is a very interesting topic, and honestly, a very practical topic. We've covered the theoretical aspects that we needed to cover, and going forward in this playlist, we will discuss this in much more detail.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593a24f-5fc0-4833-9416-c2a9790585be",
   "metadata": {},
   "source": [
    "# **Agents**\n",
    "```\n",
    "Now let's talk about the last component of LangChain, which is Agents. Agents are a component in LangChain that allows you to easily create AI agents. Now, if you consider the last six months, I'm pretty sure you've definitely heard about AI agents somewhere, sometime. Everyone is saying the same thing: that AI agents are going to be the next big thing. Which might be true, but okay, whatever it is,\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bdc07-0540-4eaa-a32b-53dda9c0acb4",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image31.png\" alt=\"Agents\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c4e76-cf57-4496-9442-47806d6962b8",
   "metadata": {},
   "source": [
    "```\n",
    "let's try to understand fundamentally, from scratch, what AI agents are and how you can build them in LangChain. So we've discussed multiple times that LLMs have two very big strengths: the first is NLU (Natural Language Understanding) and the second is text generation. This means that LLMs understand language and, after understanding it, can generate the correct text in response. So obviously, when LLMs came into the picture, the most obvious use case for LLMs was creating chatbots, and people created a lot of chatbots using LLMs, right? \n",
    "\n",
    "In fact, today's most popular AI application, ChatGPT, is itself a chatbot, right? So gradually, people started thinking that if my chatbot can understand what I'm saying and reply, then it can also perform some tasks. Let me explain with an example. \n",
    "\n",
    "Let's say I'm talking to a chatbot on a travel website, like MakeMyTrip. Let's say I asked that chatbot, \"Can you tell me what are the best travel destinations in India during the summer?\" Since their chatbot was based on an LLM, and that LLM was trained on data from the entire internet, its training data would contain information that during the summer, hill stations are the best tourist spots. So it quickly replied, \"You can go to Shimla or you can go to Manali.\" \n",
    "\n",
    "Now, what if instead of this chatbot, it was an AI agent? If I had one, I could get it to do some tasks for me. For example, I could immediately ask it this question: \"Can you tell me which is the cheapest flight on January 24th between Delhi and Shimla?\" And what this AI agent would do is instantly hit an API and retrieve the answer, saying, \"Okay, this Indigo flight is the cheapest on January 24th from Delhi to Shimla.\" Now, I can take it a step further and tell it, \"Can you book the flight?\" And since it has this capability, it will quickly go to the MakeMyTrip website and book the flight for me. And this is the main difference between a chatbot and an AI agent.\n",
    "\n",
    "It's like an AI agent is a chatbot with superpowers. A chatbot can talk to you, but an AI agent can also perform tasks for you. And I'll tell you how this works. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3f48c-92e8-497e-b25c-59dca0b5a520",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image32.png\" alt=\"Agents-capabilities-explanation\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586115a-9d27-4735-9a2b-fcb2349409f6",
   "metadata": {},
   "source": [
    "```\n",
    "So, an AI agent basically has two things that a chatbot doesn't. First, it has reasoning capabilities, and second, the AI agent has access to certain tools, such as being able to hit this API and find out which is the cheapest flight. So, the AI agent has access to various tools. Okay? With the help of these two capabilities, an AI agent can perform tasks for you. Okay? So, let me explain how an AI agent works through an example. Okay? \n",
    "\n",
    "So, let's say we have an AI agent that we created, and we gave it two tools. The first tool is a calculator; meaning, whenever my AI agent needs to perform any mathematical calculation during any conversation, it can quickly use this tool. And secondly, I've given it access to a weather API so that if it needs to find out the weather conditions of any city in the world on any given date, it can do so. These are the two tools I've given to my AI agent. \n",
    "\n",
    "Now, a user comes along and starts talking to my AI agent. And he asks the question, \"Can you multiply the temperature, or rather today's temperature of Delhi by three?\" This is the query. The user asked, \"Can you multiply today's temperature of Delhi by three and tell me the answer?\" \n",
    "\n",
    "Now, let's see how this AI agent will handle this query. So, since it has reasoning capabilities, it means it can reason about exactly what it needs to do. Okay? So, reasoning happens through different techniques here. There are many popular techniques, and one of the very popular techniques is Chain of Thought prompting, where your AI agent will break down your query step by step and then try to reason through it. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9933669-7e73-430f-a2bb-5348384af288",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-Components/Image33.png\" alt=\"Agents-workflow\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea6f37-d3fc-4171-95f4-f22a0a9a0c6e",
   "metadata": {},
   "source": [
    "```\n",
    "For example, as soon as the AI â€‹â€‹agent reads this query, \"Can you multiply today's temperature of Delhi by three?\", it will first break down this query step by step. It will say that I need to multiply today's temperature of Delhi by three, which means I need today's temperature of Delhi. And as soon as I get the temperature, I can multiply it by three. Okay? \n",
    "\n",
    "Now its entire focus will shift to finding today's temperature of Delhi. So it will go and check if it has any tool that can tell it the temperature of Delhi. It will notice that yes, it has a weather API. So it will quickly hit the weather API and give the input \"Delhi\" to the weather API.\n",
    "\n",
    "It will respond with the result, saying 25 degrees Celsius. Now your agent will say, \"I have the temperature of Delhi, and I need to multiply this number, 25 degrees Celsius, by 3. But to perform this operation, I need a calculator.\" So it will go back and check its tools to see if it has access to a calculator. The answer is yes.\n",
    "\n",
    "So it will quickly call the calculator and give it the inputs 25, 3, and the multiplication operation. These three inputs will go into the calculator. The calculator will respond with 75, and this 75 will finally become your output. And this is how AI agents work.\n",
    "\n",
    "Okay, so basically, to summarize, the only difference between an AI agent and a chatbot is that an AI agent has reasoning capabilities and access to tools.\n",
    "\n",
    "So an AI agent is just an evolved form of a chatbot where you can perform some actions with the help of AI agents because it has reasoning capabilities and access to tools. Okay? \n",
    "\n",
    "I hope you understood. It's a very interesting topic, and it seems like all the big companies and all the good researchers in the world of AI are converging on this topic. And I'm pretty sure that in the next one to one and a half years, there will be a lot of progress on this front. So we will discuss this in quite a lot of detail.\n",
    "\n",
    "LangChain has made it very easy to create AI agents, and we will show you how to create an AI agent. So with that, this concludes our introduction to all the components. \n",
    "\n",
    "Now we will study each of these components in detail, one by one. For example, in our next video, we will look at the first component, the models, in quite a lot of detail. Okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cce12e-d4cf-4936-9052-10f3c6aae93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
