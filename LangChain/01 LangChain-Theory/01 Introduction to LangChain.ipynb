{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92871411-7624-4b96-baea-41f63ca152a6",
   "metadata": {},
   "source": [
    "# **What is LangChain?**\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image01.png\" alt=\"What is LangChain?\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abff248-e986-44ad-b36a-5da7362cbb60",
   "metadata": {},
   "source": [
    "```\n",
    "So let's start our discussion around LangChain. First and the most obvious question would be, what is LangChain?\n",
    "\n",
    "What is LangChain?\n",
    "\n",
    "So I would like to answer this question through this single sentence: LangChain is an open-source framework for developing applications powered by LLMs. \n",
    "\n",
    "So if you want to build any LLM-based application, the framework that helps you in building that application is LangChain. Okay?\n",
    "\n",
    "Now, you won't understand the importance of LangChain from this single sentence. I truly feel that whenever you want to learn anything, you should first understand why that thing was needed in the first place. Okay? \n",
    "\n",
    "If you want to study what is x, you should know why x was needed in the first place. \n",
    "So I'm going to do the same. To explain what LangChain is, I will first explain why LangChain was needed. Okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc93db-ad56-4ebd-929e-c993e1bb9546",
   "metadata": {},
   "source": [
    "# **Why do we need LangChain?**\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image17.png\" alt=\"Why do we need LangChain?\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc612a-2a0a-4265-ad9c-370a5710f240",
   "metadata": {},
   "source": [
    "```\n",
    "Why do we need LangChain? \n",
    "\n",
    "So let's move on to the next slide. And let's discuss why we need LangChain. Why do we need this framework? And trust me, I'll take a slightly longer route to prove this point, but trust me, by the time this discussion is over, you will have a very deep perspective and insight into how important LangChain truly is.\n",
    "\n",
    "Okay, so I'm going to give you an example from my own life. I've been working in the startup world for quite some time, and I've started my own startups and explored many ideas over the past 10 years. So, around 2014, an idea came to my mind, and at that time, there wasn't much buzz about AI, at least not in India.  At that time, an idea came to me that people had started reading a lot of PDFs. Before that, people used to read more books, but around 2014-2015, smartphones had become quite prevalent, so people started reading a lot of PDFs.\n",
    "\n",
    "So, an idea came to my mind: what if I create an application where anyone can upload their PDFs and then read those PDFs? Obviously, it's a PDF reader, so they can read it. And not only can they read it, but at the same time, our application will have a chat feature, and by clicking on that chat feature, you can talk to your PDF. For example, let's say I've uploaded a machine learning book here, and this machine learning book covers the entire field of machine learning.\n",
    "\n",
    "So, what I can do is not only read this book, but at the same time, I can ask various questions through the chat. For example, I can ask, \"Explain page number five as if I were a 5-year-old child,\" and the chatbot would give me a very simplified, summarized version of page five. Then, I might also ask, \"Generate some true/false questions based on the linear regression we've studied so that I can practice.\" Or I could ask this question: Please generate notes on the decision tree we studied in this book. \n",
    "\n",
    "So I hope you understand that this kind of application can be incredibly useful, right? Because not only can you read, but at the same time, you can interact with your book. So in 2014, it occurred to me that this is a great idea, and maybe I should work on it. And guess what? I did work on it. \n",
    "\n",
    "So now I'll tell you next how I planned to build this application and what the system design was. How I was planning to technically execute this whole thing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe84922-d4d2-43aa-8770-8ee0f245fab4",
   "metadata": {},
   "source": [
    "# **High Level Overview**\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image02.png\" alt=\"High level overview\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4159a7b-5f17-40ae-b2d7-d13cc348268c",
   "metadata": {},
   "source": [
    "```\n",
    "Okay guys, let's have a high-level discussion on how we can build this kind of application.  Once you understand the high-level concept, we'll discuss the details.\n",
    "\n",
    "So, how will this entire system work? As soon as a user uploads a PDF to our application, we will first take that PDF and store it in our database.Now, the user opens that PDF and asks a query, a question. Let's say the question is: \"What are the assumptions of linear regression?\" So, what we need to do is first find out where in the entire book this topic is discussed, on which pages this topic is covered.\n",
    "\n",
    "Essentially, we need to perform a search operation. But there can be two types of search operations. One is a normal keyword search. In a normal keyword search, you would take these words, like \"assumptions\" and \"linear regression,\" and search for them as they are throughout the entire book. Wherever the word \"assumptions\" is used, or wherever the word \"linear regression\" is used, we will retrieve all those pages. Now you can understand that this is a bit inefficient, right? We need contextual results about \"What are the assumptions of linear regression,\" right? But in our case, many other pages are coming up. The word \"assumptions\" might appear on many pages, so we are getting many unnecessary pages as results. That's a bit inefficient. So, instead of doing a keyword search, we will do a semantic search. Semantic search is a bit different. I'm sure you've studied it in NLP. In semantic search, you try to understand the meaning of your query. You will try to understand that the user wants the assumptions of linear regression. Now, instead of searching the entire book for \"Assumptions\" you'll specifically search for \"Assumptions of Linear Regression.\"\n",
    "\n",
    "So automatically, you'll get results showing only the pages where \"Assumptions of Linear Regression\" is discussed.  This will result in fewer pages, but you'll get more meaningful results. Okay? \n",
    "\n",
    "So, what we did was, we took the user's query and performed a semantic search across our entire document. Let's say we got two pages: Page number 372 and page number 461. These two pages discuss the assumptions of linear regression. Okay? \n",
    "\n",
    "So, these results are now available to us. Now, what we'll do is take these pages that we got in the results and the user's original query, and combine them to create a system query. Okay? \n",
    "\n",
    "And we'll send this system query to the most important component of our application, which I'm currently calling the \"brain.\" Okay? \n",
    "\n",
    "Now, this brain has one purpose, actually two purposes. The first purpose is to understand this query, to understand its meaning properly. So, it should have NLU capabilities, Natural Language Understanding capabilities. Our brain should have this, which means if the query is in English, it should understand English; if it's in Hindi, it should understand Hindi. But it should be able to understand the text or the query properly so that it understands what needs to be done. Secondly, the brain should have context-aware text generation capabilities. Which means, essentially, what does it need to do? The job of our brain is that it's given this query and this document (two pages), and it's told to understand this query and extract the answer from this document. So, to understand this query, it needs NLU capabilities, and then, after understanding it, to find the relevant answer from here and generate the answer, it needs context-aware text generation. Right? \n",
    "\n",
    "So, basically, this brain of our system has only two purposes: to properly understand the user's query... Understanding it, and then, using the pages we provide, it will delve into those pages and generate context-aware text. Okay? \n",
    "\n",
    "So, it will read these pages and then extract the five components of linear regression, generate text from them, and give it to us. And that's our final output. So, this is how this entire process is working. Okay? \n",
    "\n",
    "Now, you might have a couple of questions in your mind here: Why did we do all this semantic search if my brain already understands the user's query and can find and generate the answer from the given pages? Couldn't we just give it the entire book directly? Let's say this book has a thousand pages.\n",
    "\n",
    "Why bother with all this semantic search and extracting these two pages? Wouldn't it be better to just send the thousand-page document here and tell it that the user asked this question, read this book and give the answer? \n",
    "\n",
    "Now, think about the problem here. It's a very simple problem. Imagine you're in school, and you have a doubt in your math book. So, you go to your teacher and you tell him, \"Sir, here's the book, I have a doubt in algebra.\" Okay? Now, this is scenario one. Scenario two is that you go to your teacher and specifically tell him, \"Sir, I have a doubt on page number 155.\"\n",
    "\n",
    "So, in which scenario do you think the teacher will be able to give you a quick and good response? Obviously, in the second scenario, because you gave him a single page and said, \"I have a doubt on this page,\" rather than giving him the entire book. The same scenario applies here. You gave your system's \"brain\" two very specific pages and a user query, and you told it, \"Look, read this query, understand it, and find the answer from these two pages.\" Now, if you give it the entire book, it will be computationally more extensive, and secondly, the results you get might not be as good. That's why we implemented this entire semantic search feature. So, this is a high-level overview of our entire system.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d754d-10c0-4054-8fa9-66633cb24c72",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-langchain-Intro/Image03.png\" alt=\"Kohli Explanation\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d841a9-d6e1-40fb-acb9-331e3f9ae0e9",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's take a deeper dive and understand exactly how this system can be built in detail. Now, to understand this system at a lower level, we first need to understand semantic search. A little while ago, I told you that in semantic search, you search based on the meaning of any text. Okay, so let me tell you exactly how semantic search works. The idea is very simple. Let's say you have three paragraphs about three cricketers: Virat Kohli, Jasprit Bumrah, and Rohit Sharma. And now you'll be asked a question, and you have to answer that question from within these three paragraphs.\n",
    "\n",
    "So you have to find out which paragraph contains the answer. Okay? So how will this whole thing work? Let's say the question asked is, \"How many runs has Virat scored?\" We know that the answer is hidden in the paragraph about Virat Kohli. But how will our code understand this?\n",
    "\n",
    "So what happens is that you first convert all your text into embeddings. Converting into embeddings means that you convert it into a vector. A vector means a set of numbers. So essentially, you want to represent the semantic meaning of this entire paragraph in the form of numbers.  Here, you can use many different techniques. You can use techniques like Word2Vec, Doc2Vec, or generate embeddings using BERT. There are many techniques. Okay? But the idea is that you represent this entire paragraph in the form of multiple numbers. This is a vector, the same for this paragraph and the same for this paragraph. Let's assume for a moment that the dimension of these vectors is 100. Basically, it's a 100-dimensional vector. Okay? Now we have the vectors for all three paragraphs. So what will we do now? As soon as we receive this query, we will generate an embedding for it as well. Basically, we will also give it a vector form. So, a 100-dimensional vector is created for this too. Now, I have four vectors in a 100-dimensional space. These three vectors are from your paragraphs: this is the Virat Kohli vector, this is the Jasprit Bumrah vector, and this is the Rohit Sharma vector. And let's say this is your query vector. Now, what you will do is calculate the similarity of your query vector with all three vectors. And whichever vector has the strongest similarity, you will understand that this question is related to that paragraph. And then you will use that paragraph to provide the answer. So this is how semantic search works, and this is what we need to implement in our system on our PDFs.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2d601-8eec-4591-beac-cb21f115e541",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-langchain-Intro/Image04.png\" alt=\"Flow of Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ea6fc-a02c-40d4-a3c2-edddfe6637e3",
   "metadata": {},
   "source": [
    "```\n",
    "So now that we understand how semantic search works, let me explain the design of this entire system in detail. Okay, so this whole process will start as soon as our user uploads a PDF.  The PDF will be uploaded, and we will store it somewhere in the cloud. For now, let's assume we are using AWS services, so we are storing that PDF on AWS's S3 service. Okay, now our PDF is available in the cloud. What we need to do first is load it. So, basically, we will need some kind of document loader that will help us bring the PDF into our system. Okay, now that it's in the system, the first thing I need to do is divide this entire PDF into small chunks. Okay? So, these small chunks can be based on anything. They can be based on chapters, pages, or even paragraphs. Let's assume for a moment that we have a 1000-page PDF, and we are chunking it based on pages. So, in total, we have divided this entire document into 1000 parts. So, we are basically using a text splitter that helps us separate each page. Okay? Now what we need to do is generate an embedding for each page, as I explained a little while ago. So, here we are using an embedding model. We are sending each page to this embedding model, and the embedding model is generating an embedding for that page. Embedding means it's generating a vector in an N-dimensional space. So now I have 1000 vectors, basically 1000 sets of numbers. Okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98cd9d-910a-47b2-ae1b-c67086c085ec",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-langchain-Intro/Image05.png\" alt=\"Database\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb83938-3951-4fbd-9d1b-1047bf8dda5b",
   "metadata": {},
   "source": [
    "```\n",
    "Now, what I'll do is store these embeddings in a database so that I can query them in the future.  We'll discuss this further later; there are slightly different types of databases, but essentially, they are databases, and we're storing these embeddings in them. Now, our main work begins. Our user comes, opens the PDF, and asks a query. The query is text, so what we'll do is first send this query to the same embedding model and generate an embedding for that query, again in n dimensions. And now, we already have 1000 vectors in our database. A new vector has arrived. We will compare this vector with all the other vectors, basically calculating the distance, and we will retrieve the vector or set of vectors that are closest to our query vector.  Let's say we decided to return five vectors; then the five most similar vectors will be returned, and then we will extract the corresponding pages, and those pages will be displayed here. I hope you understood this whole process. After that, the rest of the flow is exactly the same.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf34504-6a45-45ab-9650-16141286e663",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-langchain-Intro/Image06.png\" alt=\"System query and Brain\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee67c42-619f-45e2-a0cf-c3dee05e5931",
   "metadata": {},
   "source": [
    "```\n",
    "We will take the user's original query and these pages, and by combining them, we will form our system query. This system query will go into our app's brain, and in the brain, there will be NLU (Natural Language Understanding) plus context-aware text generation, which will give us our answer. We will then show that answer to the user in the form of the final output. So this is the low-level overview; we have discussed all the details here, right? So I really hope that with the time we've spent on this video so far, you have a clearer understanding of the design of this entire system. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03832d-0521-43ca-bdb8-dab93bcc0774",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-langchain-Intro/Image07.png\" alt=\"Total Problems Explantion\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237aeca-c898-443f-9b8b-196a1eb1b1e4",
   "metadata": {},
   "source": [
    "```\n",
    "Now our discussion starts from here. We've understood the system design of this entire project. Now let's have a short discussion about the challenges involved in building this project. I'll talk about the biggest challenge. In this entire flow, the most challenging aspect is building this \"brain.\" Think about it, we need to develop a component that, when we send it any query, can fully understand that query. Right? Now, this in itself is a very challenging task. Second, after understanding, it should be able to generate relevant text, which is again a very challenging task. And honestly, a lot of work has been done in NLP on these two things, but the breakthrough finally came in 2017 when the Transformers paper came out, and then after Transformers, the BERT and GPT papers came out, and then this whole episode of LLMs began. So, finally, this problem was cracked. Okay?\n",
    "\n",
    "So, guess what? We don't have to work too hard to develop this brain. LLMs are already available in the market that have both these capabilities: the capability of natural language understanding and the capability of generating context-aware text. We simply need to use an LLM. So, although this was a huge challenge in 2015, it's not a big challenge today. Today, you can easily solve this problem by using an LLM. So now, what I will do is, in this flow, I will write LLM instead of this \"brain.\" Okay? Now, there are many LLMs in the market. You can use any open-source LLM. If you are a very large company, you can train your own foundation models and create an LLM. But again, whatever it is, it's not our concern. We have found a way to solve our first challenge, and the way is to use an LLM. Okay? Now let me tell you about challenge number two. So, another big challenge is that if you use an LLM... If you want to leverage your brain, you'll have to integrate the LLM into your system somewhere, basically, you'll have to host it on your server. Now, you're probably aware of the scale of LLMs; they are very heavy models, very heavy deep learning models that are trained on data from the entire internet, right? Now, if you were to host such a large LLM model on your servers for inference, to get results from it, you would have to do a lot of engineering; you would have to solve a lot of computational problems because hosting and running such large systems on your server is not an easy task from an engineering point of view. Secondly, the cost would also be very high. \n",
    "\n",
    "So, the second major challenge is computation: how do I bring an LLM and host it on my system, on my cloud, and how do I run it and manage the cost? So, guess what? The good news is that this challenge has also been solved. So, large companies like OpenAI or Anthropic, or have put it on their own servers. For example, OpenAI has put their ChatGPT or GPT models on their own servers, and they have created an API around their LLM. Okay? Now, what's the advantage of this API? Anyone can interact with this LLM. You simply ask a question, that question goes to the LLM, and the LLM replies to you, and that reply is sent back to the user. So, you don't need to take the entire LLM and put it on your server. You can simply use these APIs to get your work done. So, in short, instead of using the LLM itself, I will use an LLM API. Okay? And the advantage of using an API is that I will only pay for what I use. So, if my usage is low, I will have to pay less. Okay? \n",
    "\n",
    "\n",
    "So, in short, the two major challenges of natural language understanding and text generation have been solved by LLMs, and The computational challenges surrounding LLMs have been solved by these APIs. So, the two major challenges that existed have already been solved as of 2025. Now, let me tell you about challenge number three in building this system. Let's talk about challenge number three.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14295d21-e2d3-4fa8-8cb8-7e0b3db9762c",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-langchain-Intro/Image08.png\" alt=\"The third challenge\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image09.png\" alt=\"The third challenge\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c243d1a-ebb8-4584-8be7-b79a4c676aa5",
   "metadata": {},
   "source": [
    "```\n",
    "Challenge number three is:\n",
    "\n",
    "Orchestrating this entire system, meaning getting all these components to work together, is a huge challenge in itself. Okay, so let me try to explain it a little. First, I want to show you how many moving components there are.  First, you have the AWS S3 component where you're storing your documents. Second, you have the Text Splitter component; this is also a type of model that decides how the splitting will happen based on the document. Third is embedding; the embedding process is also done through a model, so that's another component. Then there's the database where you store your embeddings, which is also a component. And besides that, your LLM is also a component. So we have five components in this system. Now, if I talk about the tasks, we are performing many different types of tasks here. We are performing the task of loading documents, we are performing the task of splitting text, we are performing the task of embedding, we are performing the task of database management, we are performing the task of retrieval, and we are performing the task of interacting with this LLM. So again, there are five or six tasks that we need to execute through a pipeline. Okay? \n",
    "\n",
    "So, in short, this system is made up of many moving components, and we need to execute many tasks between those components, which is a very challenging task in itself. If you were to write all this code from scratch, it could be very, very difficult. Let me give you an example. Let's say you coded this system manually, and tomorrow you find out that you don't want to use the OpenAI API anymore because it's expensive, and instead you want to use Google Gemini's API. Then suddenly, you have to change all the code you wrote to interact with OpenAI. The plan is to remove it and write new code to interact with the Google Gemini API.\n",
    "\n",
    "\n",
    "Similarly, here, instead of AWS, you might be moving to GCP.  Here, you might want to use a different model in the embedding model. So you understand, there are so many moving parts, and there's a lot of interaction between them, and it's a very complex system. Coding all of this manually is a very, very challenging task. And this is where LangChain comes into the picture. What LangChain does is provide you with built-in functionalities where you can interact with all these components in a plug-and-play manner. Not only that, if tomorrow you want to use Google's Gemini instead of OpenAI, no problem. You'll just change one line of code, and that's it, your work is done. So basically, you don't need to write a lot of boilerplate code for all these components because LangChain handles all of this behind the scenes. So, in short, if I summarize this entire conversation, it would be that if you want to build an LLM-powered application, the LLM does a lot of the heavy lifting, but running that LLM-based application end-to-end with all its moving components is a very, very difficult task, especially now because the technology is very new, so this task becomes even more difficult. And this is where LangChain comes into the picture. It says, \"You just focus on your idea; I will handle all the interfacing and orchestration work for you.\" Okay, so now you have a little intuition about why LangChain is being used in this entire system.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64af34-9c03-42fb-91de-c42413d21051",
   "metadata": {},
   "source": [
    "# **Benefits**\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image10.png\" alt=\"Benefits of LangChain\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image11.png\" alt=\"Benefits of LangChain\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image12.png\" alt=\"Benefits of LangChain\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e8a36-ab7e-4cc6-b5ac-31903f3c8fdb",
   "metadata": {},
   "source": [
    "```\n",
    "Now let me tell you about the other benefits that LangChain provides. So now that we've tried to understand the reasons behind using LangChain, I want to formally explain the benefits of LangChain. Okay, so let's discuss them one by one.\n",
    "\n",
    "The first and most important benefit of LangChain is the concept of chains. In fact, LangChain is named LangChain because of this concept of chains. So, with chains, you can give different components and different tasks the form of a chain. Basically, you can form a pipeline-sort of structure and perform even the most complex tasks.  Like in our example, you need to execute multiple tasks between multiple components: you need to load the document, split the text, perform embeddings, store it in the database, then retrieve it, and then send the data to the LLM. Now, this is a series of tasks, right? \n",
    "\n",
    "So, what you can do is convert this entire pipeline into a chain, and the best feature of that chain is that the output of one component automatically becomes the input of the next component. You don't need to manually write this code. And not only that, you can form very complex chains, you can form parallel chains, you can form conditional chains. So, no matter how complex the tasks are, you can create that pipeline in a very expressive way using chains with the help of LangChain. This is the biggest feature, which we will see later. \n",
    "\n",
    "Okay, then the second very big feature is model-agnostic development. This means it doesn't matter which model you use here. I gave you an example a little while ago. You can use OpenAI or you can use Google; you simply change one line of code, and your conversion will be done. You can use AWS or GCP here. Use it, and with just one or two lines of code, your entire codebase can be shifted to a different component. So basically, the model is agnostic. You need to focus on your core logic, your business logic. You can move components around; it doesn't matter. \n",
    "\n",
    "\n",
    "The third point is that LangChain has tried to give you a complete ecosystem. Okay? If you need a document loader, every kind of document loader is available. You can even fetch data from the cloud, you can load Excel files, you can load PDFs, you can load any kind of thing. In text splitters, you have 50 types of splits available. In embeddings, you have many embedding models available.  Many types of databases are available. So basically, for every component you use, many varieties are available. So whatever product or component your company wants to work with, its interface is available in LangChain. So it will never happen that you can't implement a component in LangChain. Okay? \n",
    "\n",
    "Lastly, here you also get memory and state handling concepts. So, for example, let's say our user queried, \"What are the assumptions of linear regression?\" Our system gave him the assumptions. Now he immediately asked the next question: \"Also give me a few interview questions on this machine learning algorithm.\" But which machine learning algorithm? We don't remember what the previous query was. We don't know that linear regression was discussed last time.\n",
    "\n",
    "LangChain solves this problem too. It has a memory concept where you can use the in-conversation memory concept. So if linear regression is being discussed in a conversation, even without mentioning linear regression again, our model will understand that linear regression is being discussed. And LangChain provides all of this. Okay?\n",
    "\n",
    "So in short, it's a great tool, a great library that provides you with... It helps a lot in building LLM-powered applications.  Okay, so the rest are very practical things. Later, when we build different applications, we will see all these concepts practically, write code, and you will appreciate them more. But since this is the first video, I really hope I was able to develop an understanding of what LangChain is and why it's needed.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84365460-2595-4192-b9ab-24da9cefcb4f",
   "metadata": {},
   "source": [
    "# **What can you build?**\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image13.png\" alt=\"What can you build?\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b36af-c928-4eeb-8a20-fd8918431e8c",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's discuss one more thing: LangChain, What can you build with LangChain? What can you create using LangChain, or what are people around the world creating using LangChain? There are multiple use cases. You can create many different things. Let me give you some examples. \n",
    "\n",
    "The first and most popular use case is that you can create conversational chatbots using LangChain. We live in the internet era, and most of the companies around us are internet-based companies. Uber, Swiggy, these are all internet-based companies. And the biggest problem for internet-based companies is scale. Scale means they have to deal with a large number of customers simultaneously, right? So, if you need to communicate with customers, one way is to set up a call center. Now, setting up a call center and hiring many people is a huge challenge, right? What if I had a chatbot that could talk exactly like a call center executive, understand the user's query, and provide solutions? Then a huge problem would be solved for me as a business.\n",
    "\n",
    "So, many companies today are creating chatbots. So, what happens is that the first layer of communication between the company and the customer is handled by chatbots, and when the chatbots can't handle the query, they forward it to a human, and then the human handles it further.So, the most popular use case is that you can create conversational chatbots using LangChain, and most people are doing just that. \n",
    "\n",
    "The second use case is AI knowledge assistants. So, an AI knowledge assistant is basically a chatbot, but it's also kind of trained on your data or has access to your data. Let's take an example. Let's say we have a campusX website where our courses are offered. Now, I want to integrate a chatbot into our courses so that if a student is watching a video or our lectures and has any doubts, they can You can quickly ask that chatbot about that particular doubt, okay? So what's happening here is that it's a chatbot, but it also knows about my data. It has knowledge about what's happening in the lecture that's currently going on. So you can easily build this knowledge assistant with the help of LangChain, and we will do that later, okay? \n",
    "\n",
    "The third use case is AI agents. This is a very popular term that you must have heard a lot in the last year: agents. Agents are basically chatbots on steroids. They can not only talk, but they can also perform tasks. For example, consider MakeMyTrip. What do people do on MakeMyTrip? They book hotels, train tickets, and flight tickets.  Generally, older people, those in their 60s, are not as fluent in using these kinds of websites or booking tickets. So what we can do is we can place an agent here, an AI agent, that will not only talk like a human but will also perform tasks for you when you tell it to.\n",
    "\n",
    "For example, if a senior citizen is talking to this AI agent, they will literally just say, \"Book me a flight ticket from this place to this place on this date, the cheapest one.\" This AI agent has the tools and the power to execute this entire task on its own. It is said that AI agents are the next big thing in the world of AI, and they most likely will be. And the good thing is that you can create agents using LangChain, okay? In fact, we will show you how to build a basic agent in this playlist, okay? So this is also a very interesting use case. \n",
    "\n",
    "Besides this, there can be many other use cases. You can automate workflows, any kind of workflow automation, at a personal or professional level. To automate workflows at the company level, you can use LangChain. \n",
    "\n",
    "The fifth use case is summarization and research helper. So, what you can do is use LangChain to simplify research papers or books, etc. You already know that you can't upload very large books to ChatGPT because of the context length problem, right? A second problem might be that sometimes you can't upload your company's private data to ChatGPT because your company has prohibited it. \n",
    "\n",
    "So, what you can do is your company can use LangChain to create a ChatGPT-like tool that can process any size document and answer related questions. And secondly, since this is your company's personal chatbot, you can also upload and discuss private data related to your company. So, many such summarization and research-related tools are being created using LangChain, and in the future, who knows what other tools will be created, but the future looks bright. We will see many more LLM-based applications, just like the boom in websites and then the boom in mobile apps. It now seems that a boom in LLM-based applications is on the horizon, and LangChain is going to play a very key role in that.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a23e28-1543-4bd7-94b1-6a6a091007c2",
   "metadata": {},
   "source": [
    "# **Alternatives**\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image14.png\" alt=\"LangChain Alternatives\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image15.png\" alt=\"LangChain Alternatives\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-langchain-Intro/Image16.png\" alt=\"LangChain Alternatives\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d6efc-cfe0-4196-b7c5-ffb6a3f26765",
   "metadata": {},
   "source": [
    "```\n",
    "One last thing I want to cover in this introduction video is that LangChain is not the only framework that helps you build LLM applications.\n",
    "\n",
    "Alternatives to LangChain:\n",
    "\n",
    "There are multiple other frameworks as well. Two of the very famous frameworks that you might have heard of are LlamaIndex and Haystack. These two frameworks are also quite popular, and many companies are using these frameworks instead of using LangChain. It really depends on where you're getting better pricing, which tool seems most suitable to you. The decision-making is based on that. But yes, LlamaIndex is a little more popular; you might have heard of it. We even have a course on it on our website. And the second one is Haystack, a similar kind of platform, a library or framework, you can say. And here too, you can very easily build LLM-based applications. Okay? \n",
    "\n",
    "Now, I could give you a comparative study between LangChain, LlamaIndex, and Haystack, but I feel that since we haven't even studied LangChain properly yet, it's not the right time. But in the future, I will definitely touch upon this topic where I will give you an analysis of the pros and cons between LangChain, LlamaIndex, and Haystack. But for now, you just need to know that it's not the only framework; there are others as well. Okay? \n",
    "\n",
    "So with that, I will conclude this video. My goal of introducing you to LangChain has been achieved, and you have an idea of what LangChain is, why it's needed, and what you can do with it. Now we are ready to move ahead in our journey. In the next video, we will discuss the complete ecosystem of LangChain, and trust me, knowing that is very important. Once we understand that, then we will jump into the practical parts and start coding LLM We will start creating web-based applications. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca1118-9f67-4d95-8c84-825b0f70a62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
