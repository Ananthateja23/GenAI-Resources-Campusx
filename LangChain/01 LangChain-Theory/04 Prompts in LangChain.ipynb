{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db719400-a272-4cdb-ac5d-f55aae7ee214",
   "metadata": {},
   "source": [
    "```\n",
    "Today's video is about prompts. I told you in the previous video that there are six important components in Land Chain, among them. In the last video, we covered the Models component, and in today's video, we'll be studying the second component, which is Prompts.  Okay, so I've made today's video quite clear and easy to understand.\n",
    "\n",
    "I'll try to explain the \"why\" behind everything we discuss. The topic itself is easy, but it can be a little confusing, so I've kept the explanation very simple. There's nothing else to say. Let's start the video.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06975c-1cb0-4180-971e-d39db7379a0b",
   "metadata": {},
   "source": [
    "# **Recap**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image01.png\" alt=\"Recap\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a1831-60d6-479f-83fa-3727e63c4d2e",
   "metadata": {},
   "source": [
    "```\n",
    "Recap:\n",
    "\n",
    "So guys, before starting the video, let's quickly recap what we have covered so far in this playlist.  So far, I have uploaded three videos in this playlist. \n",
    "\n",
    "The first video, this one, gave you a detailed introduction to LangChain, and I explained why LangChain is needed as a framework. \n",
    "\n",
    "After that, in the second video, I explained the six most important components of LangChain and tried to explain how they work with a real-life example. \n",
    "\n",
    "And then finally, in the third video, we took a deep dive into the first component. In this video, we covered the Models component of LangChain in quite detail. \n",
    "\n",
    "Now, today's video will be the fourth video in this playlist, and in this video, we will discuss the second component of LangChain, which is called Prompts. \n",
    "\n",
    "So today's video will be all about prompts. I'm going to teach you everything end-to-end about how prompts work in LangChain, what are the important classes, and in what kind of scenarios you should use them. \n",
    "\n",
    "So I hope you understood the recap and also understood what our plan of action is going to be in this video.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cae82-2351-45a7-af24-d832823d1212",
   "metadata": {},
   "source": [
    "# **Error correction**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image02.png\" alt=\"Error correction\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844b004-0fbd-498b-8def-b6e4708ce761",
   "metadata": {},
   "source": [
    "```\n",
    "So let's start the video. Before moving on, I want to have a short discussion with you. In the last video, I taught you about models in great detail. There was a small mistake in my explanation, and a student pointed it out in the comments section. So I want to rectify that before starting this video. \n",
    "\n",
    "If you remember, in the last video, when I was teaching you how to interact with different LLMs, I told you that you have some LLM parameters, such as temperature. And I told you that its value lies between zero and two. \n",
    "\n",
    "If you keep it around zero, the output of your LLM is very deterministic, and if you keep it around two, the output of your LLM is very creative, right? Now, this explanation is not completely correct. I'll tell you what the correct usage of temperature is, as that student pointed out. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceefe608-6520-4a3c-99b6-432add29cd32",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image03.png\" alt=\"Error correction\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5b4ee-42b0-43a6-84a2-c999683e3550",
   "metadata": {},
   "source": [
    "```\n",
    "So here you can see this code that we saw in the last video, where we were interacting with an LLM from OpenAI, GPT-4. And I gave it this command: \"A five-line poem on cricket.\" And here I had set the temperature value to 1.5. Now, let me set its value to zero for a while. Okay? And now I'll run this code. So I would run `python temperature.py`. Now what will happen is that our LLM will write a five-line poem about cricket. And you can see this is the poem. Now, the advantage of setting the temperature to zero is that no matter how many times you send the same input, you will get the same output every time. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5a6f2-143a-4b71-b658-230cd9f9bd92",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image04.png\" alt=\"Error correction\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd8671-3cf5-4448-88e4-cb67528d47ec",
   "metadata": {},
   "source": [
    "```\n",
    "See, if I run this code again, I haven't changed the input. My input is still exactly the same, and now if I run this code, what will happen is that the LLM will... The output will be exactly the same as last time because we have set the temperature to zero. Look at this. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bee50d-8b03-45f1-9f9c-6380c15daae9",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image05.png\" alt=\"Error correction\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa982e4-61c2-43d2-a8a1-b6e2a0c05e73",
   "metadata": {},
   "source": [
    "```\n",
    "But as soon as I start increasing its value, let's say I set it to 0.5, then what will happen is that the response will be slightly different from the response we got last time. So if I run this code again, I haven't changed the input yet. My input is still \"Write a five-line poem on cricket.\" This time you will notice a slight change in your LLM's output. Look at this. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24202558-ee3a-4204-8fd3-dbcc93cd3f83",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image06.png\" alt=\"Error correction\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf181f58-cced-4fbc-aab4-912653b8c101",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, if I increase it further, let's say to 1.5, then the output might be quite different from the previous one. Okay, you can see a very creative output is coming. Okay, so that's the idea behind temperature. \n",
    "\n",
    "If you keep the temperature at or near zero, your LLM will always give the same output for the same given input. And if you keep the temperature value around 1.5 or 2, then for the same input, your output will be quite different and creative each time. \n",
    "\n",
    "Okay, so if you are building an application where you want the same output for the same input every time, then you need to keep the temperature value around zero. If you want to build an application where you see different types of output every time for the same input, then you will keep the temperature value around 1.5. \n",
    "\n",
    "Okay, so I didn't mention this in the last class, and thank you to the student (I don't remember the name) whose comment I read, and then I immediately came and experimented. That's the advantage.\n",
    "\n",
    "So I didn't mention this in the last class, and thank you to the student, I don't remember the name, I read it in the comments, and then I immediately did the experiment. That's the advantage of putting things on YouTube, that if there's a mistake in what you're teaching, someone will point it out, and you have the option to correct it.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce3ab0-ff61-4ad3-b184-4dd04f4227b0",
   "metadata": {},
   "source": [
    "# **What are Prompts?**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image07.png\" alt=\"What are prompts\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f8d6e-e46b-4949-8816-2721827bb046",
   "metadata": {},
   "source": [
    "```\n",
    "Prompts\n",
    "Okay guys, let's discuss what are prompts?\n",
    "\n",
    "Here I have written a definition, it says that prompts are input instructions or queries given to a model to guide its output.\n",
    "\n",
    "So basically, when you interact with an LLM, the message you send to the LLM is what we call a prompt. So prompts aren't actually anything new; we used a lot of prompts in the last video. It's just that I didn't use the word \"prompt\" there. If you look at the code we wrote in the last video, when you're calling or invoking the model, you sent this message: \"Write a five-line poem on cricket.\" So this message that you're sending to the LLM is called a prompt, okay?\n",
    "\n",
    "Now, prompts can be of two types:\n",
    "\n",
    "The most general type of prompts are text-based prompts. This means that when you're talking to your LLM, you're giving your instructions. So those instructions are textual, meaning you're interacting by writing some text.\n",
    "\n",
    "And the second type is multi-modal prompts. In multi-modal prompts, you use some other mode along with text to communicate. For example, you can send an image; you can do this with chat. You can also use other modes of communication, such as sending an image. You can do this with ChatGPT; you can upload an image and then ask questions about that image. \n",
    "\n",
    "You can also use sound, for example, you could upload a song and ask, \"Who is the singer of this song?\" Or you can directly use video; you can upload a video and ask a question about it. So, there are also multimodal prompts. \n",
    "\n",
    "Our video today will focus entirely on text-based prompts because 99% of the time you work with textual prompts, at least that's the case today. Maybe in the future, multimodal prompts will become more prominent, but today's video will be about text-based prompts. \n",
    "\n",
    "Now, I want to tell you one more thing: prompts are so important that the output of the LLM depends heavily on them.  This means that if you change the prompts even slightly, the output of your LLM might change significantly. That's why prompts are super important, and multiple techniques are used to create prompts, and LangChain also has many techniques available, which we will be learning about today. \n",
    "\n",
    "In fact,  It's so important that a whole new job profile has emerged around it, which we call prompt engineering. I've also planned to create a separate playlist on this YouTube channel in the future, on the topic of prompt engineering. There, I'll teach you all the different techniques you can use to design prompts.\n",
    "\n",
    "But even in today's video, you'll get quite in-depth knowledge on how you can design different types of prompts in LangChain. So, I really hope you've understood what prompts are. Now let's move on and have a very important discussion about what static prompts are and what dynamic prompts are. Before explaining static and dynamic prompts, I want to have a short discussion with you.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e166a55-70a3-4625-989c-2e09d100a5e2",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image08.png\" alt=\"What are prompts\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303bcbf-aa24-49c2-a14d-e9b50f5c5d4f",
   "metadata": {},
   "source": [
    "```\n",
    "So, this is how we've been writing prompts so far. If you watched the previous video, you'll know this is the code from that video. If you look at this code, we're writing the prompts ourselves, as programmers, like this.\n",
    "\n",
    "So, if I want to get a poem about cricket written by my LLM, I'm directly putting this prompt inside the `model.invoke` function, and this prompt is going to the LLM, and I'm getting a response. Now, think about it and tell me, do you think this is the right way to send prompts to the LLM? I hope you understand, this is not the right way. \n",
    "\n",
    "Whenever you build a real-world application, as a programmer, you won't be writing the prompts yourself. Ideally, your user, the one who will be using your application, will send the prompts, and then you'll take that prompt and send it to the LLM. So, let's discuss this with an example.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a874e-2789-4558-a1bf-cf702e6f3321",
   "metadata": {},
   "source": [
    "# **Static vs Dynamic Prompt:**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image09.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4344976-2e54-4247-8d04-6b95ea36f3ae",
   "metadata": {},
   "source": [
    "```\n",
    "Let's say you're building a research assistant tool where any user can come and summarize any research paper. Okay? So ideally, your application would look something like this: there would be a website where you give your user an input box where they can type their prompt. \n",
    "\n",
    "For example, here they typed \"Summarize the Attention Is All You Need paper in a simple fashion.\" After typing this prompt, they will click on the summarize button. As soon as this button is clicked, what you will do is fetch the prompt that is in the text box and then send it to the LLM. Then, you will display the response that comes from the LLM here. This is the entire flow, okay? \n",
    "\n",
    "So, let me first show you this flow and let's create a UI where we will actually build this kind of research assistant, and then I'll explain to you what static and dynamic prompts are.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b033e6a-30cd-410d-84c2-64071c231689",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image10.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29feb356-ea17-483d-a898-41f5b796d06d",
   "metadata": {},
   "source": [
    "```\n",
    "So let's do one thing, first let's create a new file and let's call this file prompt_ui.py.  UI because I'm actually going to create a website here with the help of Streamlit. So if you don't know Streamlit, please make sure you read a little bit about it, but don't worry, it's not very difficult, everything is very easy. \n",
    "\n",
    "First, what we are doing is we are writing this code: from langchain.openai import ChatOpenAI. I'm using OpenAI here, but if you don't want to use OpenAI, I told you in the previous video how you can use open-source models. You can write that code.\n",
    "\n",
    "Then we are using from dotenv import load_dotenv and I'm importing Streamlit as st. This Streamlit is already installed on my machine. You can do pip install streamlit. Then we are calling the load_dotenv function to bring our OpenAI key into this file.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868618e-6ef8-4eb6-b019-dc81a0c6eb94",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image11.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee171de-a23a-467b-809a-503a5b59d034",
   "metadata": {},
   "source": [
    "```\n",
    "After that, the main code starts from here. First, what I'm doing is st.header. With st.header, you can add a header to your website. I'm writing Research Tool. Okay, after that, using st.text_input, I'm creating an input box where the user will enter their input, and whatever input they enter, I'm storing it in a variable called user_input. \n",
    "\n",
    "And inside this, I can write Enter your prompt. After this, we are creating a button, st.button. As soon as this is clicked, we are doing st.text(\"Some random text\"). For now, I'll just create the UI, and then I'll write the rest of the code. So to run this code, we will run this command: streamlit run your filename prompt_ui.py. \n",
    "\n",
    "As soon as I run this, Streamlit will create a server behind the scenes. We can access this website by going to that server. So our server is on, and here is our website. So you can see the button didn't appear. Button is a function, its... We can name it Why is it showing \"Summarize Save You Put\"? We can fix this. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684dc82-66d2-40f9-966c-0cca372d9641",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image12.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image13.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image14.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b28786-b696-45ab-82e7-88da7e567074",
   "metadata": {},
   "source": [
    "```\n",
    "Actually, I haven't used Streamlit for a long time. We'll use the streamlit.write() function and put \"result.content\" here. Save, let's run, and you can see guys, this is the output. Okay, so this is how you build an LLM application. You ask the user for input, as in, you ask the user for a prompt, and then you invoke that prompt. Okay, so first of all, this is something I wanted to show you. Now, what's important to notice here is that this prompt that we are asking from the user and directly sending to the invoke function is a static prompt.\n",
    "\n",
    "This means that if tomorrow I want to summarize another paper, the user will have to write another prompt here, such as \"Summarize the Word2Vec paper in five lines.\" Now I'm clicking on Summarize, and you can see, now I'm seeing a five-line summary of Word2Vec. But you understand this, right? That as a user, you have to write a new prompt every time to get a new response from the LLM. Okay, so we call this a static prompt. Now, honestly, static prompts are not used much. Okay?\n",
    "\n",
    "The reason behind this is that when you ask the user to write static prompts, you are giving the user a lot of control. I told you a little while ago that the output of the LLM depends a lot on the prompts; it's very sensitive to prompts. If you change the prompts even a little, the LLM's output changes significantly. \n",
    "\n",
    "Now imagine a scenario where your user doesn't even know the name of the research paper or the exact name. In such a case, when the user types the prompt, they might enter a wrong name, and you send that wrong name as a prompt to your LLM. Now the LLM... If it has to give some output, it might hallucinate or print some output that is not desirable. \n",
    "\n",
    "So basically, what I'm trying to say is that when you ask the user for the entire prompt, the chances of getting a static prompt are very high. Okay?  They might write something like, instead of five lines, they might write that they want it to be math-heavy, or they might write that they want it to be code-heavy, right? \n",
    "\n",
    "So, even such a small change in the prompt will drastically change the output you get. Ideally, when you are building an LLM application, you would always want all your users to have a consistent user experience. Maybe the unique selling point of your research tool is that whenever it summarizes a research paper, it gives great analogies and excellent examples. \n",
    "\n",
    "Now, if you are asking the user for the entire prompt, you cannot ensure this. So, this is the main problem with static prompts.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e91114-c0ee-4148-b899-0343e58ca3ab",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image15.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ebc48-7cf7-44bc-a832-82e4c9d06dee",
   "metadata": {},
   "source": [
    "```\n",
    "So ideally, what you should do, let me show you, you should prepare a template in advance, like I've created a template here for my research assistant tool. Here you can see, it says \"Please summarize the research paper titled [Paper Input]\". This is a fill-in-the-blank, with the following specifications: Explanation Style (which the user will provide) and Explanation Length (which the user will provide).\n",
    "\n",
    "Then, I've specified what the characteristics of the generated summary of the research paper will be. The first characteristic will be that we will include mathematical details. It says here, \"Include relevant mathematical equations if present in the paper. Explain the mathematical concepts using simple, intuitive code snippets wherever applicable. Also add analogies that are relatable.\" If a piece of information is not available, instead of hallucinating, simply put \"Insufficient information available.\" Also, at the end, I've written, \"Ensure the summary is clear, accurate, and aligned with the provided style and length.\" So, I've created a prompt template, okay? \n",
    "\n",
    "Now, inside this template, I will ask the user for a few things: which paper they want to summarize, in what style they want it summarized, and what length of summary they need. Okay? So, in the paper input, they can enter \"Attention Is All You Need\" or \"Word2Vec\" style. They can choose different styles: a simple style, a math-heavy style, a code-heavy style. For length, it can be short, medium, or long. Okay? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfa2f7-a333-4055-ad9b-64d0654cbf83",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image16.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f28886-0da6-428d-8cc9-fe5ba1f2fd89",
   "metadata": {},
   "source": [
    "```\n",
    "So, basically, what we'll do is, in a way, we'll change our UI. Instead of one input, we'll ask the user for three inputs. The first input will be a dropdown where we'll show them a list of all the available papers, and they will select one of them. So, there's no scope for spelling mistakes here.\n",
    "\n",
    "Then, right here, we'll give them the option of Code Heavy, Math Heavy, Simple, and Intuitive. From here, they will select the type of explanation they want. And here, we'll give them the options of Long, Medium, and Short.\n",
    "\n",
    "So, we'll create these three dropdowns, pick one value from each, and fill them into this prompt. So, what you're seeing now is called a dynamic prompt because you have a prompt, but the key values inside it are provided by the user. \n",
    "\n",
    "So, with this single prompt, you can summarize any paper in any style and at any length. Okay? So, I hope you first understood what static prompts are, what their biggest problem is, and how dynamic prompts solve that problem. Now, I'll show you how to create a dynamic prompt by writing some code.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9c5ce-04e0-42d1-b531-668c9616f479",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image17.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image18.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image19.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3cdd56-7997-4ab3-a628-e5290c2888f2",
   "metadata": {},
   "source": [
    "```\n",
    "Now I'll show you with a code example how a dynamic prompt is created. I'll demonstrate with code how a dynamic prompt is created. So, what will I do first? I'll make some changes to the UI. Currently, I was asking the user for the entire prompt. \n",
    "\n",
    "Now, instead of asking the user for the entire prompt, I will ask the user for three different values. First: Which paper does he want to summarize? Second, what type of explanation does he need? And third, what should be the length of the explanation? \n",
    "\n",
    "So I've already written some code here so that we don't waste time typing. This is Streamlit code to create a dropdown. So what I will do is, instead of asking for a prompt, I will use this code. I'm wrapping the text so you can see it properly. \n",
    "\n",
    "So basically, we are using Streamlit's selectbox tool. With its help, you can create dropdowns, okay? So you can see, in the first dropdown, I'm giving him some research paper options. In the second one, I'm asking for the explanation style, and in the third one, I'm asking for the length of the explanation, okay? Saved it. \n",
    "\n",
    "Also, for now, let's put some hardcoded strings, okay? So this is the code. Let me show you how it looks. Streamlit run prompt_ui.py. So yeah guys, from here the user can select a paper. Currently, I've given four or five options. From here, they can select an explanation style. From here, they can select an explanation length. When they click on summarize, they will see the LLM output, okay? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6586ed-61ab-483c-955a-7af84c254306",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image20.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image21.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image22.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image23.png\" alt=\"Static vs Dynamic Prompt\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275bf1e-8e13-4d44-b505-4a16357f3910",
   "metadata": {},
   "source": [
    "```\n",
    "Now what we will do is, we will design a prompt template here. So at the top, first of all, from langchain_core.prompts, I will import PromptTemplate. Here we will first create a template. Let's create a variable for creating the template. Let's call it template. PromptTemplate will be our template. So what I will do is, I'm copying this entire template and pasting it here, okay? \n",
    "\n",
    "And then once the template is created, the second thing will be the input variables. We have three variables. The name of the first one is paper_input, the name of the second one is style_input, and the name of the third one is length_input, okay? So this is my complete template. \n",
    "\n",
    "Template, now what will I do? I will fill the placeholders. So, one thing we probably didn't... Okay, I've given a name to the variable here. I thought I hadn't given a name to the variable. So now what we'll do is we'll write `template.invoke` and here, inside a dictionary, I will pass `paper_input` in place of `paper_input` (the variable set above), `style_input` in place of `style_input` (again, the variable we set above), and `length_input` in place of `length_input`. Okay? And this will give me my prompt. \n",
    "\n",
    "And once I get my prompt, what will I do? I will invoke my model above and send my prompt to it, and whatever result comes, I will store it in a variable and print `result.content`. And that's the code, guys. Let's take a look. We are asking the user for these three things. \n",
    "\n",
    "As soon as we get these three things, we are creating a prompt template, filling it, and as soon as the prompt is filled, we are sending it to our LLM. So let's refresh this whole thing. So, \"Attention is all you need,\" I need a code-oriented explanation. Instead of short, I need medium. Clicked on summarize. Okay, my bad. I wrote `print` here. It should not be `print`. \n",
    "\n",
    "It should be `st.write`. Print is printing things to the console here. So I'll go back, run summarize, and you can see this is the output. Okay? Now, very easily you can put another research paper here. Okay? \n",
    "\n",
    "From here you can do mathematical. Okay, this is a formula that is not formatted correctly, but you get the idea. Okay? You understood how you can create dynamic prompts with the help of prompt templates, and more importantly, you logically understood why there is a need to create dynamic prompts. Okay? So, I hope you've understood everything in the video so far.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed18f28-defb-4cfb-868c-9e2e86a8c6fd",
   "metadata": {},
   "source": [
    "# **Why we should use PromptTemplate?**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image24.png\" alt=\"PromptTemplate\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531abcf6-216a-4262-baf1-386be6f04394",
   "metadata": {},
   "source": [
    "```\n",
    "Prompt Template:\n",
    "\n",
    "Now, when I first read about prompt templates, a doubt came to my mind, and I'm pretty sure you're having the same doubt: why can't we simply use f-strings instead of prompt templates? What I mean is, if you look at this code, what we're doing here is inserting some dynamic placeholders inside a large string, right? And we're sending the values ​​for those placeholders at runtime.  \n",
    "\n",
    "F-strings do exactly the same thing. If this doubt is coming to your mind, it's a very valid doubt, and let me tell you that you can write this entire code as it is using f-strings, and it will work. So why do we need to use this prompt template class at all? There are multiple reasons.\n",
    "\n",
    "Let me tell you three strong reasons. The first reason is that if you create a prompt using the prompt template class, you get some validation by default.  This means, let's say you're using three placeholders here: style_input, paper_input, length_input. Now, by chance, let's say you accidentally forgot to provide the length_input in the input variable in the code. Okay?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc088d4f-b126-4aa9-a571-b6a7b66f57b0",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image25.png\" alt=\"PromptTemplate\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb3e05-01b7-4f1d-86f9-fe544fc1f711",
   "metadata": {},
   "source": [
    "```\n",
    "Now, you have an option; there's another parameter called validate_template, which, if you set to true, will automatically validate when you run the code to check if all the placeholders mentioned here have their names provided. If not, an error will be triggered automatically, and the code won't run. \n",
    "\n",
    "Let me show you. So, if I run this code again: `streamlit run prompt_ui.py`. You can see that an error has been thrown below, saying that only two placeholders were found, and the third one is missing. Okay? Or let's say you added something extra. Let's say length_input was there, but you also added name, which you didn't define above. \n",
    "\n",
    "If you don't include it, okay, in that case also your code will break, and it will tell you that you have added something extra here which is not present in the template, okay? So the first benefit is clearly visible: there are some validations by default that you can use, and this will make your code run more correctly.  \n",
    "\n",
    "It won't break at runtime on the server; if it's going to break, it will break during development, and then you can rectify it, okay? So this validation is present by default; you won't find this in f-strings.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873aaea2-b8ec-413a-9618-016e5543deb2",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image26.png\" alt=\"PromptTemplate\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be378b21-8ff1-4cd1-a3a0-4b2812f949d0",
   "metadata": {},
   "source": [
    "```\n",
    "The second benefit is that you can reuse your prompt templates, okay? This means that, as you can see here, you've written this code, and in the middle of this code, you've added such a large template, which makes your code look unnecessarily bulky. Now, this is a small code, this is a small template, but it's possible that there might be an even larger template, or it's possible that multiple web pages on your website are using the same template. \n",
    "\n",
    "In such scenarios, what you can do is save your template separately as a JSON file and then load and use it wherever needed, okay? Let me show you this. So what we are doing is creating a new file. Let's call this prompt_generator.py, okay? \n",
    "\n",
    "And I'm importing from langchain_core.prompts import PromptTemplate, okay? And what I will do is, instead of using this entire template that I had created here, I will cut and paste it here. So I've created this template in a separate file, okay? And now what I'm doing is saving this template. \n",
    "\n",
    "So for this, I simply need to call the template and then call the save function, and here I need to specify the file name where I want to store it. So I'll call it template.json. Now let's do something with this file. We're running this, so the name of this file is python_prompt_generator.py\n",
    "\n",
    "And this file ran, and look, we have our template. See? So we've saved our entire prompt template in a JSON format. Okay? Now, the fun part is that I can easily load it here. For that, I just need to do one thing: call the load_prompt function from prompts. And now I just need to come here and write template = load_prompt() and here I just need to specify the path to my file, which is template.json. Okay? Saved. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a3aaa-805b-4d2e-a18a-aba6976caff2",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image27.png\" alt=\"PromptTemplate\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image28.png\" alt=\"PromptTemplate\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image29.png\" alt=\"PromptTemplate\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749ea57-9bb6-4616-979e-5292a5d64d11",
   "metadata": {},
   "source": [
    "```\n",
    "The rest of the code will remain as it is, and now I'll run this code. Okay, my website has started. Clicked on Summarize, and look, it's working. Okay? Now, the special thing is that in this particular file, if you notice, our template isn't even in this file. Okay? \n",
    "\n",
    "Our template is stored somewhere else. This is my template that I generated with the help of this file. But the good thing is that now this is reusable code. Tomorrow, if another file needs this template, I'll simply import template.json there. Okay? So, in a large application, the PromptTemplate class is also helping you with reusability. So that's the second reason. Reason number three is that PromptTemplate is very tightly coupled with LangChain's overall ecosystem. Later on, I'm going to teach you about chains, and you can easily use PromptTemplate in the concept of chains as well. \n",
    "\n",
    "Let me show you, although we'll study this in detail later. Don't worry if you don't understand it at this point. So, if you notice, what we're doing is we're using it twice. We are calling invoke once for the prompt and then once for the model. \n",
    "\n",
    "What we can do instead of calling invoke twice is call invoke a single time for this entire thing. We just need to use a chain. So what you will do is, I'm cutting this code and for now I'm bringing it here on the button click. Okay? \n",
    "\n",
    "Now see what I'm doing. I'm creating a chain. So what I will do is, this two-step process where first I'm designing a prompt and then sending that prompt to the LLM, I'm tying these two steps together to form a chain.\n",
    "\n",
    "So I will create a chain and this chain will be consisting of two steps. One is the prompt, not prompt, one is the template that we got, and the second is our model that we created above, this one. Okay, we created the model somewhere, yes, here is our model. Okay? \n",
    "\n",
    "So we are getting the template from load_prompt, we have created the model above. We have tied these two steps together. Okay? After that, you simply need to write chain.invoke and pass this dictionary inside it like this. And then you don't need to write the rest of the code. \n",
    "\n",
    "Now you will directly get your result here and you are printing that result. So what's happening is, you called the template, formed a chain with the help of the template and the model, invoked the chain. The chain needed this as input, you provided it. \n",
    "\n",
    "So first the template was formed and then that same template, that same prompt went into the model. You got the result back from it, and you are displaying that result. So here you will call invoke only once. Okay? Let's try running this. So if I run this, see, it is working. Okay? \n",
    "\n",
    "So you can do this too if you use a prompt template. If you had used an f-string instead of a prompt template here, you wouldn't be able to put it in the chain. \n",
    "\n",
    "That's what I was saying, that the third benefit is that it fits very well into the entire LangChain universe, the prompt template. That is, you should use a prompt template. So, in a nutshell, you can use f-strings, but you should use prompt templates because there are many advantages that you won't find with f-strings. Okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c82d4-e338-4720-ba9d-510baeae72b0",
   "metadata": {},
   "source": [
    "# **Messages**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image30.png\" alt=\"Chatbot\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac97e7da-6137-415e-873f-762291837f7c",
   "metadata": {},
   "source": [
    "```\n",
    "Messages:\n",
    "\n",
    "So I hope you understood the prompt template well. Now let's do something else, we've read quite a bit, let's create a small chatbot. It will be fun. So before creating the chatbot, let me tell you a little bit about how our chatbot will look. We are not creating any GUI for this because it requires extra coding. Our chatbot will work in our console. \n",
    "\n",
    "So our console will look something like this: First, it will show \"You,\" here you will type something, and as soon as you type something, the AI's message will appear. Then you will type something again, and the AI's message will appear again. This is how we will have a conversation with our LLM. This is going to be the flow, okay? So it's very simple, let's build this small application. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1ef53-56cb-4492-a18c-313880c12d6e",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image31.png\" alt=\"Chatbot\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535a9af-9e58-4fa9-9404-775b62fe4038",
   "metadata": {},
   "source": [
    "```\n",
    "So let's create a new file, let's call it chatbot.py. Again, we need to import the same things: from langchain.openai import ChatOpenAI. I'll say it again, if you don't want to use ChatOpenAI or the OpenAI API, you can use the free ones from Hugging Face. \n",
    "\n",
    "I told you in the last video, the code is exactly the same, you just need to import a different library. Then we will write from .env import load_dotenv. We will call the load_dotenv function, and here we will create a model, the default ChatOpenAI model. Now our work begins. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadd57c-9982-4a84-a7dd-71d7708035bb",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image32.png\" alt=\"Chatbot\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd51f-29f4-4c52-afc1-8965ef645057",
   "metadata": {},
   "source": [
    "```\n",
    "So the idea is that first of all, since it's a chatbot, this program will run indefinitely unless the user types \"exit.\" So we will run an infinite loop: while True. And inside this loop, what will we do? We will ask the user for their input, and the input will look something like this. And whatever the user enters, we will store it in a variable. \n",
    "\n",
    "We will check if the input the user sent is \"exit.\" If it is \"exit,\" then we will break out of our loop, and our program will end. But if it's not \"exit,\" So we will invoke the model and we will send whatever the user's input is as the prompt. Okay, we are not using a prompt template because there is no need to make anything dynamic here. \n",
    "\n",
    "We are sending whatever is coming from the user as is; it's a static prompt. Doing this will give us a result, and we want to print this result. So it will be result.content. Also, we want to show that this message came from the AI, like this. Okay, that's it guys, this is our code. \n",
    "\n",
    "We have created a very simple chatbot. So let's run it once. To run this program, we will write python chatbot.py. I will say Hi. The AI message came: How can I assist you today? I would say, Tell me the difference between, I don't know, a framework and a library. This is the response from our AI. Tell me what is the capital of India? The capital of India is New Delhi. If I write exit now, the program stopped. And it's that simple to create a chatbot. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccece7f3-f3d5-4d5c-b699-52f675d5e4d0",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, but there is a problem. I'll show you this chatbot running again, and then I'll tell you what the problem is. So I'm saying Hi. How can I assist you? Let's say I told it, Tell me which one is greater, two or zero? So the AI says two. \n",
    "\n",
    "So I would say now, Multiply the bigger number by 10. Okay, now see what the problem is. So its response is, Let's say the bigger number is x. Multiplying x by 10, we get 10x. Ideally, what should have happened was that the bigger number between two and zero was two, so I said multiply the bigger number by 10, meaning multiply two by 10, meaning 20 should be the answer. \n",
    "\n",
    "But our chatbot didn't understand which number we were talking about. And this happened because your chatbot doesn't have context. Okay? It doesn't remember the previous messages; it doesn't even know what the conversation is about. And that's because we haven't coded this functionality into our application. \n",
    "\n",
    "So, what we need to do is maintain a chat history. We'll store all the conversations we've had so far in one place, and then we'll send that entire history to the LLM. That way, if I ask something related to a previous topic, it can look at the history and understand what's going on. So, basically, we'll have to make some changes to our code.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105340d-cedf-48e5-9e79-a7ec4a7027de",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image33.png\" alt=\"Chatbot\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829972e-e9c0-40f4-a5c6-d562c26a27d2",
   "metadata": {},
   "source": [
    "```\n",
    "So, the first change is that I need to create a list called \"chat history,\" okay? And then what I need to do is, each time the user types a message, we need to append it to the chat history like this, okay? \n",
    "\n",
    "And now when I'm sending the prompt to the LLM, my prompt won't be just a single message, but my entire chat history so far. So I will say \"send the chat history,\" and this invoke function is flexible enough that you can send a single message or a list of messages as the chat history, okay? \n",
    "\n",
    "Then, the new message that came from the LLM, I need to append that to the chat history as well. So I will write `result.content`, okay? And finally, when we come out of the loop, I can print the entire chat history. Save. \n",
    "\n",
    "Now let's exit and run this code again. Hi, which is greater, 2 or 0? 2 is greater than 0. Now multiply the bigger number by 10. Now you can see 2 multiplied by 10 is equal to 20. So, since our LLM now has the entire chat history, it can easily understand what to do by reading that chat history. Now if I exit here, this is my entire chat history. You can see this is my entire chat history, okay? \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318953af-e7d0-43a6-9c7b-03c754826aac",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image34.png\" alt=\"Chatbot\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4889ccf-4f35-4f32-828e-8fd350cb92c6",
   "metadata": {},
   "source": [
    "```\n",
    "So we created our first chatbot, a problem arose, and we solved that problem as well, okay? But is this a perfect chatbot? No, there are still some problems. Let me tell you what the problem is. So, in this chat history that you see, if you count, there are probably eight messages in total. One, two, three, four, five, six, seven messages, okay? \n",
    "\n",
    "Now, what is the problem with our chat history? We have kept all the messages there as they are. Now, if I show you this chat... History If I show you, how will you understand who sent \"Hi,\" the AI or the user?  \"Hello, how can I assist you today?\" Who sent this, the AI ​​or the user? We don't have this information. \n",
    "\n",
    "So what I'm trying to say is that we have all the messages stored in one place, but we don't have the information about who sent which message, and this is a problem. As your chat history grows, it will become more difficult for your LLM to understand whether a particular message was written by itself or if the user said it. And then, in the future, the conversations will start to get messed up. \n",
    "\n",
    "So, it's recommended that whenever you're talking to an LLM and maintaining a chat history, you should not only store all the messages, but you should also store who sent each message. So ideally, you would want to maintain a dictionary like this, where it says \"User,\" then the message, then \"AI,\" then its message, then \"User,\" then its message, and so on. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e766164-8af5-44dd-bb8d-d4c2c4e71e1a",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image35.png\" alt=\"Chatbot\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf33fa2-7752-47db-ab9b-a028d5e4f8f1",
   "metadata": {},
   "source": [
    "```\n",
    "You should maintain a dictionary in this way. Okay, so the problem now is that when I'm manually appending here, I also have to specify what the user typed, and I also have to specify that the user typed it. So instead of a list, I'll have to implement a dictionary to store all this role information.\n",
    "\n",
    "\n",
    "So the problem now is that when I manually go and append here, I also have to specify what the user typed, and at the same time, I have to specify that the user typed this. So instead of a list, I would have to implement a dictionary to store all of this information.\n",
    "\n",
    "But the good part is that LangChain has already identified this problem, and therefore, you don't need to do this yourself. LangChain has built-in classes that do exactly this for you. So next, I'm going to teach you about the different types of messages in LangChain.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5807afa2-6593-49d3-9d6f-a8428de97376",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image36.png\" alt=\"messages.py\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image37.png\" alt=\"messages.py\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9adf2-b7cd-4605-8963-fa1babd64d59",
   "metadata": {},
   "source": [
    "```\n",
    "There are basically three types of messages in LangChain. The first is system messages, the second is human messages, and the third is AI messages. LangChain contains these three types of messages, and with the help of these three types of messages, you can create any kind of chatbot. \n",
    "\n",
    "So, first, let me show you what these three messages are and how to create them. So, let's create a file to understand all the messages. So, we're creating a new file, we're calling it messages.py. And what we're doing here is first importing our three types of messages from LangChain_core.messages. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f12fe-f833-4282-9312-22c360dd74b4",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image38.png\" alt=\"messages.py\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image39.png\" alt=\"messages.py\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee208b-4d5f-47a1-9e37-dfeaa5d4887c",
   "metadata": {},
   "source": [
    "```\n",
    "The first is system messages, the second is human messages, and the third is AI messages. Okay, so before I tell you about them one by one, let's do one more thing. From LangChain_openAI, let's import a chatOpenAI and from dotenv import load_dotenv. Let's call and form a model. \n",
    "\n",
    "Okay, now let me tell you what a system message is, what a human message is, what an AI message is. So, the simplest is a human message. Basically, the message that a user sends to an AI is \"Tell me the capital of India.\" That's a human message. Okay, so what an AI message is? The message that the AI ​​replies, \"The capital of India is New Delhi.\" \n",
    "\n",
    "Okay, and the third is a system message. A system message is basically a system-level message that you send at the very beginning of a conversation, like if you start by writing, \"You're a helpful assistant, answer all my queries, patiently,\" or \"You're a very knowledgeable doctor, tell me about all the medical queries, in a very efficient manner.\" So, this is called a system message. \n",
    "\n",
    "This is always at the top, and you send it at the beginning of a conversation. Okay, so what we'll do is we'll deal with all three types of messages one at a time. So, first, I'll start with a system-level message. Messages I'm creating a list by speaking, and in this list, these messages are kind of like our chat history.\n",
    "\n",
    "Okay, and what I'm doing is first I'm creating a system message, and the content of the system message is \"Are you a helpful assistant?\"\n",
    "\n",
    "Okay, then I'm adding a human message to my chat history, and the content of the human message is \"Tell me about language.\" Okay, so this is kind of my message. Now what I need to do is I need to invoke the model, and there I'm sending these messages. \n",
    "\n",
    "Okay, so what will happen is our AI will receive these two messages. The AI will then give us the result, and what we'll do is extract the content of that result and create an AI message from it. Okay, okay, and I'll append this to messages, like this. And once this is done, I'll finally print the messages.\n",
    "\n",
    "Okay, so I hope you understand the whole flow.  First, we wrote down the input we needed to send to the AI. We took the messages inside, then sent them to the model. The model gave us the result, which we converted into an AI message and put back into the messages as in our chat history. \n",
    "\n",
    "Now we're printing our entire chat history. Okay, so if I save this and run python messages.py, look, guys, this is the output. First in our list is a system message, the content of which is this. Besides this, you have some other parameters, which you can set if you want. The second message is a human message, the content of which is this, additional keywords and response metadata. Then the third is an AI message, the content of which is this, and some additional keywords and metadata. \n",
    "\n",
    "Okay, so this is what you do. If you ever want to chat with your LLM, you chat through these messages. So what we'll do is integrate this into our chat bot application.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a2794-d9a8-4c42-8416-69507b68b600",
   "metadata": {},
   "source": [
    "# **Integrating of types of messages**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image40.png\" alt=\"chatbot-messages.py\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image41.png\" alt=\"chatbot-messages.py\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image42.png\" alt=\"chatbot-messages.py\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image43.png\" alt=\"chatbot-messages.py\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0612b-0a46-486d-99b1-8dd1636c9d78",
   "metadata": {},
   "source": [
    "```\n",
    "So I will go back and first of all, I will import from LangChain under core.messages, I will import SystemMessage, HumanMessage, and AIMessage, okay? And what I will do is, in the chat history, right at the very top, at the beginning, I will add a system message that says, \"You are a helpful AI assistant.\" You can write anything here. \n",
    "\n",
    "Currently, I don't want to assign any specific role, that's why I wrote this, okay? So this is the first message in my chat history. After that, when the user types their message, what I will do while appending to the chat history is, I will first convert the user's input into a HumanMessage. So I will write HumanMessage content equals this, okay? \n",
    "\n",
    "So now what's happening is, it's going as a HumanMessage label, okay? After that, when the result comes, I will also convert it into an AIMessage like this, okay? And that's it, guys. \n",
    "\n",
    "Now if I run this chatbot again, so, Hi, how are you? Tell me about yourself. What's your birthday/birth date? Exit. Now you can see this is our chat history, and we have solved our problem. Every message is now labeled, whether it's a system message, a human message, or an AI message. \n",
    "\n",
    "So now in the future, when we are talking to our LLM, and no matter how long our chat becomes, it doesn't matter, our LLM will always be able to understand who said what. And that's the advantage of using these messages. \n",
    "\n",
    "So going forward, whenever we create any conversational chatbot where we are directly chatting with the LLM, we will create these messages, we will create this labeling, and that's the whole idea behind messages. So I really hope I was able to explain to you what messages are and how you use them in LangChain.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c10c1-5f2e-4b80-b24e-f4d6bdd5a5af",
   "metadata": {},
   "source": [
    "# **Chat PromptTemplate**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image44.png\" alt=\"Chat Prompt Templates\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed22b2-0912-4af8-9143-ced9009cd3bf",
   "metadata": {},
   "source": [
    "```\n",
    "Chat Prompt Templates:\n",
    "\n",
    "So guys, we've covered a lot in this video so far, and before we move on, I'd like to give you a quick recap of what we've covered so far through a logical diagram.\n",
    "\n",
    "So, we've learned that the LLM we use models the invoke function. You can use it in two ways. The first way is to send a single message to the invoke function, just like we've been doing so far in the previous video. \n",
    "\n",
    "Now, you use this when you want to send single, standalone queries to your LLM, like uploading a research paper and generating a summary. This is a one-time query. Okay, you sent a query, you got your answer, and the conversation ended. So, you can do this by sending a single message. We also learned that there are two ways to do this. You can send a static message, just like we did in the previous video. Or you can also send dynamic messages. Okay, and then we saw that to send dynamic messages, we used the PromptTemplate class, where we created placeholders and then filled those placeholders at runtime. So, that's the first part. \n",
    "\n",
    "Another way to use invoke is to send a list of messages, just like we did a moment ago when creating a chatbot. You generally do this when you need a multi-turn conversation. Let's say you're building a chatbot, and you'll talk to an LLM in small, \"Hi, how are you?\" He gets a reply, \"How can I help you?\" You say, \"I need some help with my homework.\" He says, \"Okay, tell me what exact help you need.\" \n",
    "\n",
    "So, there's a proper conversation going on, so you can also send a list of messages using invoke. Within the function, we've learned how to send starter messages. There are three types of messages: system messages and human messages. And what we're getting back from there is an AI message, which we're sending back to the edge chart history. \n",
    "\n",
    "Okay, but we haven't learned how to send dynamic messages yet. Just like you could send dynamic messages to single messages, you can also send dynamic messages to a list of messages. There's a class for this in LangChain. This class is called the Chat Prompt Template. Okay, it's exactly like the Prompt Template, but you use it when you're sending a list of messages to your LLM. \n",
    "\n",
    "Okay, so next we're going to learn about the Chat Prompt Template. Okay, let me tell you again. You use the Chat Prompt Template when you're working with a list of messages and you need to create dynamic messages within that list of messages.  Like I'm showing you a Let me give you an example. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf9584-a35e-4e64-b8f2-a529493151fc",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image45.png\" alt=\"Chat Prompt Templates\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e92578-4f38-426e-a7e7-69c6806d799d",
   "metadata": {},
   "source": [
    "```\n",
    "Suppose you first want to make your system message dynamic. Okay, maybe your system message is You're a helpful domain expert. This is your message. If you don't want to decide in advance which domain the person is an expert in, then this is a dynamic system message. Similarly, your human message can also be dynamic. Explain about the topic. Now, you don't want to decide the topic in advance, but you want to send this prompt. The topic will be provided by the user. So, suppose you want to create this prompt. In this situation, both the system message and the human message are dynamic. So, we'll need to use the chat prompt template. Okay, let me show you how it works.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aaafe8-d320-4f9b-8152-4b82c0ae6687",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image46.png\" alt=\"Chat Prompt Templates\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae99c3-263a-43ed-bcf3-47c70de3af0f",
   "metadata": {},
   "source": [
    "```\n",
    "So it's very simple, just like the prompt template. What you'll do is, I've created a new file called chat_template.py. Here, you'll import ChatPromptTemplate from Langchain Core's prompts. Then, you'll import SystemMessage and HumanMessage from Langchain Core.messages.\n",
    "\n",
    "Now, you'll form your chat template, just like you formed the template there. Here, we're calling it a chat template. To form the chat template, you'll create an object of ChatPromptTemplate, and here you'll send the messages inside a list. So our first message is a system message, and its content will be \"You are a helpful domain expert.\" Here we've created a placeholder. And secondly, I want to create a human message where I've written, \"Explain in simple terms what is {topic}.\" Okay, my template is ready. \n",
    "\n",
    "Now I just need to fill this template, provide values to its placeholders. So I'm writing prompt = chat_template. Here also, there's an invoke method, and in invoke, I'm passing a dictionary. For domain, I've put \"cricket expert,\" and for topic, I've put, let's say, \"Explain what another thing in cricket is.\" \n",
    "\n",
    "Okay, and now what I'm doing is, I want to print the final prompt that's being generated. Let's run this code once. Now, as soon as I run this code, the code will work, but you'll see a problem. The problem is that this is the output you're getting. \n",
    "\n",
    "Here, a list of messages is being printed, where the system message says \"You are a helpful domain expert,\" exactly as it is, and similarly, the next human message also says \"Explain in simple terms what is topic.\"\n",
    "\n",
    "So basically, it didn't get filled. Okay, and this is a bit... The weird thing about LangChain is that the behavior here isn't exactly similar to what you find in the prompt template, so that's a bit strange.  LangChain can be a little confusing in some aspects. It's mostly because it's not a very mature library yet, so you see some weird behavior. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc03cf-464c-441d-aedf-776446f60cb0",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image47.png\" alt=\"Chat Prompt Templates\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image48.png\" alt=\"Chat Prompt Templates\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb410eda-1654-4faa-b29c-85c3fd8ef987",
   "metadata": {},
   "source": [
    "```\n",
    "If you want this code to work correctly, you'll have to write it a little differently. So what you'll have to do is, rather than passing the system message like this, you'll have to create the system message in a different way. \n",
    "\n",
    "You'll send a tuple where you send two items: the first explaining the role, that it's a system message, and the second is the string you'll send like this. And then you'll do the same for the second message. You'll first explain the role and then send the message. Okay? And you'll remove this. So basically, rather than using these classes, this is another way to create messages. \n",
    "\n",
    "We'll use this method. So let's remove this code, save it, and run the code again. And you would see that it's working now. \"You are a helpful cricket expert. Explain in simple terms what is...\" It's working. Okay? So that's a bit of weird behavior. \n",
    "\n",
    "Secondly, in some places you'll also see this syntax where the `ChatPromptTemplate.from_messages` function is used. This code will also give you the same output. See, it's giving you the same output. But I would recommend that you use what I'm showing you. This is given in the documentation of the latest version of LangChain. Okay? \n",
    "\n",
    "So this is how you can create a dynamic set of messages. Okay? So the ChatPromptTemplate has exactly the same function as the PromptTemplate: to create dynamic templates. The only difference is that you use PromptTemplate for single-turn messages and Chat... You use the prompt template for multi-turn conversational messages where you have multiple messages. To fit the placeholders into these multiple messages, you use the chat prompt template. Okay? So I hope you understand the chat prompt template.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214ed8b-001b-4c87-bcb4-55c81f19344a",
   "metadata": {},
   "source": [
    "# **Message Placeholder**\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image49.png\" alt=\"Message Placeholder\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image50.png\" alt=\"Message Placeholder\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image51.png\" alt=\"Message Placeholder\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847268be-548d-4adf-b198-502fc09d436b",
   "metadata": {},
   "source": [
    "```\n",
    "Message Placeholder:\n",
    "\n",
    "Now we'll cover one last thing in this video, and that's the message placeholder. Okay, so this is also a useful thing that you might see in some places later on. First, I'll tell you its definition, and then I'll explain through an example where message placeholders can be useful. So, as you can see here, it says, \"A message placeholder in LangChain is a special placeholder used inside a chat prompt template (which we just learned about) to dynamically insert chat history or a list of messages at runtime.\" \n",
    "\n",
    "Okay, so far we've seen that you can send a list of messages to the LLM, and to make this dynamic, we learned about chat prompt templates. Now, if you need to insert many messages in between, that's where message placeholders are used. Let me explain with an example. \n",
    "\n",
    "Let's say we have a company, and we're building chatbots for Amazon. Okay? Now, on Amazon, a user can talk to the chatbot. So, let's say a user told the chatbot, \"I need a refund for this particular order.\" The chatbot generated the refund and sent a message saying, \"You will get your refund in three to five business days.\" Now, the user didn't receive the refund, so two days later, the user will come back and ask, \"What is the status of my refund?\" Now, when they start a new chat, we need to keep track of the previous chat history so that we can answer their question about the refund status, right? \n",
    "\n",
    "So, generally, when you build chatbots, you have to maintain a chat history. So, we'll take all the conversations we've had today and store them in a database. Now, in the future, on some other date, when a new chat session starts again, what we'll do is first load the previous chat history. Okay? \n",
    "\n",
    "And then the chatting continues. This is the problem that message placeholders solve. What you'll do is... For all your chat history up to this point, you create a placeholder, and we call that placeholder a message placeholder. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14bb68-d008-42d5-9a3e-54f433d1fc7b",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image52.png\" alt=\"Message Placeholder\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc25c7d-e330-48c6-8694-2e8518f7b79e",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, I'll show you the whole thing; it will make more sense. So, let's say our customer has had this conversation with our chatbot so far. They said, \"I want to request a refund for my order 12345.\" Our chatbot replied that day, \"Your refund request for order has been initiated. It will take 2-3 business days.\" Now the chat is over. What we did was take these two messages and save them somewhere. Ideally, you store them in your database on the cloud. For now, we've saved them in a text file. Okay? \n",
    "\n",
    "Now this customer doesn't have patience, so they came back the next day and are now typing, \"Where is my refund?\" Okay? So, to understand this question, we need to know the context of the previous messages, right? So, we'll have to load those previous messages, and where will we load them? In our chat prompt template. And that's why we'll create a message placeholder. Okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f1b7c-0344-47a0-b16f-9aa8ce83a873",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image53.png\" alt=\"Message Placeholder\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image54.png\" alt=\"Message Placeholder\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43546fce-b7db-4e33-904e-1c08586be34b",
   "metadata": {},
   "source": [
    "```\n",
    "So, look, I'm writing the whole code. From `langchain_core.prompts`, we will import two things: one is `ChatPromptTemplate` and the second is `MessagePlaceholder`. Okay? Now what we have to do here is, first, we have to create a chat template, then we have to load the chat history, and then we have to create our prompt.\n",
    "\n",
    "We will work in these three steps. So, first, I'm creating a chat template. I will create a variable by the name of `chat_template`, and this will be an object of `ChatPromptTemplate`. Now, what we will do is add messages. So, first, I want to add a system message. The system message is, \"You are a helpful customer support agent.\" \n",
    "\n",
    "And then I want to add a human message where it says whatever query the user asked. And this query is from the user. The user asked today, and their query is \"Where is my refund?\" Okay, but \"Where is my refund\" won't be understood by the AI, it won't be understood by the LLM because at this point, it doesn't have any context of the past messages. \n",
    "\n",
    "That's why what we will do is create a message placeholder between the system message and the current human message. Okay? And the name of this message placeholder is \"chat history.\" So now, all the messages that have been exchanged between our customer support agent and this customer in the past will automatically come here. \n",
    "\n",
    "So that when the new message from the human comes, it will have the context of all the previous messages. So you can understand, our chat history, all the chat we've had with the customer so far, the entire history of that chat, we are going to put it in this placeholder. Okay? So I hope you understand up to this point. \n",
    "\n",
    "Now what we need to do is, first, load the chat history. So for that, we will use simple Python code. So with open, our file name is chat_history.txt, it's a text file, we are opening it as f, and what we are doing is calling f.readlines every time, which will pick up each line one by one and put it in a list. And we want to create a list called chat_history and append each line to this chat_history. So we are appending it. Okay? If you don't understand this code, don't worry, I'll show you by printing the chat history. So if I run this code now...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d1b3f-96e3-4915-b251-2d2b93f8b7d0",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Prompts/Image55.png\" alt=\"Message Placeholder\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Prompts/Image56.png\" alt=\"Message Placeholder\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fee7f6-ac31-4275-843f-fcf00f4a4842",
   "metadata": {},
   "source": [
    "```\n",
    "So there's an issue here, first a comma will come here, and you can see this has become a list of messages where the first message is a human message, \"I want to request a refund,\" and the second message is an AI message where it says, \"Your refund request.\" \n",
    "\n",
    "Okay, so we are able to create a list of messages. Okay, now we just need to put this list of messages in this place. Okay, so for this, what we will do is we will use the chat template that we created above, we will call its invoke function, and here in the dictionary, we will send the chat history. \n",
    "\n",
    "In the value of the placeholder, we will send the chat history, and in the query written above, which is our second placeholder, we will send some query. Let's say our query is \"Where is my refund?\" In fact, what we can do is convert this into a human message. Okay, now all we have to do is, first, let's turn on word wrap. Second, from langchain_core.messages, let's import HumanMessage. \n",
    "\n",
    "Okay, I don't need to do this because I have already specified here that whatever query comes here will be a HumanMessage. So I don't need to do this. Okay, anyway, so I said that in my chat prompt template, instead of the first placeholder, I want to put this chat history that I have fetched from my database, and instead of the second placeholder, the query, I want to put whatever the user asked me at this point today. \n",
    "\n",
    "Okay, and from this, I will get my final prompt that I will send to the LLM. Currently, I'm not sending it; I'm simply showing you by printing whatever my prompt is. Saved it, ran it, and you can see, guys, now this is my final list of messages. Here, first comes my system message, then whatever my chat history was, and then finally, the message I sent today as a customer, \"Where is my refund?\" The refund is coming, so now \"Where is my refund\" has the entire old chat history, so the LLM will automatically understand the entire context, okay? \n",
    "\n",
    "So this is the basic idea behind using message placeholders. A message placeholder is a placeholder where you create a placeholder for a set of messages. Generally, you use it to retrieve and store the chat history, okay? It makes the whole process very easy, okay? \n",
    "\n",
    "So I hope you have understood this well. With that, I will conclude this video. We have covered everything related to LangChain. I have planned to create a playlist related to prompt engineering in the future where I will dive a little deeper into the different prompting techniques. \n",
    "\n",
    "I will work on few-shot templates, chain of thought, which is a technique, and we will talk about that.  Besides that, I will cover some general knowledge around prompting in that playlist. But for today, whatever we needed in LangChain, we have covered everything in this video. And if anything is left, please let me know in the comments, and I will cover it in future videos. \n",
    "\n",
    "I really hope that everything we have covered in LangChain up to this point is clear to you, and I will make sure it remains that way going forward, okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c921c72-8e82-43a0-9436-9d6c387d2f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
