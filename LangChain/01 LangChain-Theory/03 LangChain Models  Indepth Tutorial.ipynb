{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928954a5-ce6f-4871-89cd-f2daaa917849",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "```\n",
    "In today's video, we'll start with the practical part, and today we'll be discussing the first component of LangChain, which is the model component. So, we'll learn about all the different types of models you can use in LangChain, and not only that, we'll also code them all. So, overall, you're going to have a lot of fun because building things and interacting with them and seeing things happen is a good feeling, right?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a600ea2-042c-4cef-b58e-925994088bde",
   "metadata": {},
   "source": [
    "# **Recap**\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image01.png\" alt=\"Recap\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf71334-fe46-4eab-99df-f7a2800ff279",
   "metadata": {},
   "source": [
    "```\n",
    "So, with that, let's start the video. So guys, before starting the video, let's quickly recap what we have covered so far in this playlist. So far, I have uploaded two videos in this playlist. \n",
    "\n",
    "In the first video, I gave you a very detailed introduction to LangChain. I told you exactly what LangChain is and why LangChain is needed. Then I also told you what kind of applications you can build with the help of LangChain and what are the alternatives to LangChain.\n",
    "\n",
    "After that, in the second video, we covered a very important topic: what components exist in LangChain. We looked at different components such as models, prompts, agents, chains, etc., and for each component, I tried to explain to you in great depth what the need for that component is, and I also gave you relevant examples so that you get an idea of where different components are used. \n",
    "\n",
    "Now, today's video is about one of those components. In fact, the first component we studied, models, today's video is completely about models. So, if you watch this video from beginning to end, we will cover models in great depth, and I will tell you in detail how you can use every type of model in LangChain.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b999cd-adb0-4f92-aec1-af32eb378fff",
   "metadata": {},
   "source": [
    "# **What are models?**\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image02.png\" alt=\"What are models?\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474d340-9bb7-4ab2-80ae-2cef73cdff88",
   "metadata": {},
   "source": [
    "```\n",
    "But before discussing the entire plan of action, I want to quickly tell you what models are, although I had already explained this in the last video. But once again, We can revise this.\n",
    "\n",
    "What are Models?\n",
    "\n",
    "So here, it says the Model Component in LangChain is a crucial part of the framework designed to facilitate interactions with various language models\n",
    "and embedding models. In simple terms, currently in the world, different types of AI models exist, okay? And the problem is that when you try to interact with different AI models, as in, try to write your code on them, the AI models from different companies behave in different ways.\n",
    "\n",
    "So, LangChain's model component kind of provides you with a common interface that helps you to easily connect with any type of AI model. So, in short, the model component in LangChain is nothing but an interface that helps you to connect with different kinds of AI models. Which kinds? Language models and embedding models, okay? \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b5dbd-71dd-4c52-9f34-125cf17db223",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image03.png\" alt=\"What are models?\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fa706-abe8-4a53-a36c-a181bca47926",
   "metadata": {},
   "source": [
    "```\n",
    "So, this is a very important thing, and this diagram is very important. You have to remember that in LangChain, there are two types of models: one is language models and the other is embedding models. I'll quickly tell you about both. So, language models are models where you give some text as input to the model, such as \"What is the capital of India?\" and this model understands that text, processes it, and returns a text output to you,\n",
    "such as \"New Delhi.\"\n",
    "\n",
    "So, these types of models, these types of AI models, are called language models. So, in LangChain, these models... There's very strong support for working with these models, and the second type of model that we'll be working with, and which LangChain works very well with, is embedding models. So, in embedding models, what happens is that you give a text as input. You give a text here, let's say your text was again, \"What is the capital of India?\" Now, this time, the model will try to understand what your text is, but this time, instead of returning text, it will give you a series of numbers. And these numbers are what we call embeddings. We've talked about this in quite a bit of detail. Embeddings are nothing but vectors, a set of numbers, and these numbers kind of represent this text or represent the context of this text. Okay?\n",
    "\n",
    "So, you'll find these two types of models in LangChain. They are different models, so they have different functions. With the help of these language models, you create applications like chatbots because you're sending text here, it's understanding the text, and replying back in text format. So, it helps you create chatbot applications.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4368c2-f8b1-4f2b-9e61-10f45d00bdbe",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image04.png\" alt=\"What are models?\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26141879-a717-43ae-957a-685eea037f20",
   "metadata": {},
   "source": [
    "```\n",
    "Embedding models help you with semantic search. I've talked about this topic in quite a bit of detail in the last two videos. So, they help you conduct semantic search, and that's why you can create RAG-based applications with their help. Okay? Now, we haven't talked about RAG, but that doesn't matter. I gave you a very good example of this in video number one, where I showed you how you can use an LLM to... You can have a chat about your documents, okay? \n",
    "\n",
    "So that was a RAG-based application, right? And there you need semantic search, and you perform that semantic search with the help of these embedding models, okay? \n",
    "\n",
    "So if we summarize our discussion, LangChain has a component called Models component, which is basically an interface that allows you to interact with different types of AI models. It's a common interface. \n",
    "\n",
    "Now, how many types of AI models can you interact with? Two types of AI models. One is Language, the second is Embedding. Language models are those where you send text and get text as output. Embedding models are those where you send text as input, and in return, you get numbers, which help you perform semantic search, okay? \n",
    "\n",
    "So I hope you first understood what the Models component is in LangChain. Now that you understand this, let me tell you what our plan of action is going to be in this video.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c73895-c794-42b2-886c-267a063c2a93",
   "metadata": {},
   "source": [
    "# **Plan of Action**\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image05.png\" alt=\"Plan of Action\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb4b42-aca9-4961-97f4-0ddcdd7f57bc",
   "metadata": {},
   "source": [
    "```\n",
    "Plan of Action:\n",
    "\n",
    "So my plan of action is very simple. Today's video is completely coding-based. We will actually write code and interact with these models. So in the first part of the video, I will teach you how to work with language models, okay? \n",
    "\n",
    "And I will teach you how to work with different types of language models. So I will teach you how to work with closed source models. Closed source models mean models that are paid. In this, we will interact with OpenAI's GPT models, then we will interact with Anthropic's Claude model, and\n",
    "Google Gemini models, okay? \n",
    "\n",
    "And then, within language models, we will also learn to interact with open-source models. Open-source models are generally available on Hugging Face. So we will learn to interact with these open-source models with the help of the Hugging Face library and LangChain, okay? \n",
    "\n",
    "And along the way, we will also build a small chatbot application, okay? Once the language models are covered properly, then we will move towards embedding models, and in embedding models, I will also teach you how to work with both types of embedding models: closed source, where we will work with OpenAI's embedding model, and then we will also work with open-source embedding models, where we will again go to Hugging Face, find an embedding model, download it to our machine, and generate embeddings, okay? \n",
    "\n",
    "And finally, I will show you how to build a small document similarity application where we will calculate the similarity between two documents and basically generate a score to determine which documents are most similar, okay? So roughly, this is going to be the plan of action for this video. \n",
    "\n",
    "So I really hope I hope I've given you the full context of the video. Now, at , let's begin our deep dive, and we'll start with our first topic: language models.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f01b0-feac-4125-be6c-c0baeaa0f0c8",
   "metadata": {},
   "source": [
    "# **Language Models**\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image06.png\" alt=\"Language Models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27296e-767c-4f83-933d-f42b6116a873",
   "metadata": {},
   "source": [
    "```\n",
    "So now we will shift our focus towards language models. So, as I mentioned a little while ago, language models are AI models where you give a text as input, these language models understand and process that text, and then give you a text as output. Okay? \n",
    "\n",
    "Now, it's important to understand that there are two types of models within language models: one which we call LLMs, and the other which we call chat models. \n",
    "\n",
    "Now, if I tell you the distinction between these two, there is a very simple distinction between them. LLMs are general purpose models, which means that you can use them in any kind of NLP application. You can do text generation with their help, text summarization as well. You can also do code generation with their help, and you can also do question answering with their help. These are general purpose language models. And what is their specialty? You give LLMs a string in plain text, and they give you back a string in plain text. Okay? \n",
    "\n",
    "Now, the interesting thing is that you don't need to learn much about LLMs because they are kind of outdated, and in LangChain too, you will gradually notice that their support is ending, and the new version of LangChain is telling you that if you are working on a new codebase, starting a new project, then please don't work on LLMs. Okay? \n",
    "\n",
    "Because who is replacing LLMs now? Chat Models Now I'll explain to you what chat models are. So chat models are these language models that are used for conversational tasks.  That means if you want to create a chatbot, then chat models are great for that purpose. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5716db66-e174-4b44-8598-11e10dcc7fd8",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image07.png\" alt=\"Language Models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66b448-b2bb-4c11-a3ea-d349cf950bc9",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, look here, it says language models that are specialized for conversational tasks. They take a sequence of messages as inputs and return chat messages as output. So what happens here is that with your chat model, you can send multiple messages as input at once, and your chat model\n",
    "can understand this entire conversation and then reply to you with a series of messagesin return. \n",
    "\n",
    "So the most important difference between an LLM and a chat model is that LLMs are kind of a general-purpose model that you can use in any type of NLP application. Chat models are specialized models that are used more in conversational tasks. So if you want to create chatbots, agents, coding assistants, then chat models are used more for these kinds of things. So today, if you talk about it, in LangChain, it's being said that you should do more work with chat models, and the support for LLMs is gradually decreasing. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72145cb4-a69c-47ff-9325-a46e8a684a21",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image08.png\" alt=\"Language Models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb96920-c906-44b8-a2c0-f06feaf95539",
   "metadata": {},
   "source": [
    "```\n",
    "This table here beautifully represents the difference between both types of models. If you talk about the purpose for which LLMs were created, the purpose of LLMs was free-form text generation. You can also get question answering done, text summarization done, translation done.That's fine. Whereas the purpose of creating chat models was to enable multi-turn conversations between the user and the AI, okay? \n",
    "\n",
    "If you talk about how these two models were trained, the training of LLMs was a very general type of training where you took a lot of text and trained your model on it, such as books, articles, Wikipedia text, etc. Whereas in the process of creating chat models, what happened is that not only was it trained on a lot of text, but once this training was done, it was fine-tuned on chat datasets where we fed a dataset into this model where conversations were going on between multiple people, such as WhatsApp chats or Reddit conversations, or you never know, the conversation between the AI and the user was also taken and used for training, okay? \n",
    "\n",
    "Then if you talk about memory, there is no concept of memory in LLM models.  This means that when you talk to it, it doesn't remember the previous conversation. Whereas the chat models support conversation history. This means that you will get this feature that your AI model will remember what was discussed in your entire conversation. \n",
    "\n",
    "After that comes role awareness. You can assign a role to chat models if you want. You might have seen this many people design prompts in this way: \"You are a very knowledgeable doctor. Now tell me about this particular disease and its symptoms.\" \n",
    "\n",
    "So here, when you are saying that you are a very... If you're a qualified doctor, you're sending a system-level message Where you're assigning a role to your AI, you get this feature in the chat model, whereas in LLMs, you don't get any such feature. You can't assign roles there. Chat models have this feature that they can understand what they themselves are, who the user is, and who the AI is. It understands all of this.\n",
    "\n",
    "Okay, if you talk about example models, these are the example models that will fall into the LLM category, and these are the models that you would put in the chat models category. Okay? \n",
    "\n",
    "And lastly, at what time should you use which model? If you want to do something text generation-based, or create a summarizer, or use it for translation, or use it for code generation, then you can maybe use LLMs. But if you want to create conversational AI chatbots, virtual assistants, or customer support chatbots, or AI tutors, then you use chat models. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed93e2c-3c5f-496e-8db9-a230ad0e9b66",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image09.png\" alt=\"Language Models\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047adc7-9cd7-4984-a012-eb570fa9b743",
   "metadata": {},
   "source": [
    "```\n",
    "And you will notice that most of the AI applications being built today fall into this category. That is why gradually, this entire Generative AI industry is shifting towards chat models, and the support for LLMs is slowly fading away. \n",
    "\n",
    "It's not that you can't still build applications using LLMs, but it's not recommended, especially with the recent versions of LangChain. So you will notice that in this particular tutorial, in this particular video, our focus will be more on chat models. But it doesn't mean that I won't teach you how to work with LLMs.\n",
    "\n",
    "In fact, first I will show you how to work with LLMs, and then when you understand that a little, then we will discuss chat models, and then going forward, throughout the video, we will only talk about chat models.\n",
    "\n",
    "So I really hope you understood what language models are and... You've also understood the two types of LLMs and chat models. So now we are ready to dive into the coding part.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ccc5c4-2dbb-4a3d-a3f0-91b1ab7681e5",
   "metadata": {},
   "source": [
    "# **Setup**\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image10.png\" alt=\"Setup\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image11.png\" alt=\"Setup\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image12.png\" alt=\"Setup\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image13.png\" alt=\"Setup\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a97c3f-55c2-40f2-bdc8-1bebb76f0be8",
   "metadata": {},
   "source": [
    "# **Demo**\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image14.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image15.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image16.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image17.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image18.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c4fea-e094-47a3-8cec-c88d256dd7e6",
   "metadata": {},
   "source": [
    "```\n",
    "So guys, our setup is ready now. We will see our first demo. My plan is that first I will show you LLMs and in LLMs I will only show you using OpenAI's AI models because you don't need to study LLMs much, but after this I will teach you chat models where we will touch upon all the major API providers. So we will work with OpenAI, Anthropic, Google, and then also with Hugging Face. \n",
    "\n",
    "Okay, so first let's discuss how LLMs work alongside LangChain. There is only one problem, since we are going to use OpenAI's APIs to run LLMs, we will first need OpenAI API keys. So for this, you will first have to go to this URL: platform.openai.com and here you will have to create an account if you haven't already created one.\n",
    "\n",
    "Now the only problem is that you can only access OpenAI's API keys when you have some credit available in your account, and it should be a minimum of $5. Unfortunately, you will have to recharge it. Until a year ago, OpenAI used to give free credits to new users, but now, as far as I know, they don't give any free credits, which means you will have to buy credits by paying money. \n",
    "\n",
    "So if you can try it, that's great, otherwise you can also contribute with two or three people if you are a student. So what you have to do is you have to go and recharge your account. I already have $5 in my account. I've recharged my account, and trust me, $5 is more than sufficient. You can do quite a lot with $5, okay? \n",
    "\n",
    "Why am I telling you to recharge and become a paid user? The reason is that even now, most companies use OpenAI's APIs to build LLM applications. So, even though I'm going to show you the free option using Hugging Face in a little while, there's a chance you might not be able to use it when you work as an AI engineer in a company. \n",
    "\n",
    "There's a chance you'll have to work with OpenAI. That's why, if you want some practice in that sense, I would recommend that you can invest $5. Okay? Once you've topped up your account, you can go to this settings button and from there, you can go to API keys. Your already available API keys will be displayed here. I currently have one key. \n",
    "\n",
    "Now, what I can do is, if I want, I can create a new secret key. I can give it a name here. Let's give it a name, My Test Key 2. Okay? From here, you can create a project and select it if you're working on a particular project. Currently, I don't have any such plan, so I'm using the default project. And from here, you can also set permissions. And now I'm clicking on Create Secret Key, and here is my secret key. Okay? \n",
    "\n",
    "I'll copy it, and we need to use this in our project. So, what you'll do is, you'll directly use this secret key in your project. Instead of writing it in the code, you'll save it separately in a dot[.] env file. So what you'll do is create a dot[.] env file in your project. A dot[.] env file is a file where you keep your environment variables, secret keys, etc. Okay? \n",
    "\n",
    "So we'll keep our OpenAI API key here, and it's very simple to do. You'll create a variable, its name will be OPENAI_API_KEY, and you'll store it inside a string. And you would save it. Okay? Now, going forward, whenever I need to use OpenAI's APIs, I can use this key. Obviously, it goes without saying that you have to keep this key hidden from everyone. \n",
    "\n",
    "I will also delete this key after launching this video on YouTube, so that no one else can use it, right? Now what will we do? We will create an LLM application. It's not really an application, honestly. We'll write a small piece of code that will help us access an LLM from OpenAI. Okay? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e2ac3-1817-4345-a934-f45b8f84450a",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image19.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image20.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f5819-d4de-43a4-acda-cf2ad73950f7",
   "metadata": {},
   "source": [
    "```\n",
    "For that, what I will do is I will go to my LLM folder and there I will create a new file, and let's name this file llmdemo.py. Okay? And now we need to write the code here. The code is very simple, although there might be some parts that you haven't seen before, but I will explain them to you as we go along. \n",
    "\n",
    "So first, you need to import a package called LangChain-OpenAI. This is the integration package between LangChain and OpenAI. This is where the code is written that allows LangChain to understand how to communicate with the OpenAI API.From here, we will import OpenAI. Okay? \n",
    "\n",
    "And the next thing you need to import is the dotenv library. We also installed this when we were working on the requirements.txt file. You can see, if you go down here, this package that we installed is what we are loading here. What dotenv does is? it loads your secret keys from your environment file into your current file. Okay? \n",
    "\n",
    "That's why we are using it here. So... From dotenv import load_dotenv. This is a function that will load the environment variables for us, okay? So first, we will invoke this function like this. Now our OpenAI key has been loaded. Once you have loaded your API key here, the main work starts now. \n",
    "\n",
    "Now what you will do is you will create an object of OpenAI, and for now, we are putting that object in a variable. We are calling it LLM, and here you will specify which OpenAI model you want to communicate with. So the name of the model we want to communicate with is GPT 3.5 Turbo Instruct. \n",
    "\n",
    "I will show you later which OpenAI models are available and what their features are, but for now, we will use this model. Once you create this object, now you can call a very important function with the help of this object, which is called invoke. \n",
    "\n",
    "Invoke, I'm sorry, is a very important function in LangChain, and you will find out later. Whether you talk about models, whether you talk about chains, whether you talk about prompts, the core components, all of them have this invoke method, and we will communicate with this particular model, GPT 3.5, with the help of this invoke method. \n",
    "\n",
    "So you don't have to do anything, you just have to pass a prompt inside invoke. what is the capital of India. So what will happen behind the scenes is that this invoke method will go and hit this model. And it will give this prompt to the model, this model will process this prompt, generate a reply, and we will get that reply back, and we will store it in a variable. Let's call that variable result, and now what we will do is print that result. And that's it. \n",
    "\n",
    "You can see how simple the entire flow is. Here we imported the libraries, here we loaded our OpenAI API key, here we created an object of OpenAI, and with the help of that object, we called the invoke function, inside which we sent our prompt. We got our response back, which we stored in the result, and now we are printing the result. That's it. Now let's run this code. So we are running the file inside the folder. Pressed Enter, and you can see guys, this is the output: The capital of India is New Delhi. \n",
    "\n",
    "Our model gave us the reply. And that's it guys, it's that simple to use LLMs. Now again, I would like to remind you that LLMs are old news, and nobody uses LLMs anymore, not even LangChain.\n",
    "\n",
    "So, even LangChain forces you to use chat models instead of LLMs. The code you're seeing here might appear in some older YouTube videos. If you're working on a new project, please make sure you don't use this code. I only showed it because I wanted to cover everything, but the chat models I'm about to show you will be the most important and relevant pieces of code.\n",
    "\n",
    "So now let's shift our attention to chat models.  Okay, I forgot to mention one thing here: if you remember the discussion we had earlier about LLMs, I told you that LLMs take a string as input (plain text) and give a string as output. So here too, you'll notice that we sent a string to this LLM, and what it sent back to us is also a string.  Okay? So this confirms that it is an LLM.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdabdf0-ee84-4c03-accd-212511d04e20",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image21.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image22.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f7fe9-fc1a-4e89-9dc6-3775a0aa91de",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's talk about the chat model interface, okay? So what we will do is, first of all, we'll go to this second folder, the one with the chat models, and here we'll create a new file. Let's call it chatmodel_openai.py. Okay? \n",
    "\n",
    "Now, the good thing about LangChain is that its entire interface is quite consistent. You have to make very few changes to the previous code, and your code will be modified for the chat model. So the first thing you have to do is, again, you will import. So you will again import from LangChain_OpenAI,but this time instead of OpenAI, you will use ChatOpenAI. Okay? \n",
    "\n",
    "This is a very big difference. So what is the difference between the two? Let me explain it to you. If you go to OpenAI, press Control and click, you will see the source code of this class. Here you will notice that it inherits from the BaseOpenAI class. Whereas if you go here and look at its code, you will see that it inherits from a BaseChatOpenAI class. If you click on this as well, you will see that the chat model class inherits from the BaseChatModel. So this is kind of the mother class for all the chat models. Later, when we use Google's chat models or Anthropic's chat models, they will all inherit from this BaseChatModel.\n",
    "\n",
    "whereas, If you talk about BaseOpenAI and then click on this as well, then this Base LLM class inheritance is the main difference between an LLM and a chat model in LangChain.  All your LLMs inherit from the Base LLM class, and all your chat models inherit from the Base Chat Model class, okay?\n",
    "\n",
    "Again, this is a bit technical. It's okay if you don't understand it; I just wanted to show you how the code is written internally, alright?\n",
    "\n",
    "Whatever it is, we have first imported from LangChain, Open AI, import ChatOpenAI, and here also you will import the load_dotenv function from dotenv. First, let's call load_dotenv and get our OpenAI key here. Now what we will do is we will create a ChatOpenAI object and here we will provide our model, okay? \n",
    "\n",
    "So I told you I would show you which models are available. If you want to check this, you will have to go to the OpenAI website. There, in the models section, you will see all the models that are available. So currently we need to go to GPT models, and you can see these are the models that are available. Not only the model name is given, but the context window is also given, and the maximum output tokens are also given. So based on this, you can decide which one to use, okay? \n",
    "\n",
    "Again, this discussion we will have later. For now, we are just using this. So let me use GPT-4. So the model we are going to use is called GPT-4, okay? And we will store this object we are creating in a variable. Let's call it model. A little while ago we were calling it LLM in that code, here we are calling it model because it is a chat model. \n",
    "\n",
    "Now this model object has a method, the same invoke method, as I said, you will see it in many places in LangChain because it is very important, but we will see the story behind it a little later when we study the Runnable interface in the future. But for now, we will just call this function and here... We will send \"What is the capital of India?\" Same question, okay? \n",
    "\n",
    "And whatever result we get from there, we will store it again in a variable. Let's call it result. And now let's print whatever is inside result. Saved. And let's change it here. We are inside the chat_models folder, and this file name is chat_model_openai. \n",
    "\n",
    "Let's run this code, and it says it can't open this file. Okay, the reason is that it should be 2.chat_model Let's run this file, and you can see this is the result. Now you will notice one thing that unlike last time, the result here is not simple plain text. You can see quite a lot here. \n",
    "\n",
    "So first of all, you see a key called content, inside which your actual answer is hidden. But along with that, there are many other additional keyword arguments, which are kind of metadata, such as how many completion tokens there were, how many tokens were in the prompt,\n",
    "how many total tokens there were, and a lot of other information. Okay? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba2032-0877-40a2-8729-a59eec05e082",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image23.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3828159-b94c-42cf-ab7b-e5be43b0f3a4",
   "metadata": {},
   "source": [
    "```\n",
    "So you can clearly see, first of all, that chat models are behaving a little differently in terms of the output provided by them. Now, generally, what you do is that if you only want to see the answer, then rather than printing the result, you fetch the content from the result. Okay? \n",
    "\n",
    "You fetch this content, and this gives you your answer. So you will have to run this code again to see the answer, and you can see this is the The output says the capital of India is New Delhi. Okay, so I hope you're getting a little idea of the difference between chat models and LLMs. You might not understand the difference very well right now, but you'll gradually start to understand it.\n",
    "\n",
    "For now, what we did was we wrote our first piece of code where we used the chat model interface and interacted with OpenAI's GPT-4 model. Now I will show you how to interact with two more different language models. Next will be Anthropic's Claude model, and after that, I will also show you how to interact with Google's Gemini model. \n",
    "\n",
    "Before using Claude and Gemini, I want to tell you something really cool. So, you can improve this code a little bit. You have the power to create this object, the Chat OpenAI object, and you can set some additional parameters here, such as... There's a very interesting parameter called temperature, and its value varies from zero to around two.  \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa3ede-8637-45d0-ba21-bc58634daa67",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image24.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8038103-b721-4b35-8ad4-0ed163cbd1c7",
   "metadata": {},
   "source": [
    "```\n",
    "In simple terms, this is a creativity parameter. You can tell your language model how creative you want its response to be. In fact, I've even written a formal definition here: Temperature is a parameter that controls the randomness of a language model's output. It affects how creative or deterministic the responses are. If you set a value in the lower range, say from 0 to around 0.3, the responses from your language model will be more deterministic and predictable. Whereas, if you set a higher value, let's say around 1.5, they will be very random, creative, and diverse. Okay? \n",
    "\n",
    "In fact, I've created a table here that will help you set the correct temperature value. If you're looking for factual answers from your language model, such as when you're working with math or code, then 0 to 0.3 is a good range. If you're looking for general question-answer explanations, then 0.5 to 0.7. If you're doing something more creative, like writing stories, telling jokes, etc., then 0.9 to 1.2. And if you need something very random and creative, you can try values ​​above 1.5, for example, during brainstorming sessions. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760b4ee-1e5a-44a9-aaf1-d49523139ed4",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image25.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be3e29-d0f5-43bb-8ff3-4ef1abf4bcc7",
   "metadata": {},
   "source": [
    "```\n",
    "So, let me show you what kind of outputs you get if you use different temperature values. Let's say I set it to zero, and now what I'll do is I'm changing my prompt, and I'm writing, \"Suggest me five Indian male names.\" Just to see what kind of names are being suggested here, and I'll run this code with a temperature value of zero. And you can see Arjun, Ravi, Sanjay, Anil, Prakash – quite regular names. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574587f3-4e3a-4e54-b574-ec1ef8984b28",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image26.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image27.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749e7aa-ad35-4506-b3a4-fedb8e8eca71",
   "metadata": {},
   "source": [
    "```\n",
    "Let's change it to 1.8 now and run it again. I don't see much of a difference, though. Arnav, Kartik... Maybe I don't know, maybe the prompt isn't right to understand this. Let's try \"Write a five-line poem on cricket.\" Okay, let's start with zero. Cricket, a game of bat and ball, under the sun, standing tall. Cheers echo as wickets fall, in the sport loved by all. A sixer's flight captivates us all. Okay, it seems decent. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78beca4-5c65-4db1-b138-4fa7cc87f73a",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image28.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efec457-1ed2-4104-b8e6-ba330152f8ec",
   "metadata": {},
   "source": [
    "```\n",
    "Let's make it 1.5 and let's run this code one more time. In the field under the sun's fury, picket boys and girls with love for cricket. The ball tossed, a moment's play, victory's cheer or a sunk wicket, proving life is defined between any pitch and wicket. I don't know, I can't judge properly. You be the judge. But anyway, this is a parameter that you use. \n",
    "\n",
    "In simple words, if I summarize, if you want to do deterministic tasks, like generating code, stay on the zero side. If you want to do more creative tasks, like story writing, poem writing, joke writing, etc., then stay a bit towards the 1.5 side. That's the advice.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a0185-2911-4360-8bf5-c18cb72f90c8",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image29.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image30.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2be66-05b8-4a21-af1d-1d3156e829c4",
   "metadata": {},
   "source": [
    "```\n",
    "Another very important parameter is Max Completion Tokens. With its help, you can specify how many tokens you want in the output. For now, you can roughly consider tokens as words. So, when I get a response back from the language model, I can restrict it to say that I want 10 words, 50 words, or 100 words in my response. Okay? Why is this helpful? \n",
    "\n",
    "Because when you're interacting with a paid language model, you have to pay per token. So, if you go to the OpenAI website, and if you go to the pricing section, whatever pricing you see there, $2, $3, whatever it is, is per 1 million tokens. So, this means that you are paying based on the number of words or tokens you request. \n",
    "\n",
    "So, sometimes, as a developer, you might want to restrict the number of tokens displayed in the output. For example, what I can do is, I can put 10 here. Now, what will happen is that when I get the response from the model, it will have a maximum of 10 tokens. \n",
    "\n",
    "Let me show you. I'll run this code again, the one I just ran, and this time you'll notice that we'll only see 10 tokens in the output. You can see these are the tokens. Okay? Tokens are roughly words, but they are not exactly words; many other things are considered in tokenization. \n",
    "\n",
    "Tokenization itself is a big topic, so we'll cover that later. For now, you can roughly assume that tokens are equal to words. Okay? So, yeah, these are the two important parameters I wanted to show you. Now that you understand these parameters, let's move on to the next chat model, which is Claude.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e7c08-8177-4a96-97d6-fd31418b5dd6",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image31.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image32.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f69b8-99f4-493b-a6cb-c3aaec48a1dc",
   "metadata": {},
   "source": [
    "```\n",
    "So, if we talk about Claude, it's also a very popular language model. In fact, you'll hear in many places that Claude has even beaten GPT models in terms of performance. So, that's why Claude is also used in many places. I mean, there's a chance that the company you work for in the future might be using Claude's APIs instead of OpenAI's APIs. Okay? So, it's possible. \n",
    "\n",
    "By the way, Claude comes from a company called Anthropic. Maybe you've heard the name. Now, what we'll do is we'll write a code where we'll hit Claude's API with the help of LangChain and get the response. Okay?\n",
    "\n",
    "The process is exactly the same. We'll have to create a new file again, and we'll name this file chat_model_anthropic.py. Okay? Now, as I said, the process is the same, so here too you'll need an API key, just like you needed in the case of OpenAI. Now, the problem is the same: just like OpenAI, Anthropic also provides a paid service, which means you'll have to pay to use their API.\n",
    "\n",
    "Now, again, it's up to you. If you want, you can pay, otherwise I would suggest you skip it and just watch the video. You don't have to do anything. You have to go to this website, you can top up your account. I've currently topped up with $5 just so I can make the video. \n",
    "\n",
    "Here you have to go to Get API Keys. I already have a key, so what I'll do is I'll click on Create Key and I'll create a new key. Again, here you can create a workspace or project. Here I'm creating a key by the name of My Secret Key. Clicked on Add, and here's my key. I copied it.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281f797-1ffc-49ee-99a5-9e0279fd4a0e",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image33.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image34.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image35.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f271b1-2a53-49a3-a9f0-7e314da6cfa4",
   "metadata": {},
   "source": [
    "```\n",
    "Now I'll go back to my environment file and here we have to create a new variable, ANTHROPIC_API_KEY. One thing I forgot to tell you, you have to write exactly the same name here that I've written. For example, if it says OPENAI_API_KEY, you have to write the exact same name. You cannot change it. If you change it, your load_dotenv function won't be able to locate the key correctly, and your code won't work. \n",
    "\n",
    "Similarly, for Anthropic, you have to write the exact same name. Now you will paste the key, and that's it. Now look at the main advantage of using LangChain: we will write the code in the exact same consistent format as we did for OpenAI, and you will notice that the same code works for Anthropic as well. \n",
    "\n",
    "First, you will import ChatAnthropic from langchain_anthropic. Just like there was ChatOpenAI, here it's ChatAnthropic. Then you will import dotenv and call dotenv.load_dotenv(). Now you have the Anthropic key. Now, again, you will create a model, and this will be an instance of ChatAnthropic. \n",
    "\n",
    "The model we will use here has a slightly long name, so let's go back to console.anthropic.com and click on Explore Documentation. User Guides. Here we can see the models. There are multiple models here. Currently, the most recent one is Claude 3.5. We are using this particular model. You can also set the temperature and max tokens here. \n",
    "\n",
    "For now, I'm not doing that. And then again, you can call model.invoke() and ask your question, \"What is the capital of India?\" And whatever result comes, we are storing it in the result variable, and then we are printing out result.content, just like we did a little while ago. Now we are going to run this code. \n",
    "\n",
    "Let's run it, and it says Chat Models... Oh, this is the second file, and you can see this is the output: New Delhi is the capital of India. And you can see some extra output here as well. But again, it varies from model to model, okay? \n",
    "\n",
    "But this is the main power of LangChain. You can see this is the code for talking to the Claude API, and this is the code for talking to the OpenAI API, and you can see there's a minimal difference; the code structure is very consistent, okay? And this helps you a lot in communicating with different types of APIs, okay?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53c5df-4a75-4aae-bc6a-1d04b76093b6",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image36.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image37.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image38.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4662c-ebdb-41aa-ab08-7d4484558eda",
   "metadata": {},
   "source": [
    "```\n",
    "So we've looked at these two models, let's look at another one - Google's Gemini model. So Gemini is a language model from Google, which in fact, now powers your phones as well; many of the AI capabilities in your phones are coming through Google's Gemini model. \n",
    "\n",
    "So in that regard, studying this model is also important because you never know which company you'll work for, whether they'll use OpenAI's APIs or Gemini's. So in any case, what you need to do is follow the same structure again. First, you need an API key. So at this URL, ai.google.dev, you'll find the Gemini API documentation, and here you can click on this button \"Get a Gemini API key,\" and from there you can create an API key. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665a233-a7b7-4fb1-9a5a-c3567752a200",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image39.png\" alt=\"Demo\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image40.png\" alt=\"Demo\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc9399-96c7-4159-b0eb-3ca803448481",
   "metadata": {},
   "source": [
    "```\n",
    "For that, you first need to create a Google Cloud project. I've already created one, so I'll use that. I'll copy this, and then I'll go back to my .env file. And there, I will create a new variable with the name Google API Key. You have to write the exact same name. Save. \n",
    "\n",
    "Now what we'll do is create a new file inside the Chart Models folder. And we'll name this file 3_Chart_Model_Google.py, and again we will start writing the same code. From LangChain_Google_GenAI, this is the package name. We will import Chat Google Generative AI and again from dot env we will import load_dotenv. \n",
    "\n",
    "We have our API Key and now we will create a model which will be an instance of Google Chat Generative AI. Here we have to provide the model name, the model we are using, its name is Gemini 1.5 Pro. And now we will call model.invoke and here we will ask \"What is the capital of India?\" Whatever result comes, we will store it in this variable and then print result.content Save and let's run this and you can see this is the output from our Gemini model. \n",
    "\n",
    "Okay, so just like that, we called three different APIs, sent prompts, and generated results. And in this process, you learned how to interact with the three most important closed-source language models, the most famous ones. \n",
    "\n",
    "Now I know you might be thinking that all this is fine, but the biggest problem with these models is that they are all paid, and we need to work with open-source models. So next, we will do the same thing with an open-source model.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33adb11c-a1e6-44d5-9b05-1554a7468cc9",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image41.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ae270-68ca-42dc-9ec9-09febd124de6",
   "metadata": {},
   "source": [
    "```\n",
    "Open Source Models:\n",
    "\n",
    "Now guys, before we start working with open-source models, I want to tell you a little bit about open-source models so you get some perspective. I've added a line here; let's go through it. Open-source models are freely available AI models that can be downloaded, modified, fine-tuned, and deployed without restrictions from a central provider.\n",
    "\n",
    "Unlike closed-source models such as GPT, Claude, and Gemini, open-source models allow full control and customization. So, the basic idea is that until now we have worked with three models, and all three were closed-source models, proprietary models belonging to a company. So, in this kind of setup, the AI model islocated on the company's server, and then the company creates an API to communicate with this model.\n",
    "\n",
    "Now, the whole world can communicate with this model through this API. But there are two flaws in this whole process. The first flaw is that you have to pay to use this API, right? And the second flaw is that since this model is located on someone else's server, and we can only communicate with it through the API, we don't have control over changing things.\n",
    "\n",
    "So, these are two very major flaws, and who solves these flaws? Open-source models. So, what's the idea behind open-source models? There's an AI model that a company or an organization properly trains and releases somewhere on the internet, right? Now, as a user, what can you do? You can download this model, this trained model, onto your machine, okay?\n",
    "\n",
    "And then you have the freedom to do whatever you want with it. So, basically, first of all, the cost that was incurred for using the API is no longer there because you have downloaded it onto your machine, so you don't need to use the API, you don't need to pay. Secondly, since it's on your machine, you can fine-tune it, modify it, and even if you want, you can deploy it. So, that's the basic concept. You have complete control. So that's the biggest selling point of open-source AI models. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d33f9d-b28b-49bf-8f2c-da955a5e29a1",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image42.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb657c-cac7-4d32-92c4-2599005149b3",
   "metadata": {},
   "source": [
    "```\n",
    "Let me give you a side-by-side comparison here. The first point is cost. With open-source models, you don't have to pay anything to use them. You can run them locally on your machine without using an API. With closed-source models, you have to pay to use the API.\n",
    "\n",
    "Regarding control, since open-source models are on your machine, on your laptop, it's very easy to fine-tune them or deploy them wherever you want. Whereas with closed-source models, you can only use the provider's infrastructure; you have zero control.\n",
    "\n",
    "If we talk about data privacy, you have zero control if we talk about data privacy which is again a very important point. Let's say you're working on very confidential data that you can't share with anyone, and your company hasn't allowed you to. So obviously, you won't be able to use that data with ChatGPT or with OpenAI's GPT model because in that case, you would have to send your data to their servers, right? This is a problem.\n",
    "\n",
    "But in the case of open source, what you're doing is downloading the model to your machine, and you're doing all the processing on your own machine. So this problem is solved here. You can integrate the LLM with your confidential documents as well. So this is a very big selling point of open-source models.\n",
    "\n",
    "After that, customization, I've already mentioned that you can fine-tune them on your own datasets if you want. In closed-source models, some providers give you this feature, but it's very limited.\n",
    "\n",
    "And finally, regarding deployment, you can deploy open-source models on your own servers or on the cloud. closed-source models there's no such option in closed-source models. Okay? So it's very important to understand this entire thing, why open-source models are useful. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09c921-e901-4d88-b935-4a7f5fc3d550",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image43.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695183a8-9943-4897-805f-08a12ec9162f",
   "metadata": {},
   "source": [
    "```\n",
    "Now, if you talk about which open-source models are very famous today, the most famous open-source language model is LLama, from Facebook. The great thing about LLama is that it comes from Facebook, and Facebook is a big company. Other tech giants have also created language models, but they keep them closed source, like OpenAI, Google, and Anthropic. But Facebook decided that no, we will open-source our model, and anyone can download it and use it on their machine. \n",
    "\n",
    "So at this point, this is the strongest open-source model for the purpose of text generation. In addition, you have Mistral, Falcon, and some very domain-specific models as well. For example, BLOOM. So these are some of the famous models that you'll see everywhere if you search for open-source language models.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f49cb3-5e1d-4bc8-b3ae-b3c92a1bec2f",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image43.png\" alt=\"Demo-OpenSource\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image44.png\" alt=\"Demo-OpenSource\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image45.png\" alt=\"Demo-OpenSource\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image46.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d3969-bf27-48d4-af1b-1c5bc7fbd71d",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, the next question that might come to your mind is, okay, this sounds good, where can I find open-source language models? Where can I download these models from? The answer is Hugging Face. So if you haven't heard of Hugging Face, Hugging Face is the largest repository of open-source LLMs. You need to go to their website, and you will find thousands of models in one place. This is the website of Hugging Face. \n",
    "\n",
    "Here, you simply need to click on Models, and you would find that there are thousands of AI models hosted on this platform. There are different types of models, multimodal models where you can send all kinds of input: audio input, video input, text input, speech input, anything. \n",
    "\n",
    "There are computer vision-specific models that perform tasks like image classification and object detection. And then there are NLP-based applications and models, and what we'll be working on currently is text generation. \n",
    "\n",
    "And if you go to text generation, you'll see some very famous names here, such as DeepSeek. If you scroll down, you'll see LLaMA, and if you explore a little more, you'll also find Qwen, which is a new one from China. So you will find all these open-source models in one place, and that is Hugging Face.\n",
    "\n",
    "In fact, when we work with an open-source model, we will also be using the Hugging Face library. Okay, so I hope your question about where to find open-source LLMs has been answered.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9dcb5-0659-4a14-8949-a2e949f943ed",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image47.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109a052-e4f8-4cbe-9eac-120ce3eb17ba",
   "metadata": {},
   "source": [
    "```\n",
    "Now, the next question is, what are the various ways in which you can use open-source models? One way I've already mentioned is that you simply download them to your machine. You download them from Hugging Face and run them on your own machine. This is The most popular way, but there is one more way, and that is to use Hugging Face's inference API, just like OpenAI, just like Google Gemini, Hugging Face also provides you with an API.\n",
    "\n",
    "And the good thing is that this API also has a free tier. Which means that for small testing purposes or for building small projects, you can use this API. It is free to use. After a certain limit, you have to pay, but again, as students, or if you are working on small projects, it won't make much difference.\n",
    "\n",
    "You can even build things using their API.  The best part is that through their API, you can use thousands of models; you have a lot of options. That's the best part about Hugging Face. Okay, so I hope you understood. \n",
    "\n",
    "We will look at both methods. We will first run an open-source model through the API and then we will download it to our machine and run it on our machine as well. We will run the open-source model using both methods. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3182e1-2ff8-4d91-b16e-c0c9caa21eb7",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image48.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979535e-adcc-49ab-ab02-1476a171806d",
   "metadata": {},
   "source": [
    "```\n",
    "Lastly, not everything is good; there are some drawbacks as well. So, if we talk about the disadvantages of open-source models, there are some major disadvantages. For example, the first disadvantage is that if you want to run an open-source model on your machine, it would require solid hardware. \n",
    "\n",
    "In fact, I will show you in a little while how it runs on my machine, a small model, not even a very large one, and you will see that my machine will start struggling because my machine's hardware specifications are not that strong. So, to run large models, you need very expensive GPUs, which are generally not available to individuals. \n",
    "\n",
    "After that, there is the complexity of the setup. You will have to do some preparation when you bring the model and run it on your machine, and that's a bit of a hectic task. \n",
    "\n",
    "Then, the third thing you will notice about open-source models is that they have less refinement. There is a technical reason for this: when open-source models are created, they don't undergo as much fine-tuning based on human feedback. This is called RLHF; \n",
    "\n",
    "we will study about it later, but for now, understand that you will feel that the response of open-source models is a little less refined compared to closed-source models. In fact, I'll show you, the model we'll be using, you might feel that the answers I'm getting aren't as good as the ones I get from OpenAI, but again, you can figure this out because you have the option of fine-tuning.\n",
    "\n",
    "And lastly, you'll mostly get limited multimodal capabilities, at least at this point in time. You'll mostly be able to work with text only. If you want to work with images or audio, you might find fewer such models available right now. Okay?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e82a4-534c-4217-96f8-b5766ea23081",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image49.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1d608-6fd2-42c3-a37b-b8148cb2b642",
   "metadata": {},
   "source": [
    "```\n",
    "So now that you have a little idea about open-source models, let's start working with them. First, I'll show you how to use Hugging Face's inference API, and then I'll show you how to download a model locally to your machine. \n",
    "\n",
    "So guys, now we'll work with an open-source model, and first, we'll use this approach where we'll use that model with the help of the Hugging Face inference API. Okay? We are not downloading the model locally right now. This particular model is hosted on Hugging Face's servers, and with the help of Hugging Face's API, we will basically implement the entire flow.\n",
    "\n",
    "So since again we need to interact with an API, we will need an API key, and we will get the API key from the Hugging Face website. So here is the Hugging Face website. You will have to create an account here. \n",
    "\n",
    "After creating an account and logging in, you will go to your account, then go to Access Tokens, and you can see I have already created an access token. I'll create another one just to show you how this whole thing works. There is a Create New Token button here. Click on it. Here we have options: Fine-grained, Read, Write. We are currently reading, so we are creating a token for reading. Let's call this token Langchain_token_create_token and I will copy this. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c1e45-1849-4a01-93b4-08b7a741369f",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image50.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2560f17-f363-4168-8b46-d9f348a05449",
   "metadata": {},
   "source": [
    "```\n",
    "Now I will go to my project and go to the environment file. And here I will create a new variable Hugging_Face_Hub_Access_Token. This is the name that you have to keep, and here we will paste our access token. Okay? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4eb62-920c-41f9-8a67-a4d4d5a3686e",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image51.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f0ef98-2b1f-44fd-ad11-ab02ecfc859b",
   "metadata": {},
   "source": [
    "```\n",
    "Now what we have to do is create a new file in our chat model folder. I will call this file 4_chat_model_hugging_face_api.py. Now the flow will be roughly the same, but you will see a couple of things that are slightly different, but it's okay, nothing too different. \n",
    "\n",
    "First, you have to import from Langchain_ Hugging Face You need to import two classes: one is Chat Hugging Face, similar to Chat OpenAI, and the second is Hugging Face Endpoint. You need to use this when you want to use the Hugging Face API.  And then we need to import `load_dotenv` from `.env`. After that, we'll call the `load_dotenv` function. Everything is similar so far. \n",
    "\n",
    "Now the main part begins. Here too, you need to create a model, which will be an object of Chat Hugging Face. But here, you need to provide a value for a parameter called `llm`. You need to send an LLM configuration here, and how do you create that LLM? \n",
    "\n",
    "To create that LLM, you will use the Hugging Face Endpoint class. Here, you don't need to specify much, just two things for now. First, you need to specify the repo ID, meaning which model you want to use from Hugging Face. There are thousands of models in Hugging Face; which model do you want to use? \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d85fa-d48e-40e2-9868-02b334f40dc1",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image52.png\" alt=\"Demo-OpenSource\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image53.png\" alt=\"Demo-OpenSource\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image54.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcf3c6-1d8f-41ee-a6d0-6ca328824368",
   "metadata": {},
   "source": [
    "```\n",
    "So, let me show you which model I'm going to use. I will go to Hugging Face, and if you go to Models, under Text Generation... here it is, Text Generation. We are going to use... these models are a bit large. We can use these models, there's no problem, but I want to show you using a smaller model. \n",
    "\n",
    "The model we are using is called TinyLlama. Yes, this one. This is a small model with 1.1 billion parameters, and it's kind of a fine-tuned model on top of the Llama 2 model. Okay, with fewer parameters. We want to use this particular model. So I will copy this. This path you see is kind of the repo ID of this particular model. So we need to copy and paste it here. \n",
    "\n",
    "Second, you need to specify what task you want to perform with this model. The task we want to perform is text generation, okay? And that's it guys. We have configured our LLM. Now we will send this LLM here. That's it. Only this step is different.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aeac5f-9d61-485b-85b7-3b5e54a21354",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image55.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768c414-5006-4b89-a9b4-ff0d06f5bcda",
   "metadata": {},
   "source": [
    "```\n",
    "Now you have your model, you will invoke the model and in the invocation, you will ask \"Who is\" or \"What is the capital of India?\" Okay, whatever response comes, we will store it in the result and print the result. Print result.content and that's it guys, it's that simple.\n",
    "\n",
    "Let's run this code. So now we are running the fourth file chart_model_huggingface_API. Let's run this one and you can see guys this is the result that we are getting from our TinyLlama model. \n",
    "\n",
    "I would like to tell you again that this model is currently on Hugging Face's servers and we are communicating with it through an API. We have not downloaded this model to our machine, okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5464e12-8d4c-4763-85df-23eea471bd56",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image56.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d4a98-763d-44ee-8b92-34f093a50d56",
   "metadata": {},
   "source": [
    "```\n",
    "Now I'm going to show you the second approach where I will download this model to my machine and we will interact with the model that's stored on our machine. Okay, so that will truly give you a flavor of how to work with open-source models. \n",
    "\n",
    "So guys, now we will write the code to download this TinyLlama model locally to our machine and then run it. So for this, I'm creating a new file, and the name of this file is chat_model_huggingface_local.py. Okay, now this code is also kind of similar, there will be a small difference, I'll tell you. \n",
    "\n",
    "First, you need to import two classes again from langchain_huggingface: the first is ChatHuggingFace, which we saw a little while ago, and the second is HuggingFacePipeline. Okay, this is different; last time we used HuggingFaceEndpoint, now we are using HuggingFacePipeline. \n",
    "\n",
    "Now, what we need to do next is create a model, and the model will be an object of ChatHuggingFace, and here we need to provide an LLM. So, again, like before, we will configure an LLM. For this, we will use HuggingFacePipeline. So, HuggingFacePipeline. There is a function here called from_model_id. \n",
    "\n",
    "You need to call this, and in it, you will specify the model ID. The model ID will again be the same as the TinyLlama repo ID. So I will copy this URL, not URL, the repo ID, and I will paste it here. And here too, you need to specify the task, and the task is text generation.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc7315-d8bc-44fc-8065-4708fdc877c7",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image57.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82337956-3a9d-4a03-bd43-dc5b4aa823f7",
   "metadata": {},
   "source": [
    "```\n",
    "Besides this, you have the option to send some keyword arguments here. There is a parameter here called pipeline_keyword_arguments, and here you can send different things, like if you want, you can set the temperature here. Let's say I set it to 0.5. You can set the max tokens here. So max_new_tokens is equal to let's say I set it to 100, so we will only see 100 tokens. Okay? \n",
    "\n",
    "And yeah, next what we can... Do is a... there's an error here. Okay, it's because this is a dictionary, my bad. Yeah, we defined the dictionary this way. But yeah, we defined the dictionary here, and our LLM is configured. We'll send this LLM here, okay? And that's it, guys. \n",
    "\n",
    "Once you've done this, you simply need to do model.invoke and here you'll send your question, \"What is the capital of India?\" Whatever comes back, we'll store it in a variable and we'll print result.content, okay? So, as soon as you run this code, what will happen is that your model, TinyLlama, and all the surrounding configuration files and tokenizers will be downloaded to your machine. \n",
    "\n",
    "These files are around 500 MB, if I remember correctly, or maybe 300-400 MB. Then everything will load into your RAM and everything will run on your machine. It helps if your machine has a GPU; the inference will be a little faster. If you run it on the CPU, the inference will be slow. And you'll see that the processing will be a bit slow on my machine because my machine's configuration isn't that strong, okay? \n",
    "\n",
    "So, let's run this. Before running it, I just want to add one line of code here, and you don't need to add this; this is a limitation of my machine. So, what happens is that by default, all the files are downloaded and stored in your machine's C drive (if you're using Windows). But my C drive is completely full, so I want to explicitly give a command to store the files in the D drive instead of the C drive because I have storage available on my D drive. \n",
    "\n",
    "So, we need to import the OS module, and from there we need to set an environment variable called HF_HOME, and its value will be D:/HuggingFace. The entire folder will be downloaded here in the underscore cache folder. Okay, you don't need to do this; in your case, everything will be downloaded to the default location.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf3210-c18f-4726-86b8-98b706a0b1ef",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image58.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588ddc5-acee-4d44-84d9-1b2e59936326",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's run this code. This is our fifth code, and instead of API, it will be local. As soon as you run it, you will see that some files are being downloaded to your system, and you can see, guys, this is the output we are getting. Okay, the entire output is being printed in a very well-formatted way.\n",
    "\n",
    "First, it shows the user's question, and then it shows the answer given by the AI assistant. It also tells the source from where it picked up this answer. Okay, so this is a very well-tuned model, TinyLlama, that's why you are getting this kind of response. \n",
    "\n",
    "Also, I would like to tell you, although in the video it might seem like I just ran the code and got the response, that's not the case. This code took around 10 minutes to run on my machine. My entire machine became very slow; I couldn't do anything else. I had to shut down and restart, and then I'm shooting this video. \n",
    "\n",
    "So I just want to tell you that inference of such large models is very heavy on small machines, especially if your machine is like mine, where there is 8GB of RAM and limited SSD space, then everything is very problematic. Okay, you need a good configuration to run these models locally on your machine. \n",
    "\n",
    "Also, I would like to tell you that in the same way we ran this particular model, you can go to Hugging Face and run any model. You can download it and use it on your machine in this way. Okay? \n",
    "\n",
    "In the coming lectures, we will do this. But for now, I just wanted to show you how you download and run open-source models on your machine. I hope you understood this. Also, I want to clarify one thing here. I told you that you would first see the model downloading and then you would see the output. \n",
    "\n",
    "So, that's what happened, but the first time my machine froze, and I had to shut it down. Then I ran the code again, and the second time the model didn't download because it was already downloaded on my machine in this folder. So, it was cached from there. That's why the second time I ran the code, you didn't see the output showing how everything was downloaded on our machine. When you run this code on your machine, you will see everything the first time. Okay?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a914b-a84c-4ff3-adcd-e21a3cf1949e",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image59.png\" alt=\"Demo-OpenSource\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8d6db-b01d-4d63-a0e3-2d5059601c13",
   "metadata": {},
   "source": [
    "```\n",
    "So now, if we summarize our progress so far, we have learned to work with language models, and within language models, we have learned to work with both types of models: LLMs and chat models. And within chat models, we have worked with both types of chat models: closed-source (proprietary models) and open-source. We have worked with both types of models. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986ceb9-6e3a-402e-8fc7-aa7666e441f0",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image60.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7349629-7b77-49ad-a6a0-74bb00d34643",
   "metadata": {},
   "source": [
    "```\n",
    "So now we will move to the second part of the lecture where I will show you how to work with embedding models. First, we will work with OpenAI's embedding model, and then I will show you how to download and run an embedding model from Hugging Face on your machine. Okay? \n",
    "\n",
    "So, yeah, let's start. Before interacting with embedding models, I would like to remind you again that we use embedding models to convert a text into a vector so that the contextual understanding of that particular text is captured within that vector. Okay?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864dc147-8cce-4730-ab80-b761d71d6ebd",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image61.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ecc2f9-f69a-4ddf-a94a-bf6695f71655",
   "metadata": {},
   "source": [
    "```\n",
    "So, first, what we will do is go to our third folder, and in this third folder, we will create a new file. Let's call this embedding_openai_query.py. So, first, you need to import a class from langchain_openai, which is called OpenAIEmbeddings. Secondly, you need to import load_dotenv from dotenv, and then you will call load_dotenv. Okay? \n",
    "\n",
    "You have already seen all this. Now, what we need to do is create an object of the OpenAIEmbeddings class, and here we need to specify which embedding model we want to use. Okay? So, multiple embedding models are available. If you go to the OpenAI website, on this page you will see that these are the models that are available. \n",
    "\n",
    "So, let's say we use this one, text-embedding. Three Large. After that, you need to specify how many dimensions you want in the vector output you're expecting.  So here you can write 200, 300, 500, anything. Currently, I'm expecting a 32-dimension vector. If you use a larger vector, more context will be captured; a smaller vector will capture less context. The advantage of using a smaller vector is that it will cost less. I'll explain the cost aspect again in a little while.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953eb36-bb38-4a43-9a09-44a33507677c",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image62.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def90d0-9cf2-4342-9f05-d3d881260157",
   "metadata": {},
   "source": [
    "```\n",
    "For now, this object has been created, and I'm calling it \"embedding.\" Now, with its help, you can perform the embedding operation. So, you need to call the embedding object, and it has a function called `embed_query`. \n",
    "\n",
    "In this `embed_query` function, you can send a single sentence, like \"Delhi is the capital of India.\" Now, this query will go to our embedding model, processing will happen there, a 32-dimension vector will be generated, and it will come back to me. I'm storing it in a variable, and now I'm printing it. To print it, I'm simply converting it to a string so that it's displayed correctly.\n",
    "\n",
    "Now we will run this code. Let's run this code, and you can see, guys, here is our output. This is the 32-dimensional vector that represents the contextual meaning of this particular sentence. Okay, you can play around with this number; instead of 32, you can use 100, 200. It's written here somewhere... Your maximum dimension... Yes, here it is written: By default, the length of the embedding vector will be 1536 for the small model and 3072 for the large model. \n",
    "\n",
    "So you can get a fairly large vector back. Okay? The larger the vector, the more contextual meaning it will capture; the smaller the model, the smaller the vector, and the less contextual meaning it will capture. Okay, so we just ran the code that generates an embedding for a single query. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1caf9d1-934b-418c-b6cd-6d67ed4bfc80",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image63.png\" alt=\"Embeddings\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image64.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698cebc-5e63-4971-8974-0d9edbdca9d4",
   "metadata": {},
   "source": [
    "```\n",
    "Now I'll show you how you can generate embeddings for multiple queries at once. Basically, how you can generate embeddings for multiple documents simultaneously. So what I'll do is I'll create one more file, let's call it embedding_openai_docs.py, and what you need to do is copy this entire code as is, you just need to make a few changes. This embedding will remain exactly the same. \n",
    "\n",
    "Here, you will create a document or a list. Let's call it documents, and inside this list, we will have multiple statements or texts. So, Delhi is the capital of India, Kolkata is the capital of West Bengal, Paris is the capital of France. \n",
    "\n",
    "Let's say these are our three documents, and we need to process all three together and generate an embedding vector for all three. So now, in this case, instead of `embed_query`, there is another function called `embed_documents`. You will call that function. This function has the capability to generate embeddings for multiple documents at once. \n",
    "\n",
    "Here we will pass the documents, and this is the result, and we are printing that result. Okay, everything is the same. Now let's run this code. So instead of `query`, it will be `docs`, and instead of `1`, it will be `2`. Let's run this code, and you can see in this case we have everything as strings.\n",
    "\n",
    "It got converted into a list, but if you look closely, you have a 2D list, and inside that list, there are three lists. Each list is the embedding vector of a document. Okay, so this is how you generate embeddings for any document. \n",
    "\n",
    "Okay, I've written very short sentences here, but you can generate embeddings for proper paragraphs, and we will do that later when we build RAG-based applications. So, for now, we have learned how to generate embeddings by working with a proprietary source model.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d046c4-4530-44f6-a117-185cb26c2138",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image65.png\" alt=\"Embeddings\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image66.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a92a1-167a-4cbc-b927-87894c1cf295",
   "metadata": {},
   "source": [
    "```\n",
    "Now, what we'll do is use an open-source model to generate embeddings. The open-source model we're going to use is called Sentence Transformer. If you search here, this is the model we're going to use. Its name is all-MiniLM-L6-v2, okay? It says here, \"This is a Sentence Transformer model. It maps sentences and paragraphs to a 384-dimensional dense vector space and can be used for tasks like clustering and semantic search.\"\n",
    "\n",
    "So, we'll use this particular model. It's not very large; it's a small model, around 90 MB, if I remember correctly. Let's copy its path or repository ID. Now let's go to our folder and here let's create a file embedding_hf_local. You can also use it through the inference API, but since it's such a small model, it's better to download it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeeb412-17fc-4285-a616-462f9d48536d",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image67.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b74ba-778e-41e5-985a-7f05b602cb6e",
   "metadata": {},
   "source": [
    "```\n",
    "So, first, you need to import a class from langchain_huggingface called HuggingFaceEmbeddings. And then you need to create an embedding object of the HuggingFaceEmbeddings class. Here, you just need to specify your model name, okay? And the model name will be the one we just copied.\n",
    "\n",
    "Once you've created this object, now create a text: \"Delhi is the capital of India,\" okay? And the rest is exactly the same. You have the same function here as well, embedding.embed_query. Send this text to it, and you'll get your vector back, and we're printing that vector as a string. Convert it, that's it guys, this is the code. \n",
    "\n",
    "Let's run it once. This is our third file, Hugging Face local. Let's run this and you can see guys, it's happening for the first time, so it's downloading to our machine, and there are different parts, different files, all of them are being downloaded. So you can see, this model is 90 MB and it's downloaded. The tokenizer is being downloaded. And yeah guys, this is the output. This is a 384-dimensional vector, that's why it looks quite large, but yeah, this is the output. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450be42-28dc-4a01-a558-4d0c64e446b6",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image68.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871e320-eb5b-4e03-8c89-09e17409fde5",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, now here you can do the same thing. If you want, you can generate embeddings for multiple documents together. So I will replace this text with this document and here, instead of text, we will send the document, and obviously, instead of embed_query, it will be embed_document. Everything else will remain exactly the same. Let's run this code one more time. Now this time it won't download because we are running it for the second time, so with the help of the cached version, the entire embedding will be generated. \n",
    "\n",
    "And you can see guys, this is the output. This is a list and there are three 384-dimensional vectors, that's why the output looks quite large, but you can easily extract all three and process them. Okay, so we have learned how to use both types of embedding models. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a62ee-08df-4c21-8517-1dad9d612123",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image69.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8030be-1add-4b66-86e6-fb3459f03197",
   "metadata": {},
   "source": [
    "```\n",
    "So, now let's create that document similarity application. So guys, now we will create a very simple document similarity search application where I will have a set of documents, okay? Let's say we have five documents, and then what will happen is that our user will ask a question, and that question will be related to one of these documents, okay? \n",
    "\n",
    "We need to find out which of these documents this question is related to. Okay, how will this whole process work? I'll tell you. We will generate the embeddings of all these documents and keep them with us, and then when this question comes, we will also generate the embedding of this question, okay? Embeddings mean vectors. So here we have five vectors, and here we have one vector, and all these vectors are of the same dimension. \n",
    "\n",
    "Let's assume for a moment that they are 300 dimensions. So now what we have to do is that we already have five vectors in a 300-dimensional space, and now a new vector has arrived. Let's say this new vector has arrived. So what we have to do is find out which of the other vectors this new vector is closest to. So we will calculate the cosine similarity, basically the angle between the red vector and all the other black vectors, and the one with the highest similarity score will be our answer, okay? \n",
    "\n",
    "So this is what we are going to do in this small application. Later on, you will notice that the same principle is used when you build RAG-based applications, okay? So the code is very simple. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f51d8d-f001-48c6-96d7-13fe64982a51",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image70.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c46df-1e69-45db-b4ea-3a4ad327e517",
   "metadata": {},
   "source": [
    "```\n",
    "Let's go to VS Code and let's create a new file. Let's call it documentsimilarity.py. So first of all, what you have to do is that we will use OpenAI. So we will write `from langchain_openai import OpenAIEmbeddings`. `from dotenv import load_dotenv`.  Besides this, we will need one more thing,\n",
    "and that is cosine similarity from scikit-learn. So we will go inside metrics, inside metrics... Pairwise, we will import cosine similarity, okay, and maybe we might need NumPy so import NumPy as np, okay. Now, first of all, we need to call load_env.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb805c6-5bd2-4f03-8ce4-c81bffbee79a",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image71.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb6af0-3530-4022-bac1-2e5dc4106334",
   "metadata": {},
   "source": [
    "```\n",
    "And after that, we will create an embedding model. Embedding is equal to Open AI Embeddings. We are using the model text-embedding-3-large. Copy-paste, and let's say we are using 300 dimensions. Okay, now we will create our document where all our documents will be in a list. I have written it down somewhere, here it is, the document. Okay, so these are five strings, each string is about a cricketer. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50020f4-62b8-442f-aa81-7f1439bc3b58",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image72.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887808b-a366-49b2-971d-6b78ba5ce21e",
   "metadata": {},
   "source": [
    "```\n",
    "And now my user came and wrote this query: Tell me about Virat Kohli. Okay, this is his query. And now, in response to this query, we have to extract the answer from here. So the most similar document should be our answer. Obviously, it is visible that the first one should be our answer. Let's try how this whole thing works. \n",
    "\n",
    "So first, what we will do is we will extract the embeddings of your document. So we will write document_embedding is equal to, here we will write embedding dot embed_documents and here we will send all our documents. So in this step, I will get five vectors. Each vector will be in a 300 dimensional space. And then similarly, we will extract the embedding of the query.\n",
    "\n",
    "And to do this, we will use embed_query and here we will send our query. Okay, now here I will get a single vector. At this point, I have a set of five vectors and then a set with one query vector. Okay, now what we have to do is calculate the similarity of the query vector with all five vectors. And for this, we will use the cosine similarity function. \n",
    "\n",
    "Here, first you have to send your query embedding, basically the query embedding vector, and you also have to send it as a 2D list. And then... You will send all your document embeddings, which are already in a 2D list. So always remember that for cosine similarity, both values you pass must be 2D lists. And by doing this, you get the similarity scores between your red vector and all the black vectors. Let's run this code and see if we get the similarity scores. And you can see, guys, these are our similarity scores. \n",
    "\n",
    "Basically, what is the similarity between our red vector and the first black vector? Or, to put it another way, what is the similarity between this text and the first text? 0.66, What is its similarity with the second one? 0.34, and so on. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb272c0-bbc2-40bf-abf5-7ee288f05d6d",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image73.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533d993-3b2d-4515-a7c5-464a8346e6d4",
   "metadata": {},
   "source": [
    "```\n",
    "Okay, now one thing to notice is that the result is a 2D list. We don't need a 2D list; we need a simple list. Also, instead of printing it, we'll store it in a variable. Let's call it scores. Now, what we need to do is, it's clear from looking at it that the first one has the highest score, but we need to fetch the relevant documents.\n",
    "\n",
    "For that, we'll have to sort this entire list. But sorting might mess up the positions. So what we'll do is, first, we'll send this entire 1D list into the enumerate function, and we'll see what the output looks like by putting it in a list. I'll show you in a moment what the benefit of this is. So pay attention to what the output will look like now. \n",
    "\n",
    "See, now what happened is that each similarity score. I've attached a number to it: 0 1 2 3 4 5. This is our index position, okay? Now this cannot be changed. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bfef0c-64f5-4dda-99b1-622aa4f55c0d",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image74.png\" alt=\"Embeddings\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image75.png\" alt=\"Embeddings\" width=\"750px\">\n",
    "<img src=\"Images/Images-of-LangChain-Models/Image76.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba50b1b-dd48-4790-bbdf-22f252c4a814",
   "metadata": {},
   "source": [
    "```\n",
    "Even if I sort this entire thing, so now I'm doing the next thing, I'm sorting, and I'm sorting based on the two numbers inside. One is this index number and the other is the similarity score. I'm sorting based on the similarity score.\n",
    "\n",
    "So I'll write here, sort on the basis of lambda x colon x's one. Basically, the second item is not sorted, it's sorted based on this second argument. We will do the sorting based on this. So if I run this, look, it's sorted in ascending order. This is the smallest, it came first, and the largest one went to the back. Okay? Now what do we need? We need the largest one, so we'll use -1.\n",
    "\n",
    "So I will get two things. First, I will know what the highest similarity score is, which is 0.66, and at which index position it is, that document is zero. So now instead of printing, I will save the index and score into variables. And now all I have to do is print this index from my document. Look, this is the document, I will extract the index from it, and along with that, I need to print Similarity score is colon score. Saved and run. \n",
    "\n",
    "Also, here I can print the query that the user asked. Saved and run. And you can see, \"Tell me about Virat Kohli,\" so \"Virat Kohli is an Indian cricketer known for his aggressive approach,\" and the similarity score is 0.66. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cea77e-3a0a-4285-9c19-48e89a3cef9a",
   "metadata": {},
   "source": [
    "<img src=\"Images/Images-of-LangChain-Models/Image77.png\" alt=\"Embeddings\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c255ba-0869-435f-8f1c-65f856faa34a",
   "metadata": {},
   "source": [
    "```\n",
    "Now let's do one thing, instead of \"Tell me about Virat Kohli,\" let's write \"Tell me about Bumrah\" and save it and now run it.\n",
    "And you can see, for \"Tell me about Bumrah,\" our output is coming: \"Jasprit Bumrah is an Indian fast bowler known for his...\" and here is our similarity score. Okay? \n",
    "\n",
    "So what's happening here is, whatever question we ask, we are getting a We are performing semantic search on this set of documents, and we are finding which document is the most similar and retrieving it. Okay? So this is what we will be doing later when we build RAG-based applications. The only difference is that we are making this whole process a bit more complex. We are not storing them anywhere, right? \n",
    "\n",
    "But this is not good. The next time I run this code again, I will again be requesting embeddings from my model. This is a costly operation, right? So the good thing is that you should generate these document embeddings once and store them somewhere. To store them, you need a database, and that database is called a vector database, which we will learn about later. Okay? And then, when a new query comes in, we generate its embedding on the fly, request it from the model, and then calculate this similarity score. This is what we call the retrieval process. \n",
    "\n",
    "So, I just wanted to give you a small perspective on why embedding models are useful. The simple answer is for conducting semantic search. So I hope you understood. With that, I will conclude this video. My plan was to teach something else here; I was planning to make a small chatbot application, but then I thought it would be better to teach you a little about prompts first. \n",
    "\n",
    "So I will cover that in the next video, where we will learn about prompts.  In this video, we covered in quite detail all the models that are in LangChain, and I really hope you have a very good visual diagram in your mind of how many types of models there are and how to work with them. If you liked this video, please like it, and if you haven't subscribed to this channel, please do subscribe. We will be putting a lot of generative AI-related content here in the future. Okay, see you in the next video. Bye.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c6092-ed26-4711-9c5f-b9dab5928937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
